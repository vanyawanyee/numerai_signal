{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eac0e4ca"
      },
      "source": [
        "![](https://miro.medium.com/max/3840/1*LHuN1tJt-abIpuX14v5M8w.png)\n",
        "\n",
        "-----------------------------\n",
        "Notebook stats:\n",
        "- takes 30 minutes with Kaggle CPU\n",
        "- notebook for rapid prototyping with 3200Mhz and 3GHz CPU\n",
        "- Expected to reach hitrate of 80 in validation\n",
        "- Use 400GB server and uncomment all features for 85% hitrate, mse: 0.2-0.21200\n",
        "- XGB 0.22198 but only after 11h+ with depth 14 xgb takes 8,5h\n",
        "- Neural Network: mse: 0.15?\n",
        "-----------------------------\n",
        "\n",
        "This is yet another starter notebook for the [Numerai Signals](https://signals.numer.ai/).\n",
        "\n",
        "What we do here includes:\n",
        "\n",
        "- fetch US stock price data via YFinance API\n",
        "- merge the data with the Numerai Signals' historical targets\n",
        "- perform feature engineering (considering stational features)\n",
        "- modeling with XGBoost\n",
        "- submit (if you want)\n",
        "\n",
        "In a kaggle dataset [YFinance Stock Price Data for Numerai Signals](https://www.kaggle.com/code1110/yfinance-stock-price-data-for-numerai-signals), I fetch the stock price data on a daily basis via the YFinance API. So if you are bothered using the API for yourself, just use this dataset (it must be up-to-date).\n",
        "\n",
        "This content is largely inspired by the following starter.\n",
        "\n",
        ">End to end notebook for Numerai Signals using completely free data from Yahoo Finance, by Jason Rosenfeld (jrAI).\n",
        "\n",
        "https://colab.research.google.com/drive/1ECh69C0LDCUnuyvEmNFZ51l_276nkQqo#scrollTo=tTBUzPep2dm3\n",
        "\n",
        "#round562 node2: start 14:19,2min used to turn LGBM off"
      ],
      "id": "eac0e4ca"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eNyIBoTnjMru"
      },
      "source": [
        "## Install Pandas using CUDF (100x speedup with GPU + less memory failures)"
      ],
      "id": "eNyIBoTnjMru"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MoFnkHbkjGED"
      },
      "source": [
        "### First restart the runtime\n",
        "### Goes automatically through popup on colab"
      ],
      "id": "MoFnkHbkjGED"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "7yo5WWrtjZCe"
      },
      "outputs": [],
      "source": [
        "#!pip install plotly-express"
      ],
      "id": "7yo5WWrtjZCe"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7ea3b833"
      },
      "source": [
        "# Libraries\n",
        "Let's import what we need..."
      ],
      "id": "7ea3b833"
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "aCcz2pLqj0v1"
      },
      "id": "aCcz2pLqj0v1",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "e0bd98ef",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "c07716e7-e88f-438d-b820-a1a04ee546ca"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: numerapi in /usr/local/lib/python3.10/dist-packages (2.19.1)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from numerapi) (2.32.3)\n",
            "Requirement already satisfied: pytz in /usr/local/lib/python3.10/dist-packages (from numerapi) (2024.1)\n",
            "Requirement already satisfied: python-dateutil in /usr/local/lib/python3.10/dist-packages (from numerapi) (2.9.0.post0)\n",
            "Requirement already satisfied: tqdm>=4.29.1 in /usr/local/lib/python3.10/dist-packages (from numerapi) (4.66.5)\n",
            "Requirement already satisfied: click>=7.0 in /usr/local/lib/python3.10/dist-packages (from numerapi) (8.1.7)\n",
            "Requirement already satisfied: pandas>=1.1.0 in /usr/local/lib/python3.10/dist-packages (from numerapi) (2.2.2)\n",
            "Requirement already satisfied: numpy>=1.22.4 in /usr/local/lib/python3.10/dist-packages (from pandas>=1.1.0->numerapi) (1.26.4)\n",
            "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.10/dist-packages (from pandas>=1.1.0->numerapi) (2024.1)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.10/dist-packages (from python-dateutil->numerapi) (1.16.0)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests->numerapi) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->numerapi) (3.8)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->numerapi) (2.2.2)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->numerapi) (2024.7.4)\n",
            "Requirement already satisfied: pandas in /usr/local/lib/python3.10/dist-packages (2.2.2)\n",
            "Requirement already satisfied: numpy>=1.22.4 in /usr/local/lib/python3.10/dist-packages (from pandas) (1.26.4)\n",
            "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.10/dist-packages (from pandas) (2.9.0.post0)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.10/dist-packages (from pandas) (2024.1)\n",
            "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.10/dist-packages (from pandas) (2024.1)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.10/dist-packages (from python-dateutil>=2.8.2->pandas) (1.16.0)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (2.32.3)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests) (3.8)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests) (2.2.2)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests) (2024.7.4)\n",
            "Requirement already satisfied: tabulate in /usr/local/lib/python3.10/dist-packages (0.9.0)\n",
            "Requirement already satisfied: future in /usr/local/lib/python3.10/dist-packages (1.0.0)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.10/dist-packages (1.16.0)\n",
            "Requirement already satisfied: matplotlib_venn in /usr/local/lib/python3.10/dist-packages (0.11.10)\n",
            "Requirement already satisfied: matplotlib in /usr/local/lib/python3.10/dist-packages (from matplotlib_venn) (3.7.1)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.10/dist-packages (from matplotlib_venn) (1.26.4)\n",
            "Requirement already satisfied: scipy in /usr/local/lib/python3.10/dist-packages (from matplotlib_venn) (1.13.1)\n",
            "Requirement already satisfied: contourpy>=1.0.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib->matplotlib_venn) (1.3.0)\n",
            "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.10/dist-packages (from matplotlib->matplotlib_venn) (0.12.1)\n",
            "Requirement already satisfied: fonttools>=4.22.0 in /usr/local/lib/python3.10/dist-packages (from matplotlib->matplotlib_venn) (4.53.1)\n",
            "Requirement already satisfied: kiwisolver>=1.0.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib->matplotlib_venn) (1.4.5)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from matplotlib->matplotlib_venn) (24.1)\n",
            "Requirement already satisfied: pillow>=6.2.0 in /usr/local/lib/python3.10/dist-packages (from matplotlib->matplotlib_venn) (10.4.0)\n",
            "Requirement already satisfied: pyparsing>=2.3.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib->matplotlib_venn) (3.1.4)\n",
            "Requirement already satisfied: python-dateutil>=2.7 in /usr/local/lib/python3.10/dist-packages (from matplotlib->matplotlib_venn) (2.9.0.post0)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.10/dist-packages (from python-dateutil>=2.7->matplotlib->matplotlib_venn) (1.16.0)\n",
            "Requirement already satisfied: pyarrow in /usr/local/lib/python3.10/dist-packages (17.0.0)\n",
            "Requirement already satisfied: numpy>=1.16.6 in /usr/local/lib/python3.10/dist-packages (from pyarrow) (1.26.4)\n",
            "Requirement already satisfied: pip in /usr/local/lib/python3.10/dist-packages (24.2)\n",
            "Requirement already satisfied: graphviz in /usr/local/lib/python3.10/dist-packages (0.20.3)\n",
            "Requirement already satisfied: joblib in /usr/local/lib/python3.10/dist-packages (1.4.2)\n",
            "Requirement already satisfied: seaborn in /usr/local/lib/python3.10/dist-packages (0.13.1)\n",
            "Requirement already satisfied: numpy!=1.24.0,>=1.20 in /usr/local/lib/python3.10/dist-packages (from seaborn) (1.26.4)\n",
            "Requirement already satisfied: pandas>=1.2 in /usr/local/lib/python3.10/dist-packages (from seaborn) (2.2.2)\n",
            "Requirement already satisfied: matplotlib!=3.6.1,>=3.4 in /usr/local/lib/python3.10/dist-packages (from seaborn) (3.7.1)\n",
            "Requirement already satisfied: contourpy>=1.0.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib!=3.6.1,>=3.4->seaborn) (1.3.0)\n",
            "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.10/dist-packages (from matplotlib!=3.6.1,>=3.4->seaborn) (0.12.1)\n",
            "Requirement already satisfied: fonttools>=4.22.0 in /usr/local/lib/python3.10/dist-packages (from matplotlib!=3.6.1,>=3.4->seaborn) (4.53.1)\n",
            "Requirement already satisfied: kiwisolver>=1.0.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib!=3.6.1,>=3.4->seaborn) (1.4.5)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from matplotlib!=3.6.1,>=3.4->seaborn) (24.1)\n",
            "Requirement already satisfied: pillow>=6.2.0 in /usr/local/lib/python3.10/dist-packages (from matplotlib!=3.6.1,>=3.4->seaborn) (10.4.0)\n",
            "Requirement already satisfied: pyparsing>=2.3.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib!=3.6.1,>=3.4->seaborn) (3.1.4)\n",
            "Requirement already satisfied: python-dateutil>=2.7 in /usr/local/lib/python3.10/dist-packages (from matplotlib!=3.6.1,>=3.4->seaborn) (2.9.0.post0)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.10/dist-packages (from pandas>=1.2->seaborn) (2024.1)\n",
            "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.10/dist-packages (from pandas>=1.2->seaborn) (2024.1)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.10/dist-packages (from python-dateutil>=2.7->matplotlib!=3.6.1,>=3.4->seaborn) (1.16.0)\n",
            "Requirement already satisfied: h2o_pysparkling_3.2 in /usr/local/lib/python3.10/dist-packages (3.46.0.4.post1)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from h2o_pysparkling_3.2) (2.32.3)\n",
            "Requirement already satisfied: tabulate in /usr/local/lib/python3.10/dist-packages (from h2o_pysparkling_3.2) (0.9.0)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests->h2o_pysparkling_3.2) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->h2o_pysparkling_3.2) (3.8)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->h2o_pysparkling_3.2) (2.2.2)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->h2o_pysparkling_3.2) (2024.7.4)\n",
            "Requirement already satisfied: xgboost in /usr/local/lib/python3.10/dist-packages (2.1.1)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.10/dist-packages (from xgboost) (1.26.4)\n",
            "Requirement already satisfied: nvidia-nccl-cu12 in /usr/local/lib/python3.10/dist-packages (from xgboost) (2.22.3)\n",
            "Requirement already satisfied: scipy in /usr/local/lib/python3.10/dist-packages (from xgboost) (1.13.1)\n",
            "Requirement already satisfied: lightgbm in /usr/local/lib/python3.10/dist-packages (4.4.0)\n",
            "Requirement already satisfied: numpy>=1.17.0 in /usr/local/lib/python3.10/dist-packages (from lightgbm) (1.26.4)\n",
            "Requirement already satisfied: scipy in /usr/local/lib/python3.10/dist-packages (from lightgbm) (1.13.1)\n",
            "Requirement already satisfied: scikit-learn in /usr/local/lib/python3.10/dist-packages (1.3.2)\n",
            "Requirement already satisfied: numpy<2.0,>=1.17.3 in /usr/local/lib/python3.10/dist-packages (from scikit-learn) (1.26.4)\n",
            "Requirement already satisfied: scipy>=1.5.0 in /usr/local/lib/python3.10/dist-packages (from scikit-learn) (1.13.1)\n",
            "Requirement already satisfied: joblib>=1.1.1 in /usr/local/lib/python3.10/dist-packages (from scikit-learn) (1.4.2)\n",
            "Requirement already satisfied: threadpoolctl>=2.0.0 in /usr/local/lib/python3.10/dist-packages (from scikit-learn) (3.5.0)\n",
            "\u001b[33mWARNING: Skipping neptune-client as it is not installed.\u001b[0m\u001b[33m\n",
            "\u001b[0mFound existing installation: neptune 1.11.1\n",
            "Uninstalling neptune-1.11.1:\n",
            "  Successfully uninstalled neptune-1.11.1\n",
            "Requirement already satisfied: scikit-learn in /usr/local/lib/python3.10/dist-packages (1.3.2)\n",
            "Requirement already satisfied: numpy<2.0,>=1.17.3 in /usr/local/lib/python3.10/dist-packages (from scikit-learn) (1.26.4)\n",
            "Requirement already satisfied: scipy>=1.5.0 in /usr/local/lib/python3.10/dist-packages (from scikit-learn) (1.13.1)\n",
            "Requirement already satisfied: joblib>=1.1.1 in /usr/local/lib/python3.10/dist-packages (from scikit-learn) (1.4.2)\n",
            "Requirement already satisfied: threadpoolctl>=2.0.0 in /usr/local/lib/python3.10/dist-packages (from scikit-learn) (3.5.0)\n",
            "Collecting neptune\n",
            "  Using cached neptune-1.11.1-py3-none-any.whl.metadata (16 kB)\n",
            "Requirement already satisfied: GitPython>=2.0.8 in /usr/local/lib/python3.10/dist-packages (from neptune) (3.1.43)\n",
            "Requirement already satisfied: Pillow>=1.1.6 in /usr/local/lib/python3.10/dist-packages (from neptune) (10.4.0)\n",
            "Requirement already satisfied: PyJWT in /usr/local/lib/python3.10/dist-packages (from neptune) (2.9.0)\n",
            "Requirement already satisfied: boto3>=1.28.0 in /usr/local/lib/python3.10/dist-packages (from neptune) (1.35.9)\n",
            "Requirement already satisfied: bravado<12.0.0,>=11.0.0 in /usr/local/lib/python3.10/dist-packages (from neptune) (11.0.3)\n",
            "Requirement already satisfied: click>=7.0 in /usr/local/lib/python3.10/dist-packages (from neptune) (8.1.7)\n",
            "Requirement already satisfied: future>=0.17.1 in /usr/local/lib/python3.10/dist-packages (from neptune) (1.0.0)\n",
            "Requirement already satisfied: oauthlib>=2.1.0 in /usr/local/lib/python3.10/dist-packages (from neptune) (3.2.2)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.10/dist-packages (from neptune) (24.1)\n",
            "Requirement already satisfied: pandas in /usr/local/lib/python3.10/dist-packages (from neptune) (2.2.2)\n",
            "Requirement already satisfied: psutil in /usr/local/lib/python3.10/dist-packages (from neptune) (6.0.0)\n",
            "Requirement already satisfied: requests>=2.20.0 in /usr/local/lib/python3.10/dist-packages (from neptune) (2.32.3)\n",
            "Requirement already satisfied: requests-oauthlib>=1.0.0 in /usr/local/lib/python3.10/dist-packages (from neptune) (1.3.1)\n",
            "Requirement already satisfied: six>=1.12.0 in /usr/local/lib/python3.10/dist-packages (from neptune) (1.16.0)\n",
            "Requirement already satisfied: swagger-spec-validator>=2.7.4 in /usr/local/lib/python3.10/dist-packages (from neptune) (3.0.4)\n",
            "Requirement already satisfied: typing-extensions>=3.10.0 in /usr/local/lib/python3.10/dist-packages (from neptune) (4.12.2)\n",
            "Requirement already satisfied: urllib3 in /usr/local/lib/python3.10/dist-packages (from neptune) (2.2.2)\n",
            "Requirement already satisfied: websocket-client!=1.0.0,>=0.35.0 in /usr/local/lib/python3.10/dist-packages (from neptune) (1.8.0)\n",
            "Requirement already satisfied: botocore<1.36.0,>=1.35.9 in /usr/local/lib/python3.10/dist-packages (from boto3>=1.28.0->neptune) (1.35.9)\n",
            "Requirement already satisfied: jmespath<2.0.0,>=0.7.1 in /usr/local/lib/python3.10/dist-packages (from boto3>=1.28.0->neptune) (1.0.1)\n",
            "Requirement already satisfied: s3transfer<0.11.0,>=0.10.0 in /usr/local/lib/python3.10/dist-packages (from boto3>=1.28.0->neptune) (0.10.2)\n",
            "Requirement already satisfied: bravado-core>=5.16.1 in /usr/local/lib/python3.10/dist-packages (from bravado<12.0.0,>=11.0.0->neptune) (6.1.1)\n",
            "Requirement already satisfied: msgpack in /usr/local/lib/python3.10/dist-packages (from bravado<12.0.0,>=11.0.0->neptune) (1.0.8)\n",
            "Requirement already satisfied: python-dateutil in /usr/local/lib/python3.10/dist-packages (from bravado<12.0.0,>=11.0.0->neptune) (2.9.0.post0)\n",
            "Requirement already satisfied: pyyaml in /usr/local/lib/python3.10/dist-packages (from bravado<12.0.0,>=11.0.0->neptune) (6.0.2)\n",
            "Requirement already satisfied: simplejson in /usr/local/lib/python3.10/dist-packages (from bravado<12.0.0,>=11.0.0->neptune) (3.19.3)\n",
            "Requirement already satisfied: monotonic in /usr/local/lib/python3.10/dist-packages (from bravado<12.0.0,>=11.0.0->neptune) (1.6)\n",
            "Requirement already satisfied: gitdb<5,>=4.0.1 in /usr/local/lib/python3.10/dist-packages (from GitPython>=2.0.8->neptune) (4.0.11)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests>=2.20.0->neptune) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests>=2.20.0->neptune) (3.8)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests>=2.20.0->neptune) (2024.7.4)\n",
            "Requirement already satisfied: jsonschema in /usr/local/lib/python3.10/dist-packages (from swagger-spec-validator>=2.7.4->neptune) (4.23.0)\n",
            "Requirement already satisfied: importlib-resources>=1.3 in /usr/local/lib/python3.10/dist-packages (from swagger-spec-validator>=2.7.4->neptune) (6.4.4)\n",
            "Requirement already satisfied: numpy>=1.22.4 in /usr/local/lib/python3.10/dist-packages (from pandas->neptune) (1.26.4)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.10/dist-packages (from pandas->neptune) (2024.1)\n",
            "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.10/dist-packages (from pandas->neptune) (2024.1)\n",
            "Requirement already satisfied: jsonref in /usr/local/lib/python3.10/dist-packages (from bravado-core>=5.16.1->bravado<12.0.0,>=11.0.0->neptune) (1.1.0)\n",
            "Requirement already satisfied: smmap<6,>=3.0.1 in /usr/local/lib/python3.10/dist-packages (from gitdb<5,>=4.0.1->GitPython>=2.0.8->neptune) (5.0.1)\n",
            "Requirement already satisfied: attrs>=22.2.0 in /usr/local/lib/python3.10/dist-packages (from jsonschema->swagger-spec-validator>=2.7.4->neptune) (24.2.0)\n",
            "Requirement already satisfied: jsonschema-specifications>=2023.03.6 in /usr/local/lib/python3.10/dist-packages (from jsonschema->swagger-spec-validator>=2.7.4->neptune) (2023.12.1)\n",
            "Requirement already satisfied: referencing>=0.28.4 in /usr/local/lib/python3.10/dist-packages (from jsonschema->swagger-spec-validator>=2.7.4->neptune) (0.35.1)\n",
            "Requirement already satisfied: rpds-py>=0.7.1 in /usr/local/lib/python3.10/dist-packages (from jsonschema->swagger-spec-validator>=2.7.4->neptune) (0.20.0)\n",
            "Requirement already satisfied: fqdn in /usr/local/lib/python3.10/dist-packages (from jsonschema[format-nongpl]>=2.5.1->bravado-core>=5.16.1->bravado<12.0.0,>=11.0.0->neptune) (1.5.1)\n",
            "Requirement already satisfied: isoduration in /usr/local/lib/python3.10/dist-packages (from jsonschema[format-nongpl]>=2.5.1->bravado-core>=5.16.1->bravado<12.0.0,>=11.0.0->neptune) (20.11.0)\n",
            "Requirement already satisfied: jsonpointer>1.13 in /usr/local/lib/python3.10/dist-packages (from jsonschema[format-nongpl]>=2.5.1->bravado-core>=5.16.1->bravado<12.0.0,>=11.0.0->neptune) (3.0.0)\n",
            "Requirement already satisfied: rfc3339-validator in /usr/local/lib/python3.10/dist-packages (from jsonschema[format-nongpl]>=2.5.1->bravado-core>=5.16.1->bravado<12.0.0,>=11.0.0->neptune) (0.1.4)\n",
            "Requirement already satisfied: rfc3986-validator>0.1.0 in /usr/local/lib/python3.10/dist-packages (from jsonschema[format-nongpl]>=2.5.1->bravado-core>=5.16.1->bravado<12.0.0,>=11.0.0->neptune) (0.1.1)\n",
            "Requirement already satisfied: uri-template in /usr/local/lib/python3.10/dist-packages (from jsonschema[format-nongpl]>=2.5.1->bravado-core>=5.16.1->bravado<12.0.0,>=11.0.0->neptune) (1.3.0)\n",
            "Requirement already satisfied: webcolors>=24.6.0 in /usr/local/lib/python3.10/dist-packages (from jsonschema[format-nongpl]>=2.5.1->bravado-core>=5.16.1->bravado<12.0.0,>=11.0.0->neptune) (24.8.0)\n",
            "Requirement already satisfied: arrow>=0.15.0 in /usr/local/lib/python3.10/dist-packages (from isoduration->jsonschema[format-nongpl]>=2.5.1->bravado-core>=5.16.1->bravado<12.0.0,>=11.0.0->neptune) (1.3.0)\n",
            "Requirement already satisfied: types-python-dateutil>=2.8.10 in /usr/local/lib/python3.10/dist-packages (from arrow>=0.15.0->isoduration->jsonschema[format-nongpl]>=2.5.1->bravado-core>=5.16.1->bravado<12.0.0,>=11.0.0->neptune) (2.9.0.20240821)\n",
            "Using cached neptune-1.11.1-py3-none-any.whl (501 kB)\n",
            "Installing collected packages: neptune\n",
            "Successfully installed neptune-1.11.1\n",
            "Requirement already satisfied: neptune-xgboost in /usr/local/lib/python3.10/dist-packages (1.1.1)\n",
            "Requirement already satisfied: graphviz in /usr/local/lib/python3.10/dist-packages (from neptune-xgboost) (0.20.3)\n",
            "Requirement already satisfied: matplotlib in /usr/local/lib/python3.10/dist-packages (from neptune-xgboost) (3.7.1)\n",
            "Requirement already satisfied: xgboost>=1.3.0 in /usr/local/lib/python3.10/dist-packages (from neptune-xgboost) (2.1.1)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.10/dist-packages (from xgboost>=1.3.0->neptune-xgboost) (1.26.4)\n",
            "Requirement already satisfied: nvidia-nccl-cu12 in /usr/local/lib/python3.10/dist-packages (from xgboost>=1.3.0->neptune-xgboost) (2.22.3)\n",
            "Requirement already satisfied: scipy in /usr/local/lib/python3.10/dist-packages (from xgboost>=1.3.0->neptune-xgboost) (1.13.1)\n",
            "Requirement already satisfied: contourpy>=1.0.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib->neptune-xgboost) (1.3.0)\n",
            "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.10/dist-packages (from matplotlib->neptune-xgboost) (0.12.1)\n",
            "Requirement already satisfied: fonttools>=4.22.0 in /usr/local/lib/python3.10/dist-packages (from matplotlib->neptune-xgboost) (4.53.1)\n",
            "Requirement already satisfied: kiwisolver>=1.0.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib->neptune-xgboost) (1.4.5)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from matplotlib->neptune-xgboost) (24.1)\n",
            "Requirement already satisfied: pillow>=6.2.0 in /usr/local/lib/python3.10/dist-packages (from matplotlib->neptune-xgboost) (10.4.0)\n",
            "Requirement already satisfied: pyparsing>=2.3.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib->neptune-xgboost) (3.1.4)\n",
            "Requirement already satisfied: python-dateutil>=2.7 in /usr/local/lib/python3.10/dist-packages (from matplotlib->neptune-xgboost) (2.9.0.post0)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.10/dist-packages (from python-dateutil>=2.7->matplotlib->neptune-xgboost) (1.16.0)\n",
            "Requirement already satisfied: pyspark==3.2.2 in /usr/local/lib/python3.10/dist-packages (3.2.2)\n",
            "Requirement already satisfied: py4j==0.10.9.5 in /usr/local/lib/python3.10/dist-packages (from pyspark==3.2.2) (0.10.9.5)\n",
            "\u001b[31mERROR: Could not find a version that satisfies the requirement pysparklingptune-lightgbm (from versions: none)\u001b[0m\u001b[31m\n",
            "\u001b[0m\u001b[31mERROR: No matching distribution found for pysparklingptune-lightgbm\u001b[0m\u001b[31m\n",
            "\u001b[0mRequirement already satisfied: dask[full] in /usr/local/lib/python3.10/dist-packages (2024.8.1)\n",
            "\u001b[33mWARNING: dask 2024.8.1 does not provide the extra 'full'\u001b[0m\u001b[33m\n",
            "\u001b[0mRequirement already satisfied: click>=8.1 in /usr/local/lib/python3.10/dist-packages (from dask[full]) (8.1.7)\n",
            "Requirement already satisfied: cloudpickle>=3.0.0 in /usr/local/lib/python3.10/dist-packages (from dask[full]) (3.0.0)\n",
            "Requirement already satisfied: fsspec>=2021.09.0 in /usr/local/lib/python3.10/dist-packages (from dask[full]) (2024.6.1)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from dask[full]) (24.1)\n",
            "Requirement already satisfied: partd>=1.4.0 in /usr/local/lib/python3.10/dist-packages (from dask[full]) (1.4.2)\n",
            "Requirement already satisfied: pyyaml>=5.3.1 in /usr/local/lib/python3.10/dist-packages (from dask[full]) (6.0.2)\n",
            "Requirement already satisfied: toolz>=0.10.0 in /usr/local/lib/python3.10/dist-packages (from dask[full]) (0.12.1)\n",
            "Requirement already satisfied: importlib-metadata>=4.13.0 in /usr/local/lib/python3.10/dist-packages (from dask[full]) (8.4.0)\n",
            "Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.10/dist-packages (from importlib-metadata>=4.13.0->dask[full]) (3.20.1)\n",
            "Requirement already satisfied: locket in /usr/local/lib/python3.10/dist-packages (from partd>=1.4.0->dask[full]) (1.0.0)\n",
            "Found existing installation: Jinja2 3.1.4\n",
            "Uninstalling Jinja2-3.1.4:\n",
            "  Successfully uninstalled Jinja2-3.1.4\n",
            "Collecting jinja2\n",
            "  Using cached jinja2-3.1.4-py3-none-any.whl.metadata (2.6 kB)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2) (2.1.5)\n",
            "Using cached jinja2-3.1.4-py3-none-any.whl (133 kB)\n",
            "Installing collected packages: jinja2\n",
            "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "panel 1.4.5 requires bokeh<3.5.0,>=3.4.0, but you have bokeh 3.5.2 which is incompatible.\u001b[0m\u001b[31m\n",
            "\u001b[0mSuccessfully installed jinja2-3.1.4\n"
          ]
        }
      ],
      "source": [
        "!pip3 install numerapi --upgrade\n",
        "!pip install pandas --upgrade\n",
        "!pip install requests\n",
        "!pip install tabulate\n",
        "!pip install future\n",
        "!pip install colorama>=0.3.8\n",
        "!pip install six\n",
        "#!wget http://h2o-release.s3.amazonaws.com/h2o/rel-zygmund/1/h2o-3.38.0.1.zip\n",
        "#!unzip h2o-3.38.0.1.zip\n",
        "#!cd h2o-3.38.0.1/\n",
        "#!bin/sparkling-shell --num-executors 3 --executor-memory 2g --master yarn --deploy-mode client\n",
        "\n",
        "#!pip install zipfile\n",
        "\n",
        "#!pip install --no-index finta --find-links file:../input/finta-package\n",
        "!pip install matplotlib_venn\n",
        "!pip3 install pyarrow\n",
        "!pip install --upgrade pip\n",
        "!pip install -U graphviz #for visualizing boosting trees\n",
        "!pip install joblib\n",
        "\n",
        "!pip install seaborn\n",
        "\n",
        "# ML packages\n",
        "!pip install h2o_pysparkling_3.2\n",
        "#!pip install h2o_pysparkling_3.4\n",
        "!pip install xgboost\n",
        "!pip install lightgbm\n",
        "!pip install scikit-learn\n",
        "!pip uninstall neptune-client -y\n",
        "!pip uninstall neptune -y\n",
        "!pip install scikit-learn #!pip install neptune-client neptune-xgboost\n",
        "!pip install neptune\n",
        "!pip install neptune-xgboost\n",
        "#!pip install -U ne#https://docs.h2o.ai/sparkling-water/3.3/latest-stable/doc/pysparkling.html\n",
        "# downgrade pyspark installation to adhere to sparkling-water setup (node 1)\n",
        "\n",
        "# Computing packages\n",
        "!pip install pyspark==3.2.2\n",
        "!pip install pysparklingptune-lightgbm\n",
        "!pip install dask[full]\n",
        "!pip uninstall jinja2 -y\n",
        "!pip install jinja2 --upgrade\n",
        "#!pip install graphviz\n",
        "\n",
        "# Could take 5min"
      ],
      "id": "e0bd98ef"
    },
    {
      "cell_type": "code",
      "source": [
        "use_gpu = 1\n",
        "if use_gpu == 1:\n",
        "  !pip uninstall cudf-cu12 -y\n",
        "  !pip install cudf-cu11 --extra-index-url=https://pypi.nvidia.com"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8aHSrpq7Li4k",
        "outputId": "636b5b6c-1fcc-45ce-ca21-b9654e8d0fed"
      },
      "id": "8aHSrpq7Li4k",
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Found existing installation: cudf-cu12 24.4.1\n",
            "Uninstalling cudf-cu12-24.4.1:\n",
            "  Successfully uninstalled cudf-cu12-24.4.1\n",
            "Looking in indexes: https://pypi.org/simple, https://pypi.nvidia.com\n",
            "Requirement already satisfied: cudf-cu11 in /usr/local/lib/python3.10/dist-packages (24.8.2)\n",
            "Requirement already satisfied: cachetools in /usr/local/lib/python3.10/dist-packages (from cudf-cu11) (5.5.0)\n",
            "Requirement already satisfied: cubinlinker-cu11 in /usr/local/lib/python3.10/dist-packages (from cudf-cu11) (0.3.0.post2)\n",
            "Requirement already satisfied: cuda-python<12.0a0,>=11.7.1 in /usr/local/lib/python3.10/dist-packages (from cudf-cu11) (11.8.3)\n",
            "Requirement already satisfied: cupy-cuda11x>=12.0.0 in /usr/local/lib/python3.10/dist-packages (from cudf-cu11) (13.3.0)\n",
            "Requirement already satisfied: fsspec>=0.6.0 in /usr/local/lib/python3.10/dist-packages (from cudf-cu11) (2024.6.1)\n",
            "Requirement already satisfied: numba>=0.57 in /usr/local/lib/python3.10/dist-packages (from cudf-cu11) (0.60.0)\n",
            "Requirement already satisfied: numpy<2.0a0,>=1.23 in /usr/local/lib/python3.10/dist-packages (from cudf-cu11) (1.26.4)\n",
            "Requirement already satisfied: nvtx>=0.2.1 in /usr/local/lib/python3.10/dist-packages (from cudf-cu11) (0.2.10)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.10/dist-packages (from cudf-cu11) (24.1)\n",
            "Requirement already satisfied: pandas<2.2.3dev0,>=2.0 in /usr/local/lib/python3.10/dist-packages (from cudf-cu11) (2.2.1)\n",
            "Requirement already satisfied: ptxcompiler-cu11 in /usr/local/lib/python3.10/dist-packages (from cudf-cu11) (0.8.1.post1)\n",
            "Requirement already satisfied: pyarrow<16.2.0a0,>=16.1.0 in /usr/local/lib/python3.10/dist-packages (from cudf-cu11) (16.1.0)\n",
            "Requirement already satisfied: rich in /usr/local/lib/python3.10/dist-packages (from cudf-cu11) (13.8.0)\n",
            "Requirement already satisfied: rmm-cu11==24.8.* in /usr/local/lib/python3.10/dist-packages (from cudf-cu11) (24.8.2)\n",
            "Requirement already satisfied: typing_extensions>=4.0.0 in /usr/local/lib/python3.10/dist-packages (from cudf-cu11) (4.12.2)\n",
            "Requirement already satisfied: fastrlock>=0.5 in /usr/local/lib/python3.10/dist-packages (from cupy-cuda11x>=12.0.0->cudf-cu11) (0.8.2)\n",
            "Requirement already satisfied: llvmlite<0.44,>=0.43.0dev0 in /usr/local/lib/python3.10/dist-packages (from numba>=0.57->cudf-cu11) (0.43.0)\n",
            "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.10/dist-packages (from pandas<2.2.3dev0,>=2.0->cudf-cu11) (2.9.0.post0)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.10/dist-packages (from pandas<2.2.3dev0,>=2.0->cudf-cu11) (2024.1)\n",
            "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.10/dist-packages (from pandas<2.2.3dev0,>=2.0->cudf-cu11) (2024.1)\n",
            "Requirement already satisfied: markdown-it-py>=2.2.0 in /usr/local/lib/python3.10/dist-packages (from rich->cudf-cu11) (3.0.0)\n",
            "Requirement already satisfied: pygments<3.0.0,>=2.13.0 in /usr/local/lib/python3.10/dist-packages (from rich->cudf-cu11) (2.16.1)\n",
            "Requirement already satisfied: mdurl~=0.1 in /usr/local/lib/python3.10/dist-packages (from markdown-it-py>=2.2.0->rich->cudf-cu11) (0.1.2)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.10/dist-packages (from python-dateutil>=2.8.2->pandas<2.2.3dev0,>=2.0->cudf-cu11) (1.16.0)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LHqim5xZ-ff6"
      },
      "source": [
        "### Restart Runtime in Colab"
      ],
      "id": "LHqim5xZ-ff6"
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install --upgrade pyarrow"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8k6U7EB6MD6O",
        "outputId": "eac0e65e-6e3f-4ac9-9e91-10927e34f2ec"
      },
      "id": "8k6U7EB6MD6O",
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: pyarrow in /usr/local/lib/python3.10/dist-packages (14.0.2)\n",
            "Collecting pyarrow\n",
            "  Using cached pyarrow-17.0.0-cp310-cp310-manylinux_2_28_x86_64.whl.metadata (3.3 kB)\n",
            "Requirement already satisfied: numpy>=1.16.6 in /usr/local/lib/python3.10/dist-packages (from pyarrow) (1.26.4)\n",
            "Using cached pyarrow-17.0.0-cp310-cp310-manylinux_2_28_x86_64.whl (39.9 MB)\n",
            "Installing collected packages: pyarrow\n",
            "  Attempting uninstall: pyarrow\n",
            "    Found existing installation: pyarrow 14.0.2\n",
            "    Uninstalling pyarrow-14.0.2:\n",
            "      Successfully uninstalled pyarrow-14.0.2\n",
            "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "cudf-cu11 24.8.2 requires cupy-cuda11x>=12.0.0, which is not installed.\n",
            "cudf-cu11 24.8.2 requires cuda-python<12.0a0,>=11.7.1, but you have cuda-python 12.6.0 which is incompatible.\n",
            "cudf-cu11 24.8.2 requires pyarrow<16.2.0a0,>=16.1.0, but you have pyarrow 17.0.0 which is incompatible.\n",
            "cudf-cu12 24.4.1 requires pyarrow<15.0.0a0,>=14.0.1, but you have pyarrow 17.0.0 which is incompatible.\n",
            "ibis-framework 8.0.0 requires pyarrow<16,>=2, but you have pyarrow 17.0.0 which is incompatible.\u001b[0m\u001b[31m\n",
            "\u001b[0mSuccessfully installed pyarrow-17.0.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "eIA_1XPLiubP",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "1d31b0a2-d0d1-49fa-bcc0-cd9a8675c4df"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "The cudf.pandas extension is already loaded. To reload it, use:\n",
            "  %reload_ext cudf.pandas\n"
          ]
        }
      ],
      "source": [
        "use_gpu = 1\n",
        "if use_gpu == 1:\n",
        "  %load_ext cudf.pandas"
      ],
      "id": "eIA_1XPLiubP"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "IqTNUFXojmjw"
      },
      "outputs": [],
      "source": [
        "\n",
        "#get_ipython().kernel.do_shutdown(restart=True)"
      ],
      "id": "IqTNUFXojmjw"
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Data API\n",
        "\n",
        "\n",
        "1. Yahoo Finance: package yfinance"
      ],
      "metadata": {
        "id": "ekHE23zOdVei"
      },
      "id": "ekHE23zOdVei"
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install numerapi\n",
        "\n",
        "from numerapi import SignalsAPI\n",
        "api = SignalsAPI()\n",
        "api.download_dataset(\n",
        "\t\"signals/v1.0/train.parquet\",\n",
        "\t\"train.parquet\"\n",
        ")\n",
        "api.download_dataset(\n",
        "\t\"signals/v1.0/validation.parquet\",\n",
        "\t\"validation.parquet\"\n",
        ")\n",
        "api.download_dataset(\n",
        "\t\"signals/v1.0/live.parquet\",\n",
        "\t\"live.parquet\"\n",
        ")\n",
        "api.download_dataset(\n",
        "\t\"signals/v1.0/live_example_preds.parquet\",\n",
        "\t\"live_example_preds.parquet\"\n",
        ")\n",
        "api.download_dataset(\n",
        "\t\"signals/v1.0/validation_example_preds.parquet\",\n",
        "\t\"validation_example_preds.parquet\"\n",
        ")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 293
        },
        "id": "7263yx04FM3S",
        "outputId": "e9d4b357-45e5-44f5-d5e6-3424bbfd8b5d"
      },
      "id": "7263yx04FM3S",
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: numerapi in /usr/local/lib/python3.10/dist-packages (2.19.1)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from numerapi) (2.32.3)\n",
            "Requirement already satisfied: pytz in /usr/local/lib/python3.10/dist-packages (from numerapi) (2024.1)\n",
            "Requirement already satisfied: python-dateutil in /usr/local/lib/python3.10/dist-packages (from numerapi) (2.9.0.post0)\n",
            "Requirement already satisfied: tqdm>=4.29.1 in /usr/local/lib/python3.10/dist-packages (from numerapi) (4.66.5)\n",
            "Requirement already satisfied: click>=7.0 in /usr/local/lib/python3.10/dist-packages (from numerapi) (8.1.7)\n",
            "Requirement already satisfied: pandas>=1.1.0 in /usr/local/lib/python3.10/dist-packages (from numerapi) (2.2.1)\n",
            "Requirement already satisfied: numpy<2,>=1.22.4 in /usr/local/lib/python3.10/dist-packages (from pandas>=1.1.0->numerapi) (1.26.4)\n",
            "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.10/dist-packages (from pandas>=1.1.0->numerapi) (2024.1)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.10/dist-packages (from python-dateutil->numerapi) (1.16.0)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests->numerapi) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->numerapi) (3.8)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->numerapi) (2.2.2)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->numerapi) (2024.7.4)\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'validation_example_preds.parquet'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 7
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "### replace Tickers for yahoo download"
      ],
      "metadata": {
        "id": "7wy1XG_8H5Jc"
      },
      "id": "7wy1XG_8H5Jc",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "3rXKsSz6jWBv",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "80b4374b-8c32-4547-dde2-c57fb2e07586"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Revenue not found for ING.AX\n",
            "Revenue not found for BKL.AX\n",
            "Revenue not found for CUV.AX\n",
            "Revenue not found for COH.AX\n",
            "Revenue not found for PXA.AX\n"
          ]
        }
      ],
      "source": [
        "import yfinance as yf\n",
        "\n",
        "def get_revenue(ticker):\n",
        "  \"\"\"\n",
        "  This function takes a ticker symbol and attempts to retrieve revenue data from Yahoo Finance.\n",
        "\n",
        "  Args:\n",
        "      ticker: Ticker symbol for the company on Yahoo Finance (e.g., \"ING.AX\")\n",
        "\n",
        "  Returns:\n",
        "      Revenue in millions as a float or None if not found.\n",
        "  \"\"\"\n",
        "  try:\n",
        "    # Download financial data for the ticker\n",
        "    company_info = yf.Ticker(ticker)\n",
        "    # Access annual financials\n",
        "    financials = company_info.financials\n",
        "    # Check if revenue data is available\n",
        "    if \"Revenue\" in financials:\n",
        "      return financials[\"Revenue\"].iloc[0] / 1e6  # Convert to millions\n",
        "    else:\n",
        "      return None\n",
        "  except (yf.DownloadError, KeyError):\n",
        "    # Handle errors gracefully\n",
        "    return None\n",
        "\n",
        "# Sample usage\n",
        "tickers = [\n",
        "  \"ING.AX\",\n",
        "  \"BKL.AX\",\n",
        "  \"CUV.AX\",\n",
        "  \"COH.AX\",\n",
        "  \"PXA.AX\"\n",
        "]\n",
        "\n",
        "for ticker in tickers:\n",
        "  revenue = get_revenue(ticker)\n",
        "  if revenue:\n",
        "    print(f\"{ticker} revenue: {revenue:.2f} million\")\n",
        "  else:\n",
        "    print(f\"Revenue not found for {ticker}\")"
      ],
      "id": "3rXKsSz6jWBv"
    },
    {
      "cell_type": "code",
      "source": [
        "#!pip install quandl"
      ],
      "metadata": {
        "id": "Pwq2KMAyP95X"
      },
      "id": "Pwq2KMAyP95X",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#import quandl"
      ],
      "metadata": {
        "id": "PZsR-6U9Pghd"
      },
      "id": "PZsR-6U9Pghd",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#!pip install nasdaq-data-link\n",
        "#import nasdaqdatalink"
      ],
      "metadata": {
        "id": "flVCMcxoQVDQ"
      },
      "id": "flVCMcxoQVDQ",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8jGWQKJainKQ",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "ca2d4831-06e5-45ef-afa3-d550e7f5cc4a"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ]
        }
      ],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "id": "8jGWQKJainKQ"
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "rn2wM93Qnv15"
      },
      "id": "rn2wM93Qnv15",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#nasdaqdatalink.read_key(filename=\"/content/drive/MyDrive/Data/API/apikeyndq\")"
      ],
      "metadata": {
        "id": "IoE-O-kNnEn-"
      },
      "id": "IoE-O-kNnEn-",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#data = nasdaqdatalink.get_table('MER/F1', compnumber=[\"39102\" , \"2438\"], paginate=True)"
      ],
      "metadata": {
        "id": "FYm1IXWUmtVD"
      },
      "id": "FYm1IXWUmtVD",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#data2 = nasdaqdatalink.get_table('MER/F1', compnumber=\"39102\", paginate=True)\n",
        "#print(data)\n",
        "#print(data2)"
      ],
      "metadata": {
        "id": "2cK3IASRmd4J"
      },
      "id": "2cK3IASRmd4J",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### CBS Data"
      ],
      "metadata": {
        "id": "HnCg-qMrdE1o"
      },
      "id": "HnCg-qMrdE1o"
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install cbsodata"
      ],
      "metadata": {
        "id": "pvvR5hr6dHUh",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "1027ebec-d971-49f5-d1bb-8c5a8ceb9d40"
      },
      "id": "pvvR5hr6dHUh",
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: cbsodata in /usr/local/lib/python3.10/dist-packages (1.3.5)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from cbsodata) (2.32.3)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests->cbsodata) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->cbsodata) (3.8)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->cbsodata) (2.2.2)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->cbsodata) (2024.7.4)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "### Use data"
      ],
      "metadata": {
        "id": "c4UwyG8rM9l5"
      },
      "id": "c4UwyG8rM9l5",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install --upgrade numpy\n",
        "!pip install scikit-learn"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ahFwvEh_OUsp",
        "outputId": "ab3d3d1f-1aa0-4feb-bc25-a74e79957798"
      },
      "id": "ahFwvEh_OUsp",
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: numpy in /usr/local/lib/python3.10/dist-packages (1.26.4)\n",
            "Collecting numpy\n",
            "  Using cached numpy-2.1.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (60 kB)\n",
            "Using cached numpy-2.1.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (16.3 MB)\n",
            "Installing collected packages: numpy\n",
            "  Attempting uninstall: numpy\n",
            "    Found existing installation: numpy 1.26.4\n",
            "    Uninstalling numpy-1.26.4:\n",
            "      Successfully uninstalled numpy-1.26.4\n",
            "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "accelerate 0.32.1 requires numpy<2.0.0,>=1.17, but you have numpy 2.1.0 which is incompatible.\n",
            "albucore 0.0.13 requires numpy<2,>=1.24.4, but you have numpy 2.1.0 which is incompatible.\n",
            "arviz 0.18.0 requires numpy<2.0,>=1.23.0, but you have numpy 2.1.0 which is incompatible.\n",
            "cudf-cu11 24.8.2 requires numpy<2.0a0,>=1.23, but you have numpy 2.1.0 which is incompatible.\n",
            "cupy-cuda12x 12.2.0 requires numpy<1.27,>=1.20, but you have numpy 2.1.0 which is incompatible.\n",
            "gensim 4.3.3 requires numpy<2.0,>=1.18.5, but you have numpy 2.1.0 which is incompatible.\n",
            "ibis-framework 8.0.0 requires numpy<2,>=1, but you have numpy 2.1.0 which is incompatible.\n",
            "ibis-framework 8.0.0 requires pyarrow<16,>=2, but you have pyarrow 16.1.0 which is incompatible.\n",
            "numba 0.60.0 requires numpy<2.1,>=1.22, but you have numpy 2.1.0 which is incompatible.\n",
            "pandas 2.2.1 requires numpy<2,>=1.22.4; python_version < \"3.11\", but you have numpy 2.1.0 which is incompatible.\n",
            "panel 1.4.5 requires bokeh<3.5.0,>=3.4.0, but you have bokeh 3.5.2 which is incompatible.\n",
            "rmm-cu11 24.8.2 requires numpy<2.0a0,>=1.23, but you have numpy 2.1.0 which is incompatible.\n",
            "rmm-cu12 24.4.0 requires cuda-python<13.0a0,>=12.0, but you have cuda-python 11.8.3 which is incompatible.\n",
            "rmm-cu12 24.4.0 requires numpy<2.0a0,>=1.23, but you have numpy 2.1.0 which is incompatible.\n",
            "scikit-learn 1.3.2 requires numpy<2.0,>=1.17.3, but you have numpy 2.1.0 which is incompatible.\n",
            "tensorflow 2.17.0 requires numpy<2.0.0,>=1.23.5; python_version <= \"3.11\", but you have numpy 2.1.0 which is incompatible.\n",
            "thinc 8.2.5 requires numpy<2.0.0,>=1.19.0; python_version >= \"3.9\", but you have numpy 2.1.0 which is incompatible.\n",
            "transformers 4.42.4 requires numpy<2.0,>=1.17, but you have numpy 2.1.0 which is incompatible.\u001b[0m\u001b[31m\n",
            "\u001b[0mSuccessfully installed numpy-2.1.0\n",
            "Requirement already satisfied: scikit-learn in /usr/local/lib/python3.10/dist-packages (1.3.2)\n",
            "Collecting numpy<2.0,>=1.17.3 (from scikit-learn)\n",
            "  Using cached numpy-1.26.4-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (61 kB)\n",
            "Requirement already satisfied: scipy>=1.5.0 in /usr/local/lib/python3.10/dist-packages (from scikit-learn) (1.13.1)\n",
            "Requirement already satisfied: joblib>=1.1.1 in /usr/local/lib/python3.10/dist-packages (from scikit-learn) (1.4.2)\n",
            "Requirement already satisfied: threadpoolctl>=2.0.0 in /usr/local/lib/python3.10/dist-packages (from scikit-learn) (3.5.0)\n",
            "Using cached numpy-1.26.4-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (18.2 MB)\n",
            "Installing collected packages: numpy\n",
            "  Attempting uninstall: numpy\n",
            "    Found existing installation: numpy 2.1.0\n",
            "    Uninstalling numpy-2.1.0:\n",
            "      Successfully uninstalled numpy-2.1.0\n",
            "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "ibis-framework 8.0.0 requires pyarrow<16,>=2, but you have pyarrow 16.1.0 which is incompatible.\n",
            "panel 1.4.5 requires bokeh<3.5.0,>=3.4.0, but you have bokeh 3.5.2 which is incompatible.\n",
            "rmm-cu12 24.4.0 requires cuda-python<13.0a0,>=12.0, but you have cuda-python 11.8.3 which is incompatible.\u001b[0m\u001b[31m\n",
            "\u001b[0mSuccessfully installed numpy-1.26.4\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "635c8850"
      },
      "outputs": [],
      "source": [
        "# This Python 3 environment comes with many helpful analytics libraries installed\n",
        "import timeit\n",
        "import numerapi\n",
        "import os\n",
        "import numpy as np # linear algebra\n",
        "import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n",
        "import gc\n",
        "import pathlib\n",
        "from tqdm.auto import tqdm\n",
        "import joblib\n",
        "import json\n",
        "import scipy.stats as stats\n",
        "#from sklearn.decomposition import PCA, FactorAnalysis\n",
        "#from sklearn.preprocessing import StandardScaler, MinMaxScaler, LabelEncoder\n",
        "#from sklearn import preprocessing\n",
        "from multiprocessing import Pool, cpu_count\n",
        "import time\n",
        "import requests as re\n",
        "from datetime import datetime\n",
        "from dateutil.relativedelta import relativedelta, FR\n",
        "# Execution time of the model start\n",
        "startnb = datetime.now()\n",
        "#import dask.dataframe as dd\n",
        "#from sklearn.preprocessing import LabelEncoder\n",
        "import tracemalloc as tm\n",
        "import os\n",
        "\n",
        "# models\n",
        "#from sklearn import utils, metrics\n",
        "#from sklearn.model_selection import GridSearchCV, train_test_split,cross_val_score\n",
        "import xgboost as xgb\n",
        "from xgboost.sklearn import XGBClassifier\n",
        "import lightgbm as lgb\n",
        "import operator\n",
        "\n",
        "# visualize\n",
        "import matplotlib.pyplot as plt\n",
        "import matplotlib.style as style\n",
        "from matplotlib_venn import venn2, venn3\n",
        "import seaborn as sns\n",
        "from matplotlib import pyplot\n",
        "from matplotlib.ticker import ScalarFormatter\n",
        "import graphviz\n",
        "from xgboost import plot_tree\n",
        "sns.set_context(\"talk\")\n",
        "style.use('seaborn-colorblind')\n",
        "\n",
        "import warnings\n",
        "warnings.simplefilter('ignore')"
      ],
      "id": "635c8850"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "5e9686ae-2b99-4ae3-9bc8-569cf564c8e6",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "523ff5a3-746c-48fe-87e3-54f19461d10f"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[neptune] [warning] NeptuneDeprecationWarning: You're importing the Neptune client library via the deprecated `neptune.new` module, which will be removed in a future release. Import directly from `neptune` instead.\n"
          ]
        }
      ],
      "source": [
        "import neptune as neptune\n",
        "from neptune.new.integrations.xgboost import NeptuneCallback\n",
        "neptune_switch = 0 #0 is off\n",
        "if neptune_switch == 1:\n",
        "    run = neptune.init(\n",
        "        project=\"develuse/Signals\",\n",
        "        api_token=\"eyJhcGlfYWRkcmVzcyI6Imh0dHBzOi8vYXBwLm5lcHR1bmUuYWkiLCJhcGlfdXJsIjoiaHR0cHM6Ly9hcHAubmVwdHVuZS5haSIsImFwaV9rZXkiOiIwM2Q3YWQ5MC0zZDA2LTRlOTgtOWRmMC1lNzc0NjIwYjUwZTgifQ==\",\n",
        "    )  # your credentials"
      ],
      "id": "5e9686ae-2b99-4ae3-9bc8-569cf564c8e6"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7856214c"
      },
      "source": [
        "# Config\n",
        "A simple config and logging setup.\n",
        "To update the Kaggle dataset set DATASET to 1 in config"
      ],
      "id": "7856214c"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "3b2f70ee",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "outputId": "30d8f0c2-8f7f-4c1e-d719-68fd2adfabdc"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'2024-08-30'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 22
        }
      ],
      "source": [
        "today = datetime.now().strftime('%Y-%m-%d')\n",
        "today"
      ],
      "id": "3b2f70ee"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "5b3946c1-9a72-4588-9910-fe9b7ec821df",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "outputId": "13a6c2b9-0709-4c33-ea2f-410b9000b10c"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'20240830'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 23
        }
      ],
      "source": [
        "today_yyyymmdd = datetime.now().strftime('%Y%m%d')\n",
        "today_yyyymmdd"
      ],
      "id": "5b3946c1-9a72-4588-9910-fe9b7ec821df"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "dc19ab09"
      },
      "outputs": [],
      "source": [
        "# config class\n",
        "class CFG:\n",
        "    \"\"\"\n",
        "    Set FETCH_VIA_API = True if you want to fetch the data via API.\n",
        "    Otherwise we use the daily-updated one in the kaggle dataset (faster).\n",
        "    \"\"\"\n",
        "    INPUT_DIR = '../../Signals/Data/'\n",
        "    #INPUT_DIR = '../input/yfinance-stock-price-data-for-numerai-signals/' #Kaggle\n",
        "    OUTPUT_DIR = '../../Signals/Submission/'\n",
        "    LOG = 0\n",
        "    DATASET = 1 #use in Colab, upload Kaggle api key an duse to update Kaggle dataset to Drive\n",
        "    EDS_DATE = 20231010\n",
        "    FUN_DATE = 20230929\n",
        "    FETCH_VIA_API = False\n",
        "    SEED = 46\n",
        "    DEBUG = False # True, test mode using small set of tickers\n",
        "    RUN = \"lgb_4D_10y_testpoly\"\n",
        "    DS_OVERRIDE = 1\n",
        "    DS = 629\n",
        "    TYPE = \"C\" #K:Katsu, E:EDS, G:Kaggle Katsu, C:Colab Katsu, CKK: Colab Kaggle Katsu\n",
        "    START = 634 #100\n",
        "    END = 1980\n",
        "    STARTDATE = 20130101\n",
        "    ENDDATE = today_yyyymmdd\n",
        "    XGB_DEPTH = 6 #was 5 #check if its faster as 14\n",
        "    XGB = 0\n",
        "    LGBM = 1\n",
        "    H2O = 0\n",
        "    H2O_RUNTIME = 6000 #600/10min 24000/6h40m\n",
        "    H2O_ALGOS = ['XGBoost','StackedEnsemble','GBM']#, 'DeepLearning', 'DRF', 'GLM', ,\n",
        "                   #include_algos = ['DRF', 'GLM', 'XGBoost', 'GBM', 'DeepLearning'],\n",
        "    ANALYSE = 1 # a lot of prints, lgbm nonrandom feature df parquet, histogram, validation\n",
        "    LEAN = 1 # for deleting variables along the way, 1 deletes them all\n",
        "#3800 for sharpe 6\n",
        "#round562 node2 100-4300 (to stay within 1:40 hours) 2011 stocks\n",
        "#round565 node2: 634-4300 rmse 0.239; 5000\n",
        "#569node2: 5300\n",
        "#573node2: 9:20 start\n",
        "#574node2: 12:22 start, hang 4h30m on xgb\n",
        "#575node2: 6150 with 1994 hangs on 1.4tb; 6040 worked;\n",
        "#578node2: 4800 #579node2: 5353 2700 stocks; 5453\n",
        "#580node2: K5390 ok;\n",
        "#581node2: start 11:30 13:52 kernel died; 14:12 start with 5460->5350 17:06:05 complete; 20:35 5430\n",
        "#583node2: 5425 kernel died; 5400 start 13:10  kerneld died at 500GB? 14:40; 5300 kernel died; 5200 15:05\n",
        "#589node2: 6535 19:05\n",
        "#590node2: 5435 kernel died; K5395 OK; K5405 OK; K5415\n",
        "#593node2: 5431 kernel died; 5429\n",
        "#594node2: 1.5144TBram K5479 OK\n",
        "#599node2: 5495 server reset; 5480"
      ],
      "id": "dc19ab09"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "GGsR4KBDubuc",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 429
        },
        "outputId": "f05adf9f-a1d7-45b4-b57a-c3bfbebaa0fd"
      },
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "\n",
              "     <input type=\"file\" id=\"files-eb051ca8-8d71-47f8-aac0-8ef16d6ba8e6\" name=\"files[]\" multiple disabled\n",
              "        style=\"border:none\" />\n",
              "     <output id=\"result-eb051ca8-8d71-47f8-aac0-8ef16d6ba8e6\">\n",
              "      Upload widget is only available when the cell has been executed in the\n",
              "      current browser session. Please rerun this cell to enable.\n",
              "      </output>\n",
              "      <script>// Copyright 2017 Google LLC\n",
              "//\n",
              "// Licensed under the Apache License, Version 2.0 (the \"License\");\n",
              "// you may not use this file except in compliance with the License.\n",
              "// You may obtain a copy of the License at\n",
              "//\n",
              "//      http://www.apache.org/licenses/LICENSE-2.0\n",
              "//\n",
              "// Unless required by applicable law or agreed to in writing, software\n",
              "// distributed under the License is distributed on an \"AS IS\" BASIS,\n",
              "// WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n",
              "// See the License for the specific language governing permissions and\n",
              "// limitations under the License.\n",
              "\n",
              "/**\n",
              " * @fileoverview Helpers for google.colab Python module.\n",
              " */\n",
              "(function(scope) {\n",
              "function span(text, styleAttributes = {}) {\n",
              "  const element = document.createElement('span');\n",
              "  element.textContent = text;\n",
              "  for (const key of Object.keys(styleAttributes)) {\n",
              "    element.style[key] = styleAttributes[key];\n",
              "  }\n",
              "  return element;\n",
              "}\n",
              "\n",
              "// Max number of bytes which will be uploaded at a time.\n",
              "const MAX_PAYLOAD_SIZE = 100 * 1024;\n",
              "\n",
              "function _uploadFiles(inputId, outputId) {\n",
              "  const steps = uploadFilesStep(inputId, outputId);\n",
              "  const outputElement = document.getElementById(outputId);\n",
              "  // Cache steps on the outputElement to make it available for the next call\n",
              "  // to uploadFilesContinue from Python.\n",
              "  outputElement.steps = steps;\n",
              "\n",
              "  return _uploadFilesContinue(outputId);\n",
              "}\n",
              "\n",
              "// This is roughly an async generator (not supported in the browser yet),\n",
              "// where there are multiple asynchronous steps and the Python side is going\n",
              "// to poll for completion of each step.\n",
              "// This uses a Promise to block the python side on completion of each step,\n",
              "// then passes the result of the previous step as the input to the next step.\n",
              "function _uploadFilesContinue(outputId) {\n",
              "  const outputElement = document.getElementById(outputId);\n",
              "  const steps = outputElement.steps;\n",
              "\n",
              "  const next = steps.next(outputElement.lastPromiseValue);\n",
              "  return Promise.resolve(next.value.promise).then((value) => {\n",
              "    // Cache the last promise value to make it available to the next\n",
              "    // step of the generator.\n",
              "    outputElement.lastPromiseValue = value;\n",
              "    return next.value.response;\n",
              "  });\n",
              "}\n",
              "\n",
              "/**\n",
              " * Generator function which is called between each async step of the upload\n",
              " * process.\n",
              " * @param {string} inputId Element ID of the input file picker element.\n",
              " * @param {string} outputId Element ID of the output display.\n",
              " * @return {!Iterable<!Object>} Iterable of next steps.\n",
              " */\n",
              "function* uploadFilesStep(inputId, outputId) {\n",
              "  const inputElement = document.getElementById(inputId);\n",
              "  inputElement.disabled = false;\n",
              "\n",
              "  const outputElement = document.getElementById(outputId);\n",
              "  outputElement.innerHTML = '';\n",
              "\n",
              "  const pickedPromise = new Promise((resolve) => {\n",
              "    inputElement.addEventListener('change', (e) => {\n",
              "      resolve(e.target.files);\n",
              "    });\n",
              "  });\n",
              "\n",
              "  const cancel = document.createElement('button');\n",
              "  inputElement.parentElement.appendChild(cancel);\n",
              "  cancel.textContent = 'Cancel upload';\n",
              "  const cancelPromise = new Promise((resolve) => {\n",
              "    cancel.onclick = () => {\n",
              "      resolve(null);\n",
              "    };\n",
              "  });\n",
              "\n",
              "  // Wait for the user to pick the files.\n",
              "  const files = yield {\n",
              "    promise: Promise.race([pickedPromise, cancelPromise]),\n",
              "    response: {\n",
              "      action: 'starting',\n",
              "    }\n",
              "  };\n",
              "\n",
              "  cancel.remove();\n",
              "\n",
              "  // Disable the input element since further picks are not allowed.\n",
              "  inputElement.disabled = true;\n",
              "\n",
              "  if (!files) {\n",
              "    return {\n",
              "      response: {\n",
              "        action: 'complete',\n",
              "      }\n",
              "    };\n",
              "  }\n",
              "\n",
              "  for (const file of files) {\n",
              "    const li = document.createElement('li');\n",
              "    li.append(span(file.name, {fontWeight: 'bold'}));\n",
              "    li.append(span(\n",
              "        `(${file.type || 'n/a'}) - ${file.size} bytes, ` +\n",
              "        `last modified: ${\n",
              "            file.lastModifiedDate ? file.lastModifiedDate.toLocaleDateString() :\n",
              "                                    'n/a'} - `));\n",
              "    const percent = span('0% done');\n",
              "    li.appendChild(percent);\n",
              "\n",
              "    outputElement.appendChild(li);\n",
              "\n",
              "    const fileDataPromise = new Promise((resolve) => {\n",
              "      const reader = new FileReader();\n",
              "      reader.onload = (e) => {\n",
              "        resolve(e.target.result);\n",
              "      };\n",
              "      reader.readAsArrayBuffer(file);\n",
              "    });\n",
              "    // Wait for the data to be ready.\n",
              "    let fileData = yield {\n",
              "      promise: fileDataPromise,\n",
              "      response: {\n",
              "        action: 'continue',\n",
              "      }\n",
              "    };\n",
              "\n",
              "    // Use a chunked sending to avoid message size limits. See b/62115660.\n",
              "    let position = 0;\n",
              "    do {\n",
              "      const length = Math.min(fileData.byteLength - position, MAX_PAYLOAD_SIZE);\n",
              "      const chunk = new Uint8Array(fileData, position, length);\n",
              "      position += length;\n",
              "\n",
              "      const base64 = btoa(String.fromCharCode.apply(null, chunk));\n",
              "      yield {\n",
              "        response: {\n",
              "          action: 'append',\n",
              "          file: file.name,\n",
              "          data: base64,\n",
              "        },\n",
              "      };\n",
              "\n",
              "      let percentDone = fileData.byteLength === 0 ?\n",
              "          100 :\n",
              "          Math.round((position / fileData.byteLength) * 100);\n",
              "      percent.textContent = `${percentDone}% done`;\n",
              "\n",
              "    } while (position < fileData.byteLength);\n",
              "  }\n",
              "\n",
              "  // All done.\n",
              "  yield {\n",
              "    response: {\n",
              "      action: 'complete',\n",
              "    }\n",
              "  };\n",
              "}\n",
              "\n",
              "scope.google = scope.google || {};\n",
              "scope.google.colab = scope.google.colab || {};\n",
              "scope.google.colab._files = {\n",
              "  _uploadFiles,\n",
              "  _uploadFilesContinue,\n",
              "};\n",
              "})(self);\n",
              "</script> "
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "cp: cannot stat 'kaggle.json': No such file or directory\n",
            "chmod: cannot access '/root/.kaggle/kaggle.json': No such file or directory\n",
            "403 - Forbidden - Permission 'datasets.get' was denied\n",
            "Dataset URL: https://www.kaggle.com/datasets/code1110/yfinance-stock-price-data-for-numerai-signals\n",
            "License(s): other\n",
            "Downloading yfinance-stock-price-data-for-numerai-signals.zip to /content\n",
            " 99% 447M/451M [00:15<00:00, 27.9MB/s]\n",
            "100% 451M/451M [00:15<00:00, 29.8MB/s]\n",
            "Mounted at /content/drive/\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "FileNotFoundError",
          "evalue": "[Errno 2] No such file or directory: 'signals-326.zip'",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<timed exec>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n",
            "\u001b[0;32m/usr/lib/python3.10/zipfile.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, file, mode, compression, allowZip64, compresslevel, strict_timestamps)\u001b[0m\n\u001b[1;32m   1251\u001b[0m             \u001b[0;32mwhile\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1252\u001b[0m                 \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1253\u001b[0;31m                     \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfp\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mio\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfile\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfilemode\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1254\u001b[0m                 \u001b[0;32mexcept\u001b[0m \u001b[0mOSError\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1255\u001b[0m                     \u001b[0;32mif\u001b[0m \u001b[0mfilemode\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mmodeDict\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: 'signals-326.zip'"
          ]
        }
      ],
      "source": [
        "%%time\n",
        "'''\n",
        "if CFG.DATASET == 1:\n",
        "    !pip install -q kaggle\n",
        "    from google.colab import files\n",
        "    files.upload()\n",
        "    !mkdir ~/.kaggle\n",
        "    !cp kaggle.json ~/.kaggle/\n",
        "    !chmod 600 ~/.kaggle/kaggle.json\n",
        "    !kaggle datasets download -d develuse/signals-326 --force\n",
        "    !kaggle datasets download -d code1110/yfinance-stock-price-data-for-numerai-signals\n",
        "    from google.colab import drive\n",
        "    drive.mount('/content/drive/', force_remount=True)\n",
        "    import zipfile\n",
        "    zip_ref = zipfile.ZipFile('signals-326.zip', 'r')\n",
        "    zip_ref.extractall('/content/drive/MyDrive/Kaggle/Signals')\n",
        "    zip_ref = zipfile.ZipFile('yfinance-stock-price-data-for-numerai-signals.zip', 'r')\n",
        "    zip_ref.extractall('/content/drive/MyDrive/Signals/Data/Market')\n",
        "    zip_ref.close()\n",
        "    #TODO: rename full_data to full_data{current_ds}\n",
        "    !mv /content/drive/MyDrive/Signals/Data/Market/full_data.parquet /content/drive/MyDrive/Signals/Data/Market/{CFG.DS}.parquet\n",
        "''\n",
        "#614kaggle: 2m1s\n",
        "#615kaggle: 2m32s; 629 1m13s"
      ],
      "id": "GGsR4KBDubuc"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8edbb327",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "4f3b45c3-2df3-4ee5-f327-e22183758924"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "CPU times: user 9 µs, sys: 0 ns, total: 9 µs\n",
            "Wall time: 13.8 µs\n"
          ]
        }
      ],
      "source": [
        "%%time\n",
        "# Logging is always nice for your experiment:)\n",
        "def init_logger(log_file='train.log'):\n",
        "    from logging import getLogger, INFO, FileHandler,  Formatter,  StreamHandler\n",
        "    logger = getLogger(__name__)\n",
        "    logger.setLevel(INFO)\n",
        "    handler1 = StreamHandler()\n",
        "    handler1.setFormatter(Formatter(\"%(message)s\"))\n",
        "    handler2 = FileHandler(filename=log_file)\n",
        "    handler2.setFormatter(Formatter(\"%(message)s\"))\n",
        "    logger.addHandler(handler1)\n",
        "    logger.addHandler(handler2)\n",
        "    return logger\n",
        "if CFG.LOG == 1:\n",
        "    logger = init_logger(log_file=f'{CFG.OUTPUT_DIR}/log/{today}.log')\n",
        "    logger.info('Start Logging...')\n",
        "#581: 4h16m 2023-09-27 21:26:59 - 2023-09-28 01:45:08\n",
        "#583:  2023-09-29 15:05:05 -\n",
        "#589: 17:51:39 20:05:04 2h 13m; 10:24:06 - 04:34:21 6h10m\n",
        "#590node2: 5 min till here first run after boot, 11:01:28 - 13:00 2h\n",
        "#590node2: K5395 14:32:07 15:56:16 1h25m9s; 16:11:33 - 17:48:36 1h37m3s\n",
        "# 19:19:40 - 20:50:47 1h31m7s"
      ],
      "id": "8edbb327"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9d2aa429"
      },
      "source": [
        "# Setup Numerai API\n",
        "First of all, let's set up the numerai signals API.\n",
        "\n",
        "We can do many things with this API:\n",
        "\n",
        "- get a ticker map (between yfinance data and numerai historical targets)\n",
        "- get the historical targets\n",
        "- get your model slot name and model_id (if private key and secret key are provided)\n",
        "- submit\n",
        "\n",
        "(well, maybe more)"
      ],
      "id": "9d2aa429"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "c8189c34"
      },
      "source": [
        "## Get Tickers for Numerai Signals\n",
        "Let's first get the ticker map."
      ],
      "id": "c8189c34"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "260e6439",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "ead582af-96c5-4844-baff-314e9375aec0"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "CPU times: user 70 µs, sys: 0 ns, total: 70 µs\n",
            "Wall time: 73.9 µs\n"
          ]
        }
      ],
      "source": [
        "%%time\n",
        "napi = numerapi.SignalsAPI()\n",
        "if CFG.LOG == 1:\n",
        "    logger.info('numerai api setup!')"
      ],
      "id": "260e6439"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "43a0c188",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "5bb479ca-e543-4edc-8dd6-2c13818b6a89"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "signals/v1.0/live.parquet: 721kB [00:00, 1.02MB/s]                          \n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "CPU times: user 96.2 ms, sys: 13.2 ms, total: 109 ms\n",
            "Wall time: 2 s\n"
          ]
        }
      ],
      "source": [
        "%%time\n",
        "# read in list of active Signals tickers which can change slightly era to era\n",
        "internet=1\n",
        "if internet==1: #1: internet available\n",
        "    eligible_tickers = pd.Series(napi.ticker_universe(), name='ticker')\n",
        "    if CFG.LOG == 1:\n",
        "          logger.info(f\"Number of eligible tickers: {len(eligible_tickers)}\")\n",
        "eligible_tickers.to_csv(f'/content/drive/MyDrive/Signals/Data/eligible_tickers_{today_yyyymmdd}.csv')\n",
        "#590node2: K5400 1.03s"
      ],
      "id": "43a0c188"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "04d4b661",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "f071e5e3-5678-40af-f419-e03ef17b91a8"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "CPU times: user 26.1 ms, sys: 2.03 ms, total: 28.1 ms\n",
            "Wall time: 914 ms\n"
          ]
        }
      ],
      "source": [
        "%%time\n",
        "#TODO: replace this with EDS tickermap\n",
        "# read in yahoo to numerai ticker map, still a work in progress, h/t wsouza and\n",
        "# this tickermap is a work in progress and not guaranteed to be 100% correct\n",
        "if internet==1:\n",
        "    ticker_map = pd.read_csv('https://numerai-signals-public-data.s3-us-west-2.amazonaws.com/signals_ticker_map_w_bbg.csv')\n",
        "    ticker_map = ticker_map[ticker_map.bloomberg_ticker.isin(eligible_tickers)]\n",
        "#else:\n",
        "    #ticker_map = pd.read_parquet('ticker_map.parquet')\n",
        "#print(ticker_map.shape)\n",
        "#ticker_map.head()\n",
        "numerai_tickers = ticker_map['ticker']\n",
        "yfinance_tickers = ticker_map['yahoo']\n",
        "#logger.info(f\"Number of eligible tickers in map: {len(ticker_map)}\")\n",
        "#590node2: 1.23s"
      ],
      "id": "04d4b661"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "e3e686ae"
      },
      "source": [
        "This ticker map is necessary for a successful submission if you use yfinance data."
      ],
      "id": "e3e686ae"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "f2eebc35"
      },
      "source": [
        "# Load Stock Price Data\n",
        "Now is the time to get the stock price data, fetched via the [YFiance API](https://pypi.org/project/yfinance/).\n",
        "\n",
        "The good thing with this API is that it is free of charge.\n",
        "\n",
        "The bad thing with this API is that the data is often not complete.\n",
        "\n",
        "For a better quality of stock price data, you might want to try out purchasing one from [Quandl](https://www.quandl.com/data/EOD-End-of-Day-US-Stock-Prices/documentation?anchor=overview).\n",
        "\n",
        "This is another starter using Quandl data:\n",
        "https://forum.numer.ai/t/signals-plugging-in-the-data-from-quandl/2431\n",
        "\n",
        "This is of course wonderful, but if you are a beginner, why not just start with a free one?"
      ],
      "id": "f2eebc35"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "1a73e3ea",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "b0f5ac47-bae4-41e5-c277-69b10e833ea1"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Current round:  629\n",
            "CPU times: user 2.11 ms, sys: 0 ns, total: 2.11 ms\n",
            "Wall time: 2.06 ms\n"
          ]
        }
      ],
      "source": [
        "%%time\n",
        "napi2 = numerapi.NumerAPI(verbosity=\"info\")\n",
        "if CFG.DS_OVERRIDE == 1:\n",
        "    current_ds=CFG.DS\n",
        "else:\n",
        "    if internet==1:\n",
        "        current_ds = napi2.get_current_round()\n",
        "    else:\n",
        "        current_ds=CFG.DS\n",
        "\n",
        "print('Current round: ',current_ds)"
      ],
      "id": "1a73e3ea"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "54c98e17-0dde-4288-9e4e-54f41f552bf8"
      },
      "source": [
        "## Adapt dataframe for weekday\n",
        "if it is a weekday we need to use the last day instead of the last available friday\n",
        "to do this we\n",
        "we determine the friday date\n",
        "we determine the last weekday\n",
        "we check if we have this last weekday for each friday date\n",
        "*if we don't have a last weekday we need to see if we have other weekdays after the friday date to replace it or keep the friday date\n",
        "remove the friday date from the dataframe\n",
        "replace the last weekday by the friday date\n",
        "replace the original dataframe"
      ],
      "id": "54c98e17-0dde-4288-9e4e-54f41f552bf8"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "c5cebb93-b735-49b9-b827-db22543d52ff"
      },
      "source": [
        "## Data Store with categorical and static data like country GICS shares(place to dynamic later)"
      ],
      "id": "c5cebb93-b735-49b9-b827-db22543d52ff"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "23a0bb4e",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "7351990d-bd0f-40d2-cdf7-5a9d64ac109d"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "CPU times: user 25.4 ms, sys: 7.06 ms, total: 32.5 ms\n",
            "Wall time: 36.1 ms\n"
          ]
        }
      ],
      "source": [
        "%%time\n",
        "if CFG.TYPE == \"C\":\n",
        "  EDS = pd.read_csv(f'/content/drive/MyDrive/Kaggle/Signals/EDS_20230824.csv')\n",
        "elif CFG.TYPE == \"CKK\":\n",
        "  EDS = pd.read_csv(f'/content/drive/MyDrive/Kaggle/Signals/EDS_20230824.csv')\n",
        "else:\n",
        "  EDS = pd.read_csv(f'{CFG.INPUT_DIR}/EDS_{CFG.EDS_DATE}.csv')\n",
        "#EDS = pd.read_csv(f'../input/signals-{current_ds}/EDS_202207.csv') #Kaggle"
      ],
      "id": "23a0bb4e"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "063a0662-97c6-4141-8f3f-b1f3192fbb27"
      },
      "source": [
        "## Data Store with dynamic fundamental data"
      ],
      "id": "063a0662-97c6-4141-8f3f-b1f3192fbb27"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8ae1b025-497a-4bd8-a376-7a616dfd4e14",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "e9af1a23-f87b-4845-a5e5-173443130341"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "CPU times: user 15.7 ms, sys: 893 µs, total: 16.6 ms\n",
            "Wall time: 19.7 ms\n"
          ]
        }
      ],
      "source": [
        "%%time\n",
        "if CFG.TYPE == \"C\":\n",
        "  FUN = pd.read_csv(f'/content/drive/MyDrive/Kaggle/Signals/FUN_20230929.csv')\n",
        "elif CFG.TYPE == \"CKK\":\n",
        "  FUN = pd.read_csv(f'/content/drive/MyDrive/Kaggle/Signals/FUN_20230929.csv')\n",
        "else:\n",
        "  FUN = pd.read_csv(f'{CFG.INPUT_DIR}/Fundamental/FUN_{CFG.FUN_DATE}.csv')"
      ],
      "id": "8ae1b025-497a-4bd8-a376-7a616dfd4e14"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "7f90cf42-d2f8-4069-a3ca-908ab939025a"
      },
      "outputs": [],
      "source": [
        "#TODO: test Write tickers with yahoodata available to file\n",
        "#pd.DataFrame(df.ticker.unique()).to_csv('yahoo_downloaded_tickers.csv')\n",
        "#TODO: check why not all eligible are downloaded"
      ],
      "id": "7f90cf42-d2f8-4069-a3ca-908ab939025a"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2e835337-1117-4ecb-b6a3-c16b350a164a"
      },
      "source": [
        "### Merge EDS with FUN\n"
      ],
      "id": "2e835337-1117-4ecb-b6a3-c16b350a164a"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "52a84042-f25c-4c15-b047-80a91adb4207"
      },
      "source": [
        "### Only merge with timeperiods 45 days after end of period else it is foreward looking bias"
      ],
      "id": "52a84042-f25c-4c15-b047-80a91adb4207"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "2250fea5-6ea6-4a37-82b9-7e968a45a92a",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "bc1e7744-4360-467d-e955-1f8f94b2ef14"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "  bloomberg_ticker  Active                              Name website  \\\n",
            "0          VTOL US     1.0   https://www.bloomberg.com/quote/VTOL:US   \n",
            "1           OFG US     1.0    https://www.bloomberg.com/quote/OFG:US   \n",
            "2          LISN SW     1.0   https://www.bloomberg.com/quote/LISN:SW   \n",
            "3          6289 JP     0.0   https://www.bloomberg.com/quote/6289:JP   \n",
            "4         FCNCA US     1.0  https://www.bloomberg.com/quote/FCNCA:US   \n",
            "\n",
            "  BB_exch_code                         Name  GICS_subindustry       shares  \\\n",
            "0           US                BRISTOW GROUP          10101020   27,983,000   \n",
            "1           US                  OFG BANCORP          40101015  183,900,000   \n",
            "2           SW               LINDTSPRUENGLI          30202030    159300000   \n",
            "3           JP                        Giken          20106020            0   \n",
            "4           US  FIRST CITIZENS BCSHS  -CL A          40201040  160,400,000   \n",
            "\n",
            "   shares_update_date   shares_source Country_of_domicile  ...  \\\n",
            "0          20230820.0  company_report                  US  ...   \n",
            "1          20230820.0  company_report                  PR  ...   \n",
            "2          20230820.0  company_report                  CH  ...   \n",
            "3          20230820.0            news                  JP  ...   \n",
            "4          20230820.0  company_report                  US  ...   \n",
            "\n",
            "  earningsQuarterlyGrowth_raw revenueGrowth_raw grossMargins_raw  \\\n",
            "0                         NaN             0.059          0.23200   \n",
            "1                       0.093             0.070          0.00000   \n",
            "2                       0.478             0.047          0.67074   \n",
            "3                         NaN            -0.044          0.37272   \n",
            "4                       1.675             1.230          0.00000   \n",
            "\n",
            "  profitMargins_raw payoutRatio_raw dividendRate_raw dividendYield_raw  \\\n",
            "0           0.01234          0.0000              NaN               NaN   \n",
            "1           0.28829          0.2246             0.88            0.0295   \n",
            "2           0.12482          0.4863          1300.00            0.0125   \n",
            "3           0.04080          1.2032            40.00            0.0198   \n",
            "4           2.05808          0.0037             3.00            0.0023   \n",
            "\n",
            "  fiveYearAvgDividendYield_raw trailingAnnualDividendRate_raw  \\\n",
            "0                          NaN                           0.00   \n",
            "1                         1.79                           0.84   \n",
            "2                         1.18                        1300.00   \n",
            "3                         1.86                          55.00   \n",
            "4                         0.30                           2.72   \n",
            "\n",
            "  trailingAnnualDividendYield_raw  \n",
            "0                        0.000000  \n",
            "1                        0.028254  \n",
            "2                        0.012476  \n",
            "3                        0.026596  \n",
            "4                        0.002055  \n",
            "\n",
            "[5 rows x 40 columns]\n",
            "Index(['bloomberg_ticker', 'Active', 'Name website', 'BB_exch_code', 'Name',\n",
            "       'GICS_subindustry', 'shares', 'shares_update_date', 'shares_source',\n",
            "       'Country_of_domicile', 'Country_of_risk_name', 'yahoo', 'ISIN_code',\n",
            "       'ticker', 'SEDOL', 'ISIN', 'Trim Name', 'Sedol_sec_name', 'Descriptive',\n",
            "       'Currency', 'Bankrupt', 'Acquired', 'Trading_suspended', 'Acquired by',\n",
            "       'Clients', 'collect_date', 'sector', 'industry', 'priceToBook_raw',\n",
            "       'lastFiscalYearEnd_raw', 'earningsQuarterlyGrowth_raw',\n",
            "       'revenueGrowth_raw', 'grossMargins_raw', 'profitMargins_raw',\n",
            "       'payoutRatio_raw', 'dividendRate_raw', 'dividendYield_raw',\n",
            "       'fiveYearAvgDividendYield_raw', 'trailingAnnualDividendRate_raw',\n",
            "       'trailingAnnualDividendYield_raw'],\n",
            "      dtype='object')\n",
            "CPU times: user 18.5 ms, sys: 3.08 ms, total: 21.5 ms\n",
            "Wall time: 21.3 ms\n"
          ]
        }
      ],
      "source": [
        "%%time\n",
        "FUN.rename(columns={\"ticker\": \"yahoo\"}, inplace=True)\n",
        "EDS.rename(columns={\"yahoo identifier\": \"yahoo\"}, inplace=True)\n",
        "EDS = EDS.merge(FUN,how='left', on=['yahoo'])\n",
        "EDS.columns = EDS.columns.str.replace(\".\", \"_\")\n",
        "if CFG.ANALYSE == 1:\n",
        "    print(EDS.head())\n",
        "    print(EDS.columns)"
      ],
      "id": "2250fea5-6ea6-4a37-82b9-7e968a45a92a"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1e4ba46a"
      },
      "source": [
        "## Load Targets for Numerai Signals\n",
        "For a supervised machine learning, we need a target label. That is available in the Numerai Signals, so we can just fetch it.\n",
        "\n",
        "Note that there are 2 target columns: 'target' and 'target_20d'. 'target' is often referred as 'target_4d', which is a shorter target.\n",
        "\n",
        "Values for target_4d and target_20d become available after they have resolved, 11 and 33 days respectively from round open. target_20d takes longer to resolve, and so the most recent dates will have a value of NaN for target_20d, while target_4d will not.\n",
        "\n",
        "target_20d is what your Signal is evaluated against for scoring and payouts, so we use 'target_20d' in this starter notebook."
      ],
      "id": "1e4ba46a"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "1b399d54",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "f4117107-a01c-4a8d-a948-9bded1722b5f"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "              0\n",
            "0     000080 KR\n",
            "1     000100 KR\n",
            "2     000120 KR\n",
            "3     000150 KR\n",
            "4     000210 KR\n",
            "...         ...\n",
            "4891      ZV IT\n",
            "4892    ZVRA US\n",
            "4893     ZWS US\n",
            "4894    ZYME US\n",
            "4895     ZZZ CA\n",
            "\n",
            "[4896 rows x 1 columns]\n",
            "CPU times: user 14.6 ms, sys: 1.9 ms, total: 16.5 ms\n",
            "Wall time: 17.2 ms\n"
          ]
        }
      ],
      "source": [
        "%%time\n",
        "\n",
        "targets = pd.read_parquet('live_example_preds.parquet')\n",
        "tickers = targets.numerai_ticker\n",
        "tickers_unique = tickers.unique()\n",
        "tickers_unique = pd.DataFrame(tickers_unique)\n",
        "print(tickers_unique)\n",
        "tickers_unique.to_csv('tickers_unique_live.csv')\n",
        "#4min58s\n",
        "#round369 node3: 15.8GB 5min31s\n",
        "#round549 node1: 54s\n",
        "#568node1: 4m30s\n",
        "#568node1: TODO: URLError: <urlopen error [Errno -3] Temporary failure in name resolution> 24m55s\n",
        "#569node1: 2m2s\n",
        "#572node2: took too long\n",
        "#573node2: 3min46s\n",
        "#583node2: 1m46s\n",
        "#589node2: 2m27s\n",
        "#590node2: 1m51s"
      ],
      "id": "1b399d54"
    },
    {
      "cell_type": "code",
      "source": [
        "targets = pd.read_parquet('live_example_preds.parquet')\n",
        "\n",
        "targets.to_csv('live_example_preds.csv')"
      ],
      "metadata": {
        "id": "MMCr398BmeAK"
      },
      "id": "MMCr398BmeAK",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### download targets"
      ],
      "metadata": {
        "id": "xfd8npsoXzKE"
      },
      "id": "xfd8npsoXzKE"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "b8933f18",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 571
        },
        "outputId": "58ab4436-6fc4-4f66-cb1e-f5f391583209"
      },
      "outputs": [
        {
          "output_type": "error",
          "ename": "KeyError",
          "evalue": "'ticker'",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/pandas/core/indexes/base.py\u001b[0m in \u001b[0;36mget_loc\u001b[0;34m(self, key)\u001b[0m\n\u001b[1;32m   3804\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 3805\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_engine\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_loc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcasted_key\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   3806\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mKeyError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0merr\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32mindex.pyx\u001b[0m in \u001b[0;36mpandas._libs.index.IndexEngine.get_loc\u001b[0;34m()\u001b[0m\n",
            "\u001b[0;32mindex.pyx\u001b[0m in \u001b[0;36mpandas._libs.index.IndexEngine.get_loc\u001b[0;34m()\u001b[0m\n",
            "\u001b[0;32mpandas/_libs/hashtable_class_helper.pxi\u001b[0m in \u001b[0;36mpandas._libs.hashtable.PyObjectHashTable.get_item\u001b[0;34m()\u001b[0m\n",
            "\u001b[0;32mpandas/_libs/hashtable_class_helper.pxi\u001b[0m in \u001b[0;36mpandas._libs.hashtable.PyObjectHashTable.get_item\u001b[0;34m()\u001b[0m\n",
            "\u001b[0;31mKeyError\u001b[0m: 'ticker'",
            "\nThe above exception was the direct cause of the following exception:\n",
            "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
            "\u001b[0;32m<timed exec>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/pandas/core/frame.py\u001b[0m in \u001b[0;36m__getitem__\u001b[0;34m(self, key)\u001b[0m\n\u001b[1;32m   4088\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcolumns\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnlevels\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   4089\u001b[0m                 \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_getitem_multilevel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 4090\u001b[0;31m             \u001b[0mindexer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcolumns\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_loc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   4091\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mis_integer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mindexer\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   4092\u001b[0m                 \u001b[0mindexer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mindexer\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/pandas/core/indexes/base.py\u001b[0m in \u001b[0;36mget_loc\u001b[0;34m(self, key)\u001b[0m\n\u001b[1;32m   3810\u001b[0m             ):\n\u001b[1;32m   3811\u001b[0m                 \u001b[0;32mraise\u001b[0m \u001b[0mInvalidIndexError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 3812\u001b[0;31m             \u001b[0;32mraise\u001b[0m \u001b[0mKeyError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0merr\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   3813\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mTypeError\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3814\u001b[0m             \u001b[0;31m# If we have a listlike key, _check_indexing_error will raise\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyError\u001b[0m: 'ticker'"
          ]
        }
      ],
      "source": [
        "%%time\n",
        "# convert to numerai ticker, if the target ticker is not\n",
        "if 'bloomberg_ticker' in targets.columns.values.tolist():\n",
        "    targets['ticker'] = targets['bloomberg_ticker'].map(\n",
        "        dict(zip(ticker_map['bloomberg_ticker'], ticker_map['ticker']))\n",
        "    )\n",
        "if 'bloomberg_ticker' not in targets.columns.values.tolist():\n",
        "    targets['bloomberg_ticker'] = targets['ticker'].map(\n",
        "        dict(zip(ticker_map['ticker'], ticker_map['bloomberg_ticker']))\n",
        "    )\n",
        "if CFG.ANALYSE == 1:\n",
        "    print(targets.shape, targets['friday_date'].min(), targets['friday_date'].max())\n",
        "#targets.head()\n",
        "#583node2: 1s\n",
        "#589node2: 1.08s\n",
        "#590node2: 1.4s"
      ],
      "id": "b8933f18"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8697ba95",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 825
        },
        "outputId": "0a1c8aae-cd0a-4f3a-b8cc-5140687abb8f"
      },
      "outputs": [
        {
          "output_type": "error",
          "ename": "KeyError",
          "evalue": "'target_20d'",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/pandas/core/indexes/base.py\u001b[0m in \u001b[0;36mget_loc\u001b[0;34m(self, key)\u001b[0m\n\u001b[1;32m   3804\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 3805\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_engine\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_loc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcasted_key\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   3806\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mKeyError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0merr\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32mindex.pyx\u001b[0m in \u001b[0;36mpandas._libs.index.IndexEngine.get_loc\u001b[0;34m()\u001b[0m\n",
            "\u001b[0;32mindex.pyx\u001b[0m in \u001b[0;36mpandas._libs.index.IndexEngine.get_loc\u001b[0;34m()\u001b[0m\n",
            "\u001b[0;32mpandas/_libs/hashtable_class_helper.pxi\u001b[0m in \u001b[0;36mpandas._libs.hashtable.PyObjectHashTable.get_item\u001b[0;34m()\u001b[0m\n",
            "\u001b[0;32mpandas/_libs/hashtable_class_helper.pxi\u001b[0m in \u001b[0;36mpandas._libs.hashtable.PyObjectHashTable.get_item\u001b[0;34m()\u001b[0m\n",
            "\u001b[0;31mKeyError\u001b[0m: 'target_20d'",
            "\nThe above exception was the direct cause of the following exception:\n",
            "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
            "\u001b[0;32m<timed exec>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/pandas/core/frame.py\u001b[0m in \u001b[0;36m__getitem__\u001b[0;34m(self, key)\u001b[0m\n\u001b[1;32m   4088\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcolumns\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnlevels\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   4089\u001b[0m                 \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_getitem_multilevel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 4090\u001b[0;31m             \u001b[0mindexer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcolumns\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_loc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   4091\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mis_integer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mindexer\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   4092\u001b[0m                 \u001b[0mindexer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mindexer\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/pandas/core/indexes/base.py\u001b[0m in \u001b[0;36mget_loc\u001b[0;34m(self, key)\u001b[0m\n\u001b[1;32m   3810\u001b[0m             ):\n\u001b[1;32m   3811\u001b[0m                 \u001b[0;32mraise\u001b[0m \u001b[0mInvalidIndexError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 3812\u001b[0;31m             \u001b[0;32mraise\u001b[0m \u001b[0mKeyError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0merr\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   3813\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mTypeError\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3814\u001b[0m             \u001b[0;31m# If we have a listlike key, _check_indexing_error will raise\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyError\u001b[0m: 'target_20d'"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 1600x400 with 2 Axes>"
            ],
            "image/png": "iVBORw0KGgoAAAANSUhEUgAABTQAAAF5CAYAAAChnqWvAAAAOXRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjcuMSwgaHR0cHM6Ly9tYXRwbG90bGliLm9yZy/bCgiHAAAACXBIWXMAAA9hAAAPYQGoP6dpAAA1D0lEQVR4nO3df5SV5WEn8O/wY4ZRiFFhFWoYCMqPREKAhYQmrsqYGFNqqItnS7Ymx02EbU+WTpOmqZ5QstWzWxODyRKzZTRqjj2lq8YctZy4NYHuknUN4Uck1pGfCtGiNoJGCIwj8+4f7owhzMS5Mwzyzv18zuEcct/nfe5zeZx7v/neee+tKYqiCAAAAABACQx6qxcAAAAAANBTCk0AAAAAoDQUmgAAAABAaSg0AQAAAIDSUGgCAAAAAKWh0AQAAAAASkOhCQAAAACUhkITAAAAACgNhSYAAAAAUBoKTQAAAACgNHpVaD733HO56667smTJksyZMyf19fWpqanJRRdd1OcFrV27NvPmzcuoUaNSX1+fyZMnZ+nSpTl48GCf5wYAYGCQRwEAqldNURRFpSd97Wtfy5/8yZ8cc/uFF16Yf/zHf+z1YlasWJE//uM/TlEUOeecczJq1Kg88cQTaW1tzZQpU/LDH/4wZ5xxRq/nBwBgYJBHAQCqV69+Q/Ntb3tbLrnkklx77bW57777snTp0j4vZOPGjWlqakqSrFy5Mnv27MmmTZuya9euzJw5My0tLbnmmmv6fD8AAJSfPAoAUL169Ruav+4b3/hG/tN/+k99ekd8/vz5uf/++/OJT3wi3/72t486tn379kyePDnt7e157LHH8p73vKevSwYAYACRRwEAqsdJ8aVABw4cyEMPPZQkWbRo0THHzzvvvMydOzdJcs8995zQtQEAMPDJowAA5XFSFJqbN29Oa2tr6urqMnv27C7HXHDBBUmSRx999EQuDQCAKiCPAgCUx0lRaG7bti1JMnbs2AwdOrTLMRMmTEiSbN269YStCwCA6iCPAgCUx5C3egFJsm/fviT5jd8Y2XFs//79v3GulStXprm5uUf3+/jjj6coigwfPjzjx4/v4WoBAE4OTz31VA4fPpx/9a/+VZ5++um3ejmlJo8CAPTOW5FJT4pC8/Dhw0mS2trabsfU1dUlSQ4dOvQb59q7d282bdpU0f3v37//TYMpAMDJ6oUXXnirl1B68igAQN+cyEx6UhSaw4YNS5K8+uqr3Y5pbW1NktTX1//GuUaPHp0ZM2b06H4fe+yxHDlyJPX19ZkyZUoPVwsAcHJoaWnJoUOHOrMUvSePAgD0zluRSU+KQvP0009P8salPl3pONYxtjuLFy/O4sWLe3S/M2fOzKZNmzJlypRs3Lixh6sFADg5dGQZlyr3nTwKANA7b0UmPSm+FGjixIlJkj179qStra3LMTt37jxqLAAAHC/yKABAeZwUheb06dNTW1ub1tbWrF+/vssx69atS5LMmTPnRC4NAIAqII8CAJTHSVFojhgxIpdeemmSdPmNkNu3b8+aNWuSJAsWLDihawMAYOCTRwEAyuOEFpof/OAHM27cuHzta1875tjSpUtTU1OTu+66K83NzSmKIsnr3xK5cOHCtLe3Z/78+Zk2bdqJXDIAAAOIPAoAUH69KjR/9rOfZeTIkZ1//vzP/zxJ8n/+z/856vYvf/nLR533zDPPZPfu3XnppZeOmXPWrFlZvnx5ktc/SL2hoSEzZszI+PHjs3HjxkyaNCm33nprb5YLAMAAI48CAFSvXn3L+ZEjR/Liiy8ec/trr7121O2//OUvK5q3qakpU6dOzVe/+tX86Ec/ygsvvJCGhoYsWLAg1157bYYPH96b5QIAMMDIowAA1atXhea4ceM6L8GpxNNPP/2mYxobG9PY2NiLVQEAUC3kUQCA6nVSfCkQAAAAAEBPKDQBAAAAgNJQaAIAAAAApaHQBAAAAABKQ6EJAAAAAJSGQhMAAAAAKA2FJgAAAABQGgpNAAAAAKA0FJoAAAAAQGkoNAEAAACA0lBoAgAAAAClodAEAAAAAEpDoQkAAAAAlIZCEwAAAAAoDYUmAAAAAFAaCk0AAAAAoDQUmgAAAABAaSg0AQAAAIDSUGgCAAAAAKWh0AQAAAAASkOhCQAAAACUhkITAAAAACgNhSYAAAAAUBoKTQAAAACgNBSaAAAAAEBpKDQBAAAAgNJQaAIAAAAApaHQBAAAAABKQ6EJAAAAAJSGQhMAAAAAKA2FJgAAAABQGgpNAAAAAKA0FJoAAAAAQGkoNAEAAACA0lBoAgAAAAClodAEAAAAAEpDoQkAAAAAlIZCEwAAAAAoDYUmAAAAAFAaCk0AAAAAoDQUmgAAAABAaSg0AQAAAIDSUGgCAAAAAKWh0AQAAAAASqNPhebatWszb968jBo1KvX19Zk8eXKWLl2agwcP9mq+PXv2ZMmSJZk8eXJOOeWUDBs2LOPHj8/VV1+dn/70p31ZKgAAA5A8CgBQfXpdaK5YsSKNjY1ZvXp1hg0blilTpuTpp5/ODTfckFmzZmXfvn0Vzfd//+//zfnnn58VK1Zk165dGTt2bCZOnJjnn38+d955Z2bMmJF77rmnt8sFAGCAkUcBAKpTrwrNjRs3pqmpKUmycuXK7NmzJ5s2bcquXbsyc+bMtLS05JprrunxfEVR5BOf+EReeeWVzJkzJzt27MiTTz6ZLVu2ZO/evfn4xz+e1157LZ/+9Kfz8ssv92bJAAAMIPIoAED16lWhef3116e9vT1XXXVVFi1alJqamiTJmDFjsmrVqgwaNCj33XdftmzZ0qP5nnjiiezYsSNJ8t//+3/P2LFjO4+ddtppuf3223PqqafmF7/4RdatW9ebJQMAMIDIowAA1aviQvPAgQN56KGHkiSLFi065vh5552XuXPnJkmPL8k5dOhQ598nTJhwzPG6urqcc845SZK2trZKlwwAwAAijwIAVLeKC83NmzentbU1dXV1mT17dpdjLrjggiTJo48+2qM5J02alPr6+iTJI488cszxvXv3ZteuXRk8eHBmzJhR6ZIBABhA5FEAgOo2pNITtm3bliQZO3Zshg4d2uWYjne1t27d2qM5R4wYkaVLl+a6667L1Vdfna9//eu5+OKLU1tbmw0bNuSzn/1s2tra8sUvfjENDQ2/ca6VK1emubm5R/fb0tLSo3EAAJw85FEAgOpWcaHZ8W2RZ5xxRrdjOo7t37+/x/Nee+21GT16dL7yla/kyiuvPOrYxIkT83d/93f5d//u373pPHv37s2mTZt6fL8AAJSLPAoAUN0qLjQPHz6cJKmtre12TF1dXZKjP4vozbS1tWXXrl3Zt29fhgwZkvHjx6e2tjY7duzI9u3b861vfSsf+MAHOj+7qDujR4/u8WVALS0tFa0RAIC3njwKAFDdKi40hw0bliR59dVXux3T2tqaJJ2fQ9QTv/d7v5fVq1fnsssuy2233ZYxY8Ykef1d9SVLluRv/uZvMmfOnDzxxBMZMWJEt/MsXrw4ixcv7tF9zpw507vnAAAlI48CAFS3ir8U6PTTT0/yxqU+Xek41jH2zTz44INZvXp1Ro4cmVWrVnWGx445br/99kyePDnPPPNMbrnllkqXDADAACKPAgBUt4oLzYkTJyZJ9uzZk7a2ti7H7Ny586ixb2bdunVJktmzZ+e000475vjQoUNz8cUXJ0k2bNhQ6ZIBABhA5FEAgOpWcaE5ffr01NbWprW1NevXr+9yTEcgnDNnTo/mfOWVV3p8/x2fmQQAQHWSRwEAqlvFheaIESNy6aWXJkmam5uPOb59+/asWbMmSbJgwYIezdnxzvn69evz8ssvH3O8ra0ta9euTZJMmjSp0iUDADCAyKMAANWt4kIzSZYuXZqamprcddddaW5uTlEUSZK9e/dm4cKFaW9vz/z58zNt2rSjzhs3blzGjRuXe++996jbr7zyytTV1eXnP/95Fi5cmH/+53/uPLZ///78h//wH/Lkk0+mpqYmf/AHf9CbJQMAMIDIowAA1atXheasWbOyfPnyJK9/i2NDQ0NmzJiR8ePHZ+PGjZk0aVJuvfXWY87bvXt3du/enQMHDhx1+znnnJPm5uYMGTIk3/ve99LQ0JBJkybl/PPPz+jRo/M3f/M3qampyY033pjp06f3ZskAAAwg8igAQPXqVaGZJE1NTXn44Ydz2WWX5eDBg3niiSfS0NCQ6667Lhs2bMjIkSMrmu8Tn/hENmzYkKuvvjoNDQ3ZvXt3tm/fnrPOOiu///u/n3Xr1uXzn/98b5cLAMAAI48CAFSnIX05ubGxMY2NjT0e33EpUHemTZuW22+/vS9LAgCgisijAADVp9e/oQkAAAAAcKIpNAEAAACA0lBoAgAAAAClodAEAAAAAEpDoQkAAAAAlIZCEwAAAAAoDYUmAAAAAFAaCk0AAAAAoDQUmgAAAABAaSg0AQAAAIDSUGgCAAAAAKWh0AQAAAAASkOhCQAAAACUhkITAAAAACgNhSYAAAAAUBoKTQAAAACgNBSaAAAAAEBpKDQBAAAAgNJQaAIAAAAApaHQBAAAAABKQ6EJAAAAAJSGQhMAAAAAKA2FJgAAAABQGgpNAAAAAKA0FJoAAAAAQGkoNAEAAACA0lBoAgAAAAClodAEAAAAAEpDoQkAAAAAlIZCEwAAAAAoDYUmAAAAAFAaCk0AAAAAoDQUmgAAAABAaSg0AQAAAIDSUGgCAAAAAKWh0AQAAAAASkOhCQAAAACUhkITAAAAACgNhSYAAAAAUBoKTQAAAACgNBSaAAAAAEBpKDQBAAAAgNJQaAIAAAAApdGnQnPt2rWZN29eRo0alfr6+kyePDlLly7NwYMHez1nURRZtWpVPvKRj+Sss85KXV1dxowZk8bGxtx00019WS4AAAOMPAoAUH16XWiuWLEijY2NWb16dYYNG5YpU6bk6aefzg033JBZs2Zl3759Fc954MCBfPjDH87HP/7x/M//+T8zfPjwTJs2LUOHDs3/+l//K3/1V3/V2+UCADDAyKMAANWpV4Xmxo0b09TUlCRZuXJl9uzZk02bNmXXrl2ZOXNmWlpacs0111Q0Z1EUueKKK/L9738/H/nIR7Jjx47s3Lkz69evz+7du/Pzn/88t99+e2+WCwDAACOPAgBUr14Vmtdff33a29tz1VVXZdGiRampqUmSjBkzJqtWrcqgQYNy3333ZcuWLT2e884778zDDz+c973vfXnwwQczYcKEo46//e1vz+WXX96b5QIAMMDIowAA1aviQvPAgQN56KGHkiSLFi065vh5552XuXPnJknuueeeHs+7fPnyJMkXv/jFDBkypNJlAQBQJeRRAIDqVnFS27x5c1pbW1NXV5fZs2d3OeaCCy7I97///Tz66KM9mnPnzp15/PHHM2jQoFx88cX50Y9+lDvuuCM7duzI8OHD8/73vz+f/vSnM3LkyEqXCwDAACOPAgBUt4oLzW3btiVJxo4dm6FDh3Y5puPynK1bt/Zozg0bNiRJzjzzzNxyyy358z//8xRF0Xn8/vvvz4033pj77rsvF198caVLBgBgAJFHAQCqW8WFZse3RZ5xxhndjuk4tn///h7NuXfv3s7xX/jCF/I7v/M7+fKXv5xzzz03W7duTVNTU9asWZMrrrgiP/3pT3POOed0O9fKlSvT3Nzco/ttaWnp0TgAAE4e8igAQHWruNA8fPhwkqS2trbbMXV1dUmSQ4cO9WjOAwcOJElee+21TJgwId/97nc7322fOnVq54eyP/fcc/na176Wm266qdu59u7dm02bNvXofgEAKB95FACgulVcaA4bNixJ8uqrr3Y7prW1NUlSX19f0ZxJ8pnPfOaYS4dOOeWU/OEf/mGWLVuWhx566DcGyNGjR2fGjBk9ut+WlpYeh1wAAE4O8igAQHWruNA8/fTTk7xxqU9XOo51jO3pnEkyZcqULsd03P7UU0/9xrkWL16cxYsX9+h+Z86c6d1zAICSkUcBAKrboEpPmDhxYpJkz549aWtr63LMzp07jxr7ZiZPntz59+4uHep417y9vb3HawUAYOCRRwEAqlvFheb06dNTW1ub1tbWrF+/vssx69atS5LMmTOnx3N2XA60a9euLsd0hNLf9AHsAAAMfPIoAEB1q7jQHDFiRC699NIk6fLbG7dv3541a9YkSRYsWNCjOU855ZT87u/+bpLk29/+9jHHi6LInXfemSRpbGysdMkAAAwg8igAQHWruNBMkqVLl6ampiZ33XVXmpubUxRFkte/0XHhwoVpb2/P/PnzM23atKPOGzduXMaNG5d77733mDmXLVuWIUOGZN26dbn++utz5MiRJK9/0+QXvvCFPPbYYxk2bFj+5E/+pDdLBgBgAJFHAQCqV68KzVmzZmX58uVJXv/Q84aGhsyYMSPjx4/Pxo0bM2nSpNx6663HnLd79+7s3r07Bw4cOObYu971rtx2220ZPHhw/uIv/iKjR4/O+973vpx99tn5yle+kqFDh+b222/PpEmTerNkAAAGEHkUAKB69arQTJKmpqY8/PDDueyyy3Lw4ME88cQTaWhoyHXXXZcNGzZk5MiRFc/5yU9+Mo8++miuvPLKDBo0KJs3b87QoUOzcOHC/PjHP87ChQt7u1wAAAYYeRQAoDoN6cvJjY2NFX2GUMelQL/Jv/7X/zp33313X5YFAECVkEcBAKpPr39DEwAAAADgRFNoAgAAAAClodAEAAAAAEpDoQkAAAAAlIZCEwAAAAAoDYUmAAAAAFAaCk0AAAAAoDQUmgAAAABAaSg0AQAAAIDSUGgCAAAAAKWh0AQAAAAASkOhCQAAAACUhkITAAAAACgNhSYAAAAAUBoKTQAAAACgNBSaAAAAAEBpKDQBAAAAgNJQaAIAAAAApaHQBAAAAABKQ6EJAAAAAJSGQhMAAAAAKA2FJgAAAABQGgpNAAAAAKA0FJoAAAAAQGkoNAEAAACA0lBoAgAAAAClodAEAAAAAEpDoQkAAAAAlIZCEwAAAAAoDYUmAAAAAFAaCk0AAAAAoDQUmgAAAABAaSg0AQAAAIDSUGgCAAAAAKWh0AQAAAAASkOhCQAAAACUhkITAAAAACgNhSYAAAAAUBoKTQAAAACgNBSaAAAAAEBpKDQBAAAAgNJQaAIAAAAApaHQBAAAAABKQ6EJAAAAAJRGnwrNtWvXZt68eRk1alTq6+szefLkLF26NAcPHjwui/vmN7+Zmpqa1NTU5KKLLjoucwIAMHDIowAA1afXheaKFSvS2NiY1atXZ9iwYZkyZUqefvrp3HDDDZk1a1b27dvXp4U9++yzufbaa/s0BwAAA5c8CgBQnXpVaG7cuDFNTU1JkpUrV2bPnj3ZtGlTdu3alZkzZ6alpSXXXHNNnxb2R3/0Rzl48GDmzZvXp3kAABh45FEAgOrVq0Lz+uuvT3t7e6666qosWrQoNTU1SZIxY8Zk1apVGTRoUO67775s2bKlV4u6++6788ADD+Qzn/lMZs6c2as5AAAYuORRAIDqVXGheeDAgTz00ENJkkWLFh1z/LzzzsvcuXOTJPfcc0/FC9q/f3+WLFmSc845JzfccEPF5wMAMLDJowAA1a3iQnPz5s1pbW1NXV1dZs+e3eWYCy64IEny6KOPVrygz33uc3n++eezYsWKDB8+vOLzAQAY2ORRAIDqNqTSE7Zt25YkGTt2bIYOHdrlmAkTJiRJtm7dWtHca9asyR133JHLL7888+fPr3RpSV7/DKXm5uYejW1paenVfQAA8NaRRwEAqlvFhWbHt0WeccYZ3Y7pOLZ///4ez3vo0KEsWrQow4cPzze+8Y1Kl9Vp79692bRpU6/PBwDg5CaPAgBUt4oLzcOHDydJamtrux1TV1eX5PVQ2FPLli3Lzp07s3z58rzjHe+odFmdRo8enRkzZvRobEtLS0VrBADgrSePAgBUt4oLzWHDhiVJXn311W7HtLa2Jknq6+t7NOfmzZtz8803Z/r06VmyZEmlSzrK4sWLs3jx4h6NnTlzpnfPAQBKRh4FAKhuFX8p0Omnn57kjUt9utJxrGPsm/nUpz6V9vb2NDc3Z/DgwZUuCQCAKiKPAgBUt4p/Q3PixIlJkj179qStra3LD2LfuXPnUWPfzObNmzN48ODMmzfvmGMHDhxIkjzyyCM5++yzkyQ//vGP+3QZEAAA5SWPAgBUt4oLzenTp6e2tjatra1Zv359PvCBDxwzZt26dUmSOXPm9HjeI0eO5Pnnn+/2eFtbW+fxI0eOVLhqAAAGCnkUAKC6VXzJ+YgRI3LppZcmSZqbm485vn379qxZsyZJsmDBgh7NWRRFt3+WLVuWJLnwwgs7bxs3blylywYAYICQRwEAqlvFhWaSLF26NDU1NbnrrrvS3NycoiiSJHv37s3ChQvT3t6e+fPnZ9q0aUedN27cuIwbNy733ntv31cOAEDVkkcBAKpXrwrNWbNmZfny5Ule/xbHhoaGzJgxI+PHj8/GjRszadKk3Hrrrcect3v37uzevbvzc4gAAKA35FEAgOrVq0IzSZqamvLwww/nsssuy8GDB/PEE0+koaEh1113XTZs2JCRI0cez3UCAMBR5FEAgOpU8ZcC/arGxsY0Njb2eHzHpUCV+NKXvpQvfelLFZ8HAMDAJ48CAFSfXv+GJgAAAADAiabQBAAAAABKQ6EJAAAAAJSGQhMAAAAAKA2FJgAAAABQGgpNAAAAAKA0FJoAAAAAQGkoNAEAAACA0lBoAgAAAAClodAEAAAAAEpDoQkAAAAAlIZCEwAAAAAoDYUmAAAAAFAaCk0AAAAAoDQUmgAAAABAaSg0AQAAAIDSUGgCAAAAAKWh0AQAAAAASkOhCQAAAACUhkITAAAAACgNhSYAAAAAUBoKTQAAAACgNBSaAAAAAEBpKDQBAAAAgNJQaAIAAAAApaHQBAAAAABKQ6EJAAAAAJSGQhMAAAAAKA2FJgAAAABQGgpNAAAAAKA0FJoAAAAAQGkoNAEAAACA0lBoAgAAAAClodAEAAAAAEpDoQkAAAAAlIZCEwAAAAAoDYUmAAAAAFAaCk0AAAAAoDQUmgAAAABAaSg0AQAAAIDSUGgCAAAAAKWh0AQAAAAASkOhCQAAAACURp8KzbVr12bevHkZNWpU6uvrM3ny5CxdujQHDx6saJ4jR47k4YcfTlNTU2bPnp23v/3tqa2tzejRo/Oxj30sq1ev7ssyAQAYoORRAIDq0+tCc8WKFWlsbMzq1aszbNiwTJkyJU8//XRuuOGGzJo1K/v27evxXHfeeWc+/OEP5+tf/3o2btyYs846K1OnTs0vf/nLPPDAA5k3b14WL16coih6u1wAAAYYeRQAoDr1qtDcuHFjmpqakiQrV67Mnj17smnTpuzatSszZ85MS0tLrrnmmh7PVxRF3vOe9+S2227Lvn37snXr1mzcuDEvvvhivvKVr6SmpibNzc3567/+694sFwCAAUYeBQCoXr0qNK+//vq0t7fnqquuyqJFi1JTU5MkGTNmTFatWpVBgwblvvvuy5YtW3o03xVXXJGf/OQn+dSnPpXTTjut8/YhQ4bkT//0T/PpT386yethFQAA5FEAgOpVcaF54MCBPPTQQ0mSRYsWHXP8vPPOy9y5c5Mk99xzT4/mPOOMMzpDaFcuu+yyJMnWrVsrXS4AAAOMPAoAUN0qLjQ3b96c1tbW1NXVZfbs2V2OueCCC5Ikjz76aN9W9/8dOnQoSXLKKaccl/kAACgveRQAoLpVXGhu27YtSTJ27NgMHTq0yzETJkxIcvzewV61alWSN4IpAADVSx4FAKhuQyo9oePbIs8444xux3Qc279/fy+X9Yb7778/f//3f5+ampr82Z/92ZuOX7lyZZqbm3s0d0tLS1+XBwDACSaPAgBUt4oLzcOHDydJamtrux1TV1eX5I1Lc3rrySefzCc/+ckkSVNTU377t3/7Tc/Zu3dvNm3a1Kf7BQDg5CWPAgBUt4oLzWHDhiVJXn311W7HtLa2Jknq6+t7uazkZz/7WS699NK8/PLL+ehHP5obb7yxR+eNHj06M2bM6NHYlpaWPodcAABOLHkUAKC6VVxonn766UneuNSnKx3HOsZW6rnnnktjY2P27NmTiy66KN/5zne6/XykX7d48eIsXry4R2Nnzpzp3XMAgJKRRwEAqlvFXwo0ceLEJMmePXvS1tbW5ZidO3ceNbYSL7zwQubOnZvt27dnzpw5efDBBzvfhQcAAHkUAKC6VVxoTp8+PbW1tWltbc369eu7HLNu3bokyZw5cyqae9++ffnQhz6UlpaWzJgxI9/73vcyfPjwSpcIAMAAJo8CAFS3igvNESNG5NJLL02SLr+9cfv27VmzZk2SZMGCBT2e9xe/+EU+/OEPZ8uWLTn//PPzD//wDznttNMqXR4AAAOcPAoAUN0qLjSTZOnSpampqcldd92V5ubmFEWR5PVvdFy4cGHa29szf/78TJs27ajzxo0bl3HjxuXee+896vZf/vKX+Z3f+Z1s3LgxkydPzg9+8IOceeaZvXxIAAAMdPIoAED1qvhLgZJk1qxZWb58eT772c9m8eLFueGGGzJy5Mg88cQTaW1tzaRJk3Lrrbcec97u3buTJAcOHDjq9q9//ev54Q9/2Pm/r7jiim7v+957783ZZ5/dm2UDADBAyKMAANWrV4VmkjQ1NWXq1Kn56le/mh/96Ed54YUX0tDQkAULFuTaa6+t6LOGWltbO//+5JNP/saxhw8f7u2SAQAYQORRAIDq1OtCM0kaGxvT2NjY4/EdlwL9ui996Uv50pe+1JelAABQheRRAIDq06vP0AQAAAAAeCsoNAEAAACA0lBoAgAAAAClodAEAAAAAEpDoQkAAAAAlIZCEwAAAAAoDYUmAAAAAFAaCk0AAAAAoDQUmgAAAABAaSg0AQAAAIDSUGgCAAAAAKWh0AQAAAAASkOhCQAAAACUhkITAAAAACgNhSYAAAAAUBoKTQAAAACgNBSaAAAAAEBpKDQBAAAAgNJQaAIAAAAApaHQBAAAAABKQ6EJAAAAAJSGQhMAAAAAKA2FJgAAAABQGgpNAAAAAKA0FJoAAAAAQGkoNAEAAACA0lBoAgAAAAClodAEAAAAAEpDoQkAAAAAlIZCEwAAAAAoDYUmAAAAAFAaCk0AAAAAoDQUmgAAAABAaSg0AQAAAIDSUGgCAAAAAKWh0AQAAAAASkOhCQAAAACUhkITAAAAACgNhSYAAAAAUBoKTQAAAACgNBSaAAAAAEBpKDQBAAAAgNJQaAIAAAAApaHQBAAAAABKo0+F5tq1azNv3ryMGjUq9fX1mTx5cpYuXZqDBw/2es7vfOc7ufjii3P66afn1FNPzXvf+97cdNNNaWtr68tSAQAYgORRAIDq0+tCc8WKFWlsbMzq1aszbNiwTJkyJU8//XRuuOGGzJo1K/v27at4zj/90z/NggUL8o//+I8588wzc+655+bxxx/P5z//+VxyySVpbW3t7XIBABhg5FEAgOrUq0Jz48aNaWpqSpKsXLkye/bsyaZNm7Jr167MnDkzLS0tueaaayqa87vf/W6++tWvpq6uLvfff3927NiRxx57LI8//njGjx+f//2//3euu+663iwXAIABRh4FAKhevSo0r7/++rS3t+eqq67KokWLUlNTkyQZM2ZMVq1alUGDBuW+++7Lli1bejznf/7P/zlJ8oUvfCGXX3555+2TJ0/ObbfdliS55ZZb8i//8i+9WTIAAAOIPAoAUL0qLjQPHDiQhx56KEmyaNGiY46fd955mTt3bpLknnvu6dGc27dvz2OPPdbtnHPnzs25556b1tbWPPDAA5UuGQCAAUQeBQCobhUXmps3b05ra2vq6uoye/bsLsdccMEFSZJHH320R3N2jHvnO9+Z3/qt3zoucwIAMDDJowAA1a3iQnPbtm1JkrFjx2bo0KFdjpkwYUKSZOvWrRXN2XHe8ZgTAICBSR4FAKhuQyo9oePbIs8444xux3Qc279//wmfc+XKlWlubu7R/XZcVtTS0pKZM2f26BwAgJNFS0tLkuSpp556i1dyYsmjAAAnj7cik1ZcaB4+fDhJUltb2+2Yurq6JMmhQ4dO+Jx79+7Npk2benS/HQ4dOlTxOQAAJ4sDBw681Us4oeRRAICTz4nMpBUXmsOGDUuSvPrqq92OaW1tTZLU19ef8DlHjx6dGTNm9Oh+N2/enKIoMnjw4EybNq1H53ByaWlpyaFDh1JfX58pU6a81cuhQvav/Oxh+dnDcnvsscdy5MiRzm/4rhbyKCcTz6PlZw/Lzf6Vnz0sv7cik1ZcaJ5++ulJ3rgspysdxzrGnsg5Fy9enMWLF/fofmfOnJlNmzZl2rRp2bhxY4/O4eTSsYdTpkyxhyVk/8rPHpafPSy3jv07//zz3+qlnFDyKCcTz6PlZw/Lzf6Vnz0sv7cik1b8pUATJ05MkuzZsydtbW1djtm5c+dRY3s6544dO7odU+mcAAAMTPIoAEB1q7jQnD59empra9Pa2pr169d3OWbdunVJkjlz5vRozve///1JXv/w0Gefffa4zAkAwMAkjwIAVLeKC80RI0bk0ksvTZIuv71x+/btWbNmTZJkwYIFPZpz4sSJmTp1ardzrlmzJjt27EhtbW0uv/zySpcMAMAAIo8CAFS3igvNJFm6dGlqampy1113pbm5OUVRJHn9Gx0XLlyY9vb2zJ8//5gPNh83blzGjRuXe++995g5ly1bliS58cYb8+CDD3bevnXr1nz6059OkvzRH/1RRo0a1ZslAwAwgMijAADVq1eF5qxZs7J8+fIkr3/oeUNDQ2bMmJHx48dn48aNmTRpUm699dZjztu9e3d2797d5de4/9t/+2/T1NSU1tbWXH755Tn33HPz3ve+N+9+97vz1FNP5YMf/GD+63/9r71ZLgAAA4w8CgBQvXpVaCZJU1NTHn744Vx22WU5ePBgnnjiiTQ0NOS6667Lhg0bMnLkyIrnvPnmm3P33XfnwgsvzM9//vNs27Yt73rXu3LjjTdmzZo1GTZsWG+XCwDAACOPAgBUpyF9ObmxsTGNjY09Ht9xKdBvcuWVV+bKK6/sy7IAAKgS8igAQPXp9W9oAgAAAACcaApNAAAAAKA0FJoAAAAAQGn06TM0y27RokXZu3dvRo8e/VYvhV6yh+Vm/8rPHpafPSw3+1d+9rD87GH52cNys3/lZw/L763Yw5qiJ5+MDgAAAABwEnDJOQAAAABQGgpNAAAAAKA0FJoAAAAAQGkMmEJz7dq1mTdvXkaNGpX6+vpMnjw5S5cuzcGDB3s953e+851cfPHFOf3003Pqqafmve99b2666aa0tbUdx5XT4Xjt4ZEjR/Lwww+nqakps2fPztvf/vbU1tZm9OjR+djHPpbVq1f30yOgP34Of9U3v/nN1NTUpKamJhdddNFxmZM39Mf+FUWRVatW5SMf+UjOOuus1NXVZcyYMWlsbMxNN910HFdPcvz3cM+ePVmyZEkmT56cU045JcOGDcv48eNz9dVX56c//elxXn11e+6553LXXXdlyZIlmTNnTurr64/bc11/PzdzNJm03OTR8pNHy08mLTd5tLxKl0eLAeC//bf/VtTU1BRJinPOOaeYPn16UVdXVyQppkyZUrz44osVz/m5z32uSFIkKSZMmFC85z3vKQYPHlwkKf7Nv/k3xeHDh/vhkVSv47mHt912W+feDRo0qJg4cWIxY8aM4m1ve1vn7YsWLSra29v78RFVn/74OfxVzzzzzFF7eOGFFx6fhVMURf/s3yuvvFJccsklnXv2zne+s5g1a1YxduzYYvDgwcWZZ57ZD4+keh3vPXzkkUeKESNGFEmKoUOHFpMmTSqmTp1a1NfXF0mKIUOGFHfffXc/PZrqc/PNN3f+rPzqn74+1/X3czNHk0nLTR4tP3m0/GTScpNHy61sebT0heaGDRuKQYMGFTU1NcXKlSs7Q8Gzzz5bzJw5s0hSXHHFFRXNed999xVJirq6uuL+++/vvL2lpaUYP358kaT47Gc/e1wfRzU73nt46623Fu95z3uK2267rXjppZc6b29rayu+8pWvdP4gffOb3zzuj6Va9cfP4a+7/PLLi8GDBxfz5s0TII+z/ti/9vb24kMf+lCRpPjIRz5S7Nix46jj+/fvP+r5lb453nvY3t5enHvuuUWSYs6cOcXu3bs7j7300kvFxz/+8SJJ8ba3ve2o51l671vf+lZxySWXFNdee21x3333FUuXLu3zc92JeG7mDTJpucmj5SePlp9MWm7yaPmVLY+WvtD82Mc+ViQpPvGJTxxzbNu2bcWgQYOKJMVjjz3W4zmnTZtWJCn+4i/+4phjP/jBDzqD5QsvvNCntfO6472HL7744m98t/uaa64pkhTTpk3r7ZL5Nf3xc/ir/sf/+B9FkuKP//iPi2XLlgmQx1l/7N/tt99eJCne9773FW1tbcdzuXTheO/h448/3vmO7E9+8pNjjh8+fLg49dRTiyTFgw8+2Of1c6wVK1b0+bmuv5+bOZpMWm7yaPnJo+Unk5abPDrwnOx5tNSfoXngwIE89NBDSZJFixYdc/y8887L3LlzkyT33HNPj+bcvn17HnvssW7nnDt3bs4999y0trbmgQce6O3S+f/6Yw/POOOM1NTUdHv8sssuS5Js3bq10uXShf7Yw1+1f//+LFmyJOecc05uuOGGvi2WY/TX/i1fvjxJ8sUvfjFDhgw5DiulO/2xh4cOHer8+4QJE445XldXl3POOSdJfIbfSaq/n5s5mkxabvJo+cmj5SeTlps8Slf6+7m51IXm5s2b09ramrq6usyePbvLMRdccEGS5NFHH+3RnB3j3vnOd+a3fuu3jsucdK8/9vDNdDwxnnLKKcdlvmrX33v4uc99Ls8//3xWrFiR4cOH92mtHKs/9m/nzp15/PHHM2jQoFx88cX50Y9+lP/4H/9jLrnkksyfPz9/9Vd/lZ///OfH7TFUu/7Yw0mTJqW+vj5J8sgjjxxzfO/evdm1a1cGDx6cGTNm9HLl9Ke34vW1msmk5SaPlp88Wn4yabnJo3Slv5+bS11obtu2LUkyduzYDB06tMsxHU1+T9/97Jizq3cAejsn3euPPXwzq1atSvLGDw590597uGbNmtxxxx25/PLLM3/+/D6tk671x/5t2LAhSXLmmWfmlltuyZw5c7Jy5cr84Ac/yP33359rr7025513XtauXXscHgH9sYcjRozI0qVLkyRXX3117r333rz44ot55ZVXsnbt2nz0ox9NW1tbrr322jQ0NByHR8Hx9la8vlYzmbTc5NHyk0fLTyYtN3mUrvT362upC819+/Ylef2Sju50HNu/f/9bNifdO9H/3vfff3/+/u//PjU1NfmzP/uzPs9H/+3hoUOHsmjRogwfPjzf+MY3+rZIutUf+7d3797O8V/4whfy0Y9+NP/0T/+U1tbWbNmyJXPnzs1LL72UK664Is8880wfHwH99TN47bXX5o477sjb3/72XHnllRk5cmTe9ra3Ze7cufnlL3+Zv/u7v8v111/ft8XTb+SZE0smLTd5tPzk0fKTSctNHqUr/f36WupC8/Dhw0mS2trabsfU1dUlOfrzF070nHTvRP57P/nkk/nkJz+ZJGlqaspv//Zv92k+Xtdfe7hs2bLs3Lkzf/mXf5l3vOMdfVsk3eqP/Ttw4ECS5LXXXsuECRPy3e9+N+9617tSW1ubqVOn5sEHH8zZZ5+dl156KV/72tf69gDot5/Btra27Nq1K/v27cuQIUNy3nnn5d3vfnfq6uqyffv2fOtb3xL+T2LyzIklk5abPFp+8mj5yaTlJo/Slf5+fS11oTls2LAkyauvvtrtmNbW1iTp/OyFt2JOunei/r1/9rOf5dJLL83LL7+cj370o7nxxht7PRdH64893Lx5c26++eZMnz49S5Ys6fsi6VZ/Po8myWc+85ljLi845ZRT8od/+IdJ0vkh0fRefz2P/t7v/V6uv/76TJ8+Pbt37862bdvy+OOPZ+/evfn3//7f5+GHH86cOXPyyiuv9O0B0C/kmRNLJi03ebT85NHyk0nLTR6lK/39+lrqQvP0009P8savsXal41jH2LdiTrp3Iv69n3vuuTQ2NmbPnj256KKL8p3vfKfbz2+gcv2xh5/61KfS3t6e5ubmDB48uO+LpFv9+TyaJFOmTOlyTMftTz31VI/mpHv9sYcPPvhgVq9enZEjR2bVqlUZM2bMUfd3++23Z/LkyXnmmWdyyy239GH19Bd55sSSSctNHi0/ebT8ZNJyk0fpSn+/vg7p3bJODhMnTkyS7NmzJ21tbV2Ggp07dx41tqdz7tixo9sxlc5J9/pjD3/VCy+8kLlz52b79u2ZM2dOHnzwwaPeqaPv+mMPN2/enMGDB2fevHnHHOu4dOSRRx7J2WefnST58Y9/7DKgXuqP/Zs8eXLn37u7vKDj57C9vb2i9XKs/tjDdevWJUlmz56d00477ZjjQ4cOzcUXX5wnn3yy8wP3Obn09+srR5NJy00eLT95tPxk0nKTR+lKf7++lvo3NKdPn57a2tq0trZm/fr1XY7p+CGYM2dOj+Z8//vfn+T1d2ieffbZ4zIn3euPPeywb9++fOhDH0pLS0tmzJiR733vexk+fHif18zR+msPjxw5kueff/6YPwcPHkzy+uepdNx25MiRvj+QKtUf+zd9+vTOSwZ27drV5ZiOF65zzjmn0iXza/pjDyu5bKfjs3E4ufTn6yvHkknLTR4tP3m0/GTScpNH6Up/59FSF5ojRozIpZdemiRpbm4+5vj27duzZs2aJMmCBQt6NOfEiRMzderUbudcs2ZNduzYkdra2lx++eW9XTr/X3/sYZL84he/yIc//OFs2bIl559/fv7hH/6hy3d16Lv+2MOiKLr9s2zZsiTJhRde2HnbuHHjjs+DqUL9sX+nnHJKfvd3fzdJ8u1vf/uY40VR5M4770ySNDY29mbZ/Ir+ei1MkvXr1+fll18+5nhbW1vWrl2bJJk0aVKv1k3/6q/XV7omk5abPFp+8mj5yaTlJo/SlX7Po0XJrV+/vqipqSlqamqKlStXFu3t7UVRFMU///M/FzNnziySFPPnzz/mvIaGhqKhoaG45557jjl27733FkmKurq64oEHHui8/cknnyzGjx9fJCmampr670FVmeO9hwcPHiw++MEPFkmKyZMnF88///wJeRzVrD9+DruzbNmyIklx4YUXHq/lV73+2L9/+qd/KoYMGVIkKf7yL/+yeO2114qiKIq2trbi85//fJGkGDZsWPHkk0/274OrEsd7D3/2s58VdXV1RZLisssuK5599tnOY/v27Sv+4A/+oEhS1NTUFJs2berfB1elVqxY0aPnug984ANFQ0NDcfPNNx9zrLf/XdA7Mmm5yaPlJ4+Wn0xabvLowHOy59HSF5pFURQ333xzUVNTUyQp3vGOdxTTp0/v/A9/0qRJxb/8y78cc06SIklxxx13dDlnU1NT55gJEyYU06ZNKwYPHlwkKT74wQ8Whw4d6udHVV2O5x7+l//yXzqPTZ48ufjABz7Q7Z+9e/eeoEc48PXHz2FXBMj+0R/7d+edd3Y+b44aNaqYPXt2ceaZZxZJiqFDhxZ/+7d/28+Pqroc7z389re/3fl/AIYMGVJMnDixePe73905Z01NTfHlL3/5BDyy6rBnz57izDPP7Pxz6qmndv7b/+rtN95441HnNTQ0FEmKZcuWdTlvb/67oPdk0nKTR8tPHi0/mbTc5NFyK1seHRCFZlEUxfe///3isssuK84444yirq6umDhxYnHdddcVr7zySpfje/LCdffddxcXXnhhcdpppxX19fXF1KlTixtvvLF49dVX++lRVLfjtYcdAaMnf5566qn+f2BVpD9+Dn+dANl/+mP/fvzjHxdXXnllcdZZZxVDhw4tzj777GLhwoXFT37yk356FNXteO/hT37yk+Lqq68uJkyYUNTV1RW1tbXF2LFji9///d8vfvjDH/bjI6k+Tz31VI9et349KL5ZgCyKyv+7oG9k0nKTR8tPHi0/mbTc5NHyKlserSmKoggAAAAAQAmU+kuBAAAAAIDqotAEAAAAAEpDoQkAAAAAlIZCEwAAAAAoDYUmAAAAAFAaCk0AAAAAoDQUmgAAAABAaSg0AQAAAIDSUGgCAAAAAKWh0AQAAAAASkOhCQAAAACUhkITAAAAACgNhSYAAAAAUBoKTQAAAACgNP4fcaYHKl8ZBi0AAAAASUVORK5CYII=\n"
          },
          "metadata": {}
        }
      ],
      "source": [
        "%%time\n",
        "if CFG.ANALYSE == 1:\n",
        "    # there are train and validation...\n",
        "    fig, ax = plt.subplots(1, 2, figsize=(16, 4))\n",
        "    ax = ax.flatten()\n",
        "\n",
        "    for i, data_type in enumerate(['train', 'validation']):\n",
        "        # slice\n",
        "        targets_ = targets.query(f'data_type == \"{data_type}\"')\n",
        "        \"\"\"\n",
        "        logger.info('*' * 50)\n",
        "        logger.info('{} target: {:,} numerai tickers , {:,} bloomberg tickers (friday_date: {} - {})'.format(\n",
        "            data_type,\n",
        "            targets_['ticker'].nunique(),\n",
        "            targets_['bloomberg_ticker'].nunique(),\n",
        "            targets_['friday_date'].min(),\n",
        "            targets_['friday_date'].max(),\n",
        "        ))\n",
        "        \"\"\"\n",
        "        # plot target\n",
        "    #     ax[i].hist(targets_['target'])\n",
        "        ax[i].hist(targets_['target_20d'])\n",
        "        ax[i].set_title(f'{data_type}')"
      ],
      "id": "8697ba95"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8f953fc6",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 122
        },
        "outputId": "51c893eb-6304-4657-b822-fbae919c24e0"
      },
      "outputs": [
        {
          "output_type": "error",
          "ename": "NameError",
          "evalue": "name 'targets' is not defined",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<timed exec>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n",
            "\u001b[0;31mNameError\u001b[0m: name 'targets' is not defined"
          ]
        }
      ],
      "source": [
        "%%time\n",
        "d= []\n",
        "if CFG.ANALYSE == 1:\n",
        "    # target relations\n",
        "    d = pd.crosstab(\n",
        "        targets['target_4d']\n",
        "        , targets['target_20d']\n",
        "    )\n",
        "    d['sum'] = d.values.sum(axis=1)\n",
        "    for i, f in enumerate(d.columns):\n",
        "        d[f] = d.apply(lambda row : 100*row[f]/row['sum'], axis=1)\n",
        "    d.drop(columns=['sum'], inplace=True)\n",
        "\n",
        "    print('target transition matrix (%)')\n",
        "    d.astype(int).style.background_gradient(cmap='viridis', axis=1)\n",
        "#583node2: 4.46s; 3.32s"
      ],
      "id": "8f953fc6"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7c2441b8"
      },
      "source": [
        "The target looks exactly like the one from the Numerai Tournament, where both features and targets are given to the participants.\n",
        "\n",
        "Also note that the train-validation split is based on time (i.e., Time-Series Split):\n",
        "\n",
        "- train friday_date: 20030131 ~ 20121228\n",
        "- validation friday_date: 20130104 ~"
      ],
      "id": "7c2441b8"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0b84d464"
      },
      "source": [
        "## Check Ticker Overlaps\n",
        "Let's see if we have enough overlap of tickers between our yfiance stock data and the numerai targets. We need at least 5 tickers for submission."
      ],
      "id": "0b84d464"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "e3c93de7",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 122
        },
        "outputId": "82599f8d-659e-40a3-bfa7-b156d04a8bea"
      },
      "outputs": [
        {
          "output_type": "error",
          "ename": "NameError",
          "evalue": "name 'df' is not defined",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<timed exec>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n",
            "\u001b[0;31mNameError\u001b[0m: name 'df' is not defined"
          ]
        }
      ],
      "source": [
        "%%time\n",
        "if CFG.ANALYSE == 1:\n",
        "    # ticker overlap\n",
        "    venn3(\n",
        "        [\n",
        "            set(df['ticker'].unique().tolist())\n",
        "            , set(targets.query('data_type == \"train\"')['ticker'].unique().tolist())\n",
        "            , set(targets.query('data_type == \"validation\"')['ticker'].unique().tolist())\n",
        "        ],\n",
        "        set_labels=('yf price', 'train target', 'valid target')\n",
        "    )\n",
        "#583node2: 7.32s"
      ],
      "id": "e3c93de7"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "d66b63a0"
      },
      "source": [
        "Ah, yeah, not bad, I guess?\n",
        "\n",
        "Here I only use our stock price data which have ticker overlaps such that we can build a supervised machine learning model."
      ],
      "id": "d66b63a0"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "10f4e1ec"
      },
      "outputs": [],
      "source": [
        "def reduce_mem_usage(df, verbose=False):\n",
        "    start_mem = df.memory_usage().sum() / 1024**2\n",
        "    #print('Memory usage before optimization 2 is: {:.2f} MB'.format(start_mem))\n",
        "    #print(\"The dataframe 2 has {} columns.\".format(df.shape[1]))\n",
        "    numerics = ['int16', 'int32', 'int64', 'float16', 'float32', 'float64']\n",
        "    for col in df.columns:\n",
        "        col_type = df[col].dtypes\n",
        "        if col_type in numerics:\n",
        "            c_min = df[col].min()\n",
        "            c_max = df[col].max()\n",
        "            if str(col_type)[:3] == 'int':\n",
        "                if c_min > np.iinfo(np.int8).min and c_max < np.iinfo(np.int8).max:\n",
        "                    df[col] = df[col].astype(np.int8)\n",
        "                elif c_min > np.iinfo(np.int16).min and c_max < np.iinfo(np.int16).max:\n",
        "                    df[col] = df[col].astype(np.int16)\n",
        "                elif c_min > np.iinfo(np.int32).min and c_max < np.iinfo(np.int32).max:\n",
        "                    df[col] = df[col].astype(np.int32)\n",
        "                elif c_min > np.iinfo(np.int64).min and c_max < np.iinfo(np.int64).max:\n",
        "                    df[col] = df[col].astype(np.int64)\n",
        "            else:\n",
        "                if c_min > np.finfo(np.float16).min and c_max < np.finfo(np.float16).max:\n",
        "                    df[col] = df[col].astype(np.float16)\n",
        "                elif c_min > np.finfo(np.float32).min and c_max < np.finfo(np.float32).max:\n",
        "                    df[col] = df[col].astype(np.float32)\n",
        "                else:\n",
        "                    df[col] = df[col].astype(np.float64)\n",
        "    end_mem = df.memory_usage().sum() / 1024**2\n",
        "    start_mem = end_mem\n",
        "    #print('Memory usage after optimization 2 is: {:.2f} MB'.format(end_mem))\n",
        "    #print(\"The reduced dataframe 2 has {} columns.\".format(df.shape[1]))\n",
        "    return df"
      ],
      "id": "10f4e1ec"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "0799027e-5b18-4093-ad16-f262fa2aac72",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 122
        },
        "outputId": "0c433f86-9485-4497-93b7-871a7fcfdb6a"
      },
      "outputs": [
        {
          "output_type": "error",
          "ename": "NameError",
          "evalue": "name 'targets' is not defined",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<timed exec>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n",
            "\u001b[0;31mNameError\u001b[0m: name 'targets' is not defined"
          ]
        }
      ],
      "source": [
        "%%time\n",
        "targets = reduce_mem_usage(targets)\n",
        "#583node2: 6.32s; 5.94s\n",
        "#589node2: 6138T 6.15s\n",
        "#590node2: 6.85s\n",
        "#594node2: 8.1s"
      ],
      "id": "0799027e-5b18-4093-ad16-f262fa2aac72"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "f077d47e",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 122
        },
        "outputId": "fe6218c8-beda-445a-d467-ca8bb8d89a33"
      },
      "outputs": [
        {
          "output_type": "error",
          "ename": "NameError",
          "evalue": "name 'df' is not defined",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<timed exec>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n",
            "\u001b[0;31mNameError\u001b[0m: name 'df' is not defined"
          ]
        }
      ],
      "source": [
        "%%time\n",
        "# select target-only tickers\n",
        "df = df.loc[df['ticker'].isin(targets['ticker'])].reset_index(drop=True)\n",
        "if CFG.ANALYSE == 1:\n",
        "    print('{:,} tickers: {:,} records'.format(df['ticker'].nunique(), len(df)))\n",
        "#583node2: 14.7s; 12.6s\n",
        "#589node2: 6138T 12.9s\n",
        "#590node2: 12.2s\n",
        "#594node2: 17.4s"
      ],
      "id": "f077d47e"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "488e3552"
      },
      "source": [
        "As I mentioned earlier, the yfiance stock data is not complete. Let's see if we have enough records per ticker."
      ],
      "id": "488e3552"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "df48b47a",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 122
        },
        "outputId": "b70ab3db-72b2-4f2d-a801-4bb2b0e39728"
      },
      "outputs": [
        {
          "output_type": "error",
          "ename": "NameError",
          "evalue": "name 'df' is not defined",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<timed exec>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n",
            "\u001b[0;31mNameError\u001b[0m: name 'df' is not defined"
          ]
        }
      ],
      "source": [
        "%%time\n",
        "record_per_ticker = df.groupby('ticker')['date'].nunique().reset_index().sort_values(by='date')\n",
        "#record_per_ticker\n",
        "#583node2: 27.2s; 23.7s\n",
        "#589node2: 6138T 29.5s\n",
        "#590node2: 25.9s\n",
        "#594node2: 33.8s"
      ],
      "id": "df48b47a"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "3af862b2",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 122
        },
        "outputId": "76d2aace-1106-41aa-fc06-c1842b872cfe"
      },
      "outputs": [
        {
          "output_type": "error",
          "ename": "NameError",
          "evalue": "name 'record_per_ticker' is not defined",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<timed exec>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n",
            "\u001b[0;31mNameError\u001b[0m: name 'record_per_ticker' is not defined"
          ]
        }
      ],
      "source": [
        "%%time\n",
        "if CFG.ANALYSE == 1:\n",
        "    record_per_ticker['date'].hist()\n",
        "#print(record_per_ticker['date'].describe())"
      ],
      "id": "3af862b2"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aff4bb59"
      },
      "source": [
        "There are unfortunately some tickers where the number of records is small.\n",
        "\n",
        "Here I only use tickers with more than 1,000 records."
      ],
      "id": "aff4bb59"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "5161619e-6ce5-4589-899e-95ce45fdbaa2",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 156
        },
        "outputId": "7b33828b-7cbb-4729-adbf-e3024399b481"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "634\n",
            "1980\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "NameError",
          "evalue": "name 'df' is not defined",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<timed exec>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n",
            "\u001b[0;31mNameError\u001b[0m: name 'df' is not defined"
          ]
        }
      ],
      "source": [
        "%%time\n",
        "if CFG.ANALYSE == 1:\n",
        "    print(CFG.START)\n",
        "    print(CFG.END)\n",
        "    ## Dataframe statistics\n",
        "    print(df.shape)\n",
        "    print(df.describe())\n",
        "    print(df.info())\n",
        "    print(df.ticker)\n",
        "#583node2: 1.1GB 40.7s; 5400 (18942554, 8) 1.1GB 33.3s"
      ],
      "id": "5161619e-6ce5-4589-899e-95ce45fdbaa2"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "c494284f-18c3-4bc0-a253-34007d6aa071",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 122
        },
        "outputId": "33630b8f-ff0d-4e63-df0a-db8ff15087d9"
      },
      "outputs": [
        {
          "output_type": "error",
          "ename": "NameError",
          "evalue": "name 'record_per_ticker' is not defined",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<timed exec>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n",
            "\u001b[0;31mNameError\u001b[0m: name 'record_per_ticker' is not defined"
          ]
        }
      ],
      "source": [
        "%%time\n",
        "#TODO: check need for DEBUG\n",
        "if CFG.DEBUG: # debug mode, using small set of data\n",
        "    tickers_with_records = record_per_ticker.query(f'date >= {CFG.START} & date < {CFG.END}')['ticker'].values\n",
        "    #tickers_with_records = tickers_with_records.query('date < 3430')['ticker'].values\n",
        "else:\n",
        "    tickers_with_records = record_per_ticker.query(f'date >= {CFG.START} & date < {CFG.END}')['ticker'].values\n",
        "    #tickers_with_records = tickers_with_records.query('date < 3430')['ticker'].values\n",
        "df_full = df\n",
        "df = df_full.loc[df_full['ticker'].isin(tickers_with_records)].reset_index(drop=True)\n",
        "del df_full\n",
        "\n",
        "## Dataframe statistics\n",
        "if CFG.ANALYSE == 1:\n",
        "    print(df.shape)\n",
        "    print(df.describe())\n",
        "    print(df.info())\n",
        "    print('Here, we use {:,} tickers: {:,} records'.format(df['ticker'].nunique(), len(df)))\n",
        "\n",
        "#round348 node3: 1000-4600 2205 stocks 4,647,601 records\n",
        "#round355 node3: 3000-5000\n",
        "#round368 node3: 1500-4800 501 stocks 1,643,778 records\n",
        "#574node2: 440MB\n",
        "#580node2: 318.1MB\n",
        "#583node2: 409.7MB 5s; ANALYSE=0 4.32s\n",
        "#589node2: 6138T 11.2s\n",
        "#590node2: 4.17s\n",
        "#594node2: 11.2s"
      ],
      "id": "c494284f-18c3-4bc0-a253-34007d6aa071"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "57f9e233",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 122
        },
        "outputId": "3b7a3d95-71b5-4f4d-88b1-5f84787e6d25"
      },
      "outputs": [
        {
          "output_type": "error",
          "ename": "NameError",
          "evalue": "name 'df' is not defined",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<timed exec>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n",
            "\u001b[0;31mNameError\u001b[0m: name 'df' is not defined"
          ]
        }
      ],
      "source": [
        "%%time\n",
        "#EDS= EDS.rename(columns={'bloomberg_ticker':'ticker'})\n",
        "EDSdf = df.merge(EDS,how='left',on='ticker')\n",
        "\n",
        "## Dataframe statistics\n",
        "if CFG.ANALYSE == 1:\n",
        "    print(EDSdf.shape)\n",
        "    print(EDSdf.describe())\n",
        "    print(EDSdf.info())\n",
        "    print('Here, we use {:,} tickers: {:,} records'.format(df['ticker'].nunique(), len(df)))\n",
        "del df\n",
        "gc.collect()\n",
        "#574node2: 2487 stocks\n",
        "#575node2: 4608 1.1GB (20874395,7)\n",
        "#583node2: 5400 409.7MB (6712970,8) 2152 tickers 50.9s ANALYSE=0 55.2s\n",
        "#586node2: all 10y 3m8s\n",
        "#589node2: 6138T 2m19s\n",
        "#590node2: 39.8s 2831\n",
        "#594node2: 3m3s"
      ],
      "id": "57f9e233"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0ddd1c7c-f8e4-4c3e-8624-73d8c5979ef9"
      },
      "source": [
        "### Data preparation"
      ],
      "id": "0ddd1c7c-f8e4-4c3e-8624-73d8c5979ef9"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "06ef532a",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 122
        },
        "outputId": "61c2c16d-778a-4ac4-c50c-59ad2b8ced65"
      },
      "outputs": [
        {
          "output_type": "error",
          "ename": "NameError",
          "evalue": "name 'EDSdf' is not defined",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<timed exec>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n",
            "\u001b[0;31mNameError\u001b[0m: name 'EDSdf' is not defined"
          ]
        }
      ],
      "source": [
        "%%time\n",
        "# Fill NaN values in the 'shares' column with 1000000\n",
        "EDSdf['shares'] = EDSdf['shares'].fillna(1000000)\n",
        "EDSdf['sector'] = EDSdf['sector'].fillna('unknown')\n",
        "EDSdf['industry'] = EDSdf['industry'].fillna('unknown')\n",
        "#TODO: numerical labelencode, have blanks filled, first try if the blanks just get labeled\n",
        "EDSdf['priceToBook_raw'] = EDSdf['priceToBook_raw'].fillna(1) #TODO: adjust 'priceToBook_raw' so it is numerical/int got error of xgboost\n",
        "#EDSdf['lastFiscalYearEnd_raw'] = EDSdf['lastFiscalYearEnd_raw'].fillna(1000000)\n",
        "EDSdf['earningsQuarterlyGrowth_raw'] = EDSdf['earningsQuarterlyGrowth_raw'].fillna(1000000)\n",
        "EDSdf['revenueGrowth_raw'] = EDSdf['revenueGrowth_raw'].fillna(0.01)\n",
        "EDSdf['grossMargins_raw'] = EDSdf['grossMargins_raw'].fillna(0.2)\n",
        "EDSdf['profitMargins_raw'] = EDSdf['profitMargins_raw'].fillna(0.1)\n",
        "# Dividends\n",
        "EDSdf['payoutRatio_raw'] = EDSdf['payoutRatio_raw'].fillna(0.1)\n",
        "EDSdf['dividendRate_raw'] = EDSdf['dividendRate_raw'].fillna(0.05)\n",
        "EDSdf['dividendYield_raw'] = EDSdf['dividendYield_raw'].fillna(0.05)\n",
        "EDSdf['fiveYearAvgDividendYield_raw'] = EDSdf['fiveYearAvgDividendYield_raw'].fillna(0.02)\n",
        "EDSdf['trailingAnnualDividendRate_raw'] = EDSdf['trailingAnnualDividendRate_raw'].fillna(0.04)\n",
        "EDSdf['trailingAnnualDividendYield_raw'] = EDSdf['trailingAnnualDividendYield_raw'].fillna(0.04)\n",
        "#TODO: build elastic net imputer\n",
        "# Convert 'shares' column to numeric, and then round and convert to int\n",
        "EDSdf['shares'] = pd.to_numeric(EDSdf['shares'], errors='coerce').fillna(1000000).round().astype(int)\n",
        "\n",
        "# Filter rows with shares greater than 0\n",
        "df = EDSdf[EDSdf['shares'] > 0]\n",
        "\n",
        "# Only keep fridays\n",
        "df = df.set_index('date')\n",
        "#df = df[df.index.weekday == 4]  # ensure we have only fridays\n",
        "full_data = df\n",
        "if CFG.LEAN == 1:\n",
        "    del df, EDSdf, EDS\n",
        "    gc.collect()\n",
        "if CFG.ANALYSE == 1:\n",
        "    full_data.info()\n",
        "#583node2: 5425 3.4GB; 5200 ok; 5400 2.9GB 58s; 56.4s; 1m1s\n",
        "#TODO: set last weekday of dataset as a friday before only keeping fridays\n",
        "#586node2: all 10y 4m5s\n",
        "#589node2: 6138T 3m37s\n",
        "#590node2: 42.9s\n",
        "#594node2: 3m30s"
      ],
      "id": "06ef532a"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "4a997049-9c88-49c7-88ce-af6d9d8ad79d"
      },
      "outputs": [],
      "source": [
        "#print(EDSdf.GICS_subindustry.head())"
      ],
      "id": "4a997049-9c88-49c7-88ce-af6d9d8ad79d"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ae120253",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 122
        },
        "outputId": "cb373c31-1340-42f4-8e36-df5239860457"
      },
      "outputs": [
        {
          "output_type": "error",
          "ename": "NameError",
          "evalue": "name 'full_data' is not defined",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<timed exec>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n",
            "\u001b[0;31mNameError\u001b[0m: name 'full_data' is not defined"
          ]
        }
      ],
      "source": [
        "%%time\n",
        "full_data['GICS_1']= full_data['GICS_subindustry'].apply(lambda x: str(x)[:2])\n",
        "full_data['GICS_2']= full_data['GICS_subindustry'].apply(lambda x: str(x)[:4])\n",
        "full_data['GICS_3']= full_data['GICS_subindustry'].apply(lambda x: str(x)[:6])\n",
        "#583node2: 13.4s\n",
        "#586node2: 59s\n",
        "#589node2: 6138T 37.8s\n",
        "#590node2: 20.2s\n",
        "#594node2: 47s"
      ],
      "id": "ae120253"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "629a10ef-df89-4fb4-886a-b76a1cd36704",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 122
        },
        "outputId": "ae9b41d0-cfed-4512-d7de-52a7bb55a3d5"
      },
      "outputs": [
        {
          "output_type": "error",
          "ename": "NameError",
          "evalue": "name 'full_data' is not defined",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<timed exec>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n",
            "\u001b[0;31mNameError\u001b[0m: name 'full_data' is not defined"
          ]
        }
      ],
      "source": [
        "%%time\n",
        "if CFG.ANALYSE == 1:\n",
        "    print(full_data.GICS_2.unique())"
      ],
      "id": "629a10ef-df89-4fb4-886a-b76a1cd36704"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "410d83cb-6473-4846-bf9b-0409abefdd50"
      },
      "source": [
        "## possibility to subsample/ filter"
      ],
      "id": "410d83cb-6473-4846-bf9b-0409abefdd50"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "0de9b999-bd10-4ef9-b400-a9bbe928d0fa"
      },
      "outputs": [],
      "source": [
        "## Select semiconductors based on Industry\n",
        "\n",
        "#filter on GICS 4 character Industry Group full_data['GICS_2'] = 4530\n",
        "#full_data = full_data[full_data['GICS_2'] == '4530']\n",
        "#round559 64 stocks"
      ],
      "id": "0de9b999-bd10-4ef9-b400-a9bbe928d0fa"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "a997c1d8",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 122
        },
        "outputId": "5a6e81ec-64e9-482a-b81f-48747ca570bb"
      },
      "outputs": [
        {
          "output_type": "error",
          "ename": "NameError",
          "evalue": "name 'preprocessing' is not defined",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<timed exec>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n",
            "\u001b[0;31mNameError\u001b[0m: name 'preprocessing' is not defined"
          ]
        }
      ],
      "source": [
        "%%time\n",
        "# Import label encoder\n",
        "# label_encoder object knows how to understand word labels.\n",
        "label_encoder = preprocessing.LabelEncoder()\n",
        "# Encode labels in column 'Country'.\n",
        "full_data['GICS_1']= label_encoder.fit_transform(full_data['GICS_1'])\n",
        "full_data['GICS_2']= label_encoder.fit_transform(full_data['GICS_2'])\n",
        "full_data['GICS_3']= label_encoder.fit_transform(full_data['GICS_3'])\n",
        "full_data['GICS_subindustry']= label_encoder.fit_transform(full_data['GICS_subindustry'])\n",
        "# Fit and transform the country, currency and ISIN columns\n",
        "full_data.rename(columns={\"Country_of_risk_name\": \"CountryRisk_name\"}, inplace=True)\n",
        "full_data.rename(columns={\"Country_of_domicile\": \"CountryDomicile_ISO2\"}, inplace=True)\n",
        "\n",
        "full_data['CountryRisk_name'] = label_encoder.fit_transform(full_data['CountryRisk_name'])\n",
        "full_data['CountryDomicile_ISO2'] = label_encoder.fit_transform(full_data['CountryDomicile_ISO2'])\n",
        "full_data['Currency'] = label_encoder.fit_transform(full_data['Currency'])\n",
        "full_data['ISIN_code'] = label_encoder.fit_transform(full_data['ISIN_code'])\n",
        "full_data['sector'] = label_encoder.fit_transform(full_data['sector'])\n",
        "full_data['industry'] = label_encoder.fit_transform(full_data['industry'])\n",
        "#575node2: 30GB in memory here with peak of 50GB 1m59s\n",
        "#581node2: 17.8S\n",
        "#583node2: 17.7s; 26.1s\n",
        "#586node2: all 10y 1m25s\n",
        "#589node2: 6138T 1m15s\n",
        "#590node2: 30.4s\n",
        "#594node2: 1m15s"
      ],
      "id": "a997c1d8"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "adcdc0e3"
      },
      "outputs": [],
      "source": [
        "#full_data['Country_of_risk_name']= label_encoder.fit_transform(full_data['Country_of_risk_name'])\n",
        "#full_data['ISIN_code']= label_encoder.fit_transform(full_data['ISIN_code'])\n",
        "#full_data['Name']= label_encoder.fit_transform(full_data['Name'])\n",
        "KEEP_FEAT = ['ticker', 'Name', 'investment_id','target_4d','target_20d','SEDOL']"
      ],
      "id": "adcdc0e3"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "74e3f76d",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 122
        },
        "outputId": "a7d10fba-e77a-4227-eb21-6cc25a2538f5"
      },
      "outputs": [
        {
          "output_type": "error",
          "ename": "NameError",
          "evalue": "name 'full_data' is not defined",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<timed exec>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n",
            "\u001b[0;31mNameError\u001b[0m: name 'full_data' is not defined"
          ]
        }
      ],
      "source": [
        "%%time\n",
        "# save result dataframe\n",
        "full_data.reset_index(inplace=True)\n",
        "#full_data.astype('float32', errors='ignore').to_parquet(pathlib.Path(f'{CFG.OUTPUT_DIR}/stock_f_tgt_encoded_data.parquet'))\n",
        "# Check dataframe min date, max date, day count, number of stocks\n",
        "if CFG.ANALYSE == 1:\n",
        "    print(full_data.groupby('ticker')['date'].agg(['min', 'max', 'count']))\n",
        "    print(full_data.isnull().sum())\n",
        "df = full_data\n",
        "if CFG.LEAN == 1:\n",
        "    del full_data\n",
        "    gc.collect()\n",
        "#round 332 node1:2 months of data\n",
        "#581node2:11.5S\n",
        "#583node2: 12.6s; 16.2s; ANALYSE=0 2s\n",
        "#586node2: all 10y4.93s\n",
        "#589node2: 6138T 2.31s"
      ],
      "id": "74e3f76d"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "fb4c9f71-17e8-4acc-ab72-3ab003e97396"
      },
      "outputs": [],
      "source": [
        "def expand_mem_usage(df, verbose=False):\n",
        "    start_mem = df.memory_usage().sum() / 1024**2\n",
        "    #print('Memory usage before optimization 2 is: {:.2f} MB'.format(start_mem))\n",
        "    #print(\"The dataframe 2 has {} columns.\".format(df.shape[1]))\n",
        "    halfs = ['int16','float16']\n",
        "    for col in df.columns:\n",
        "        col_type = df[col].dtypes\n",
        "        if col_type in halfs:\n",
        "            c_min = df[col].min()\n",
        "            c_max = df[col].max()\n",
        "            if str(col_type)[:3] == 'int':\n",
        "                if c_min > np.iinfo(np.int8).min and c_max < np.iinfo(np.int8).max:\n",
        "                    df[col] = df[col].astype(np.int8)\n",
        "                elif c_min > np.iinfo(np.int16).min and c_max < np.iinfo(np.int16).max:\n",
        "                    df[col] = df[col].astype(np.int32)\n",
        "                elif c_min > np.iinfo(np.int32).min and c_max < np.iinfo(np.int32).max:\n",
        "                    df[col] = df[col].astype(np.int32)\n",
        "                elif c_min > np.iinfo(np.int64).min and c_max < np.iinfo(np.int64).max:\n",
        "                    df[col] = df[col].astype(np.int64)\n",
        "            else:\n",
        "                if c_min > np.finfo(np.float16).min and c_max < np.finfo(np.float16).max:\n",
        "                    df[col] = df[col].astype(np.float32)\n",
        "                elif c_min > np.finfo(np.float32).min and c_max < np.finfo(np.float32).max:\n",
        "                    df[col] = df[col].astype(np.float32)\n",
        "                else:\n",
        "                    df[col] = df[col].astype(np.float64)\n",
        "    end_mem = df.memory_usage().sum() / 1024**2\n",
        "    start_mem = end_mem\n",
        "    #print('Memory usage after optimization 2 is: {:.2f} MB'.format(end_mem))\n",
        "    #print(\"The reduced dataframe 2 has {} columns.\".format(df.shape[1]))\n",
        "    return df"
      ],
      "id": "fb4c9f71-17e8-4acc-ab72-3ab003e97396"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "16ab2169-18a8-4b9c-b06d-5f39507601a0",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 122
        },
        "outputId": "e6248c90-94c1-4664-b80c-73e571352983"
      },
      "outputs": [
        {
          "output_type": "error",
          "ename": "NameError",
          "evalue": "name 'df' is not defined",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<timed exec>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n",
            "\u001b[0;31mNameError\u001b[0m: name 'df' is not defined"
          ]
        }
      ],
      "source": [
        "%%time\n",
        "#df = reduce_mem_usage(df)\n",
        "if CFG.ANALYSE == 1:\n",
        "    print(df.info())\n",
        "#df.dtypes\n",
        "#575node2: 8.6GB dataset\n",
        "#580node2: 2GB\n",
        "#df = reduce_mem_usage(df)\n",
        "#581node2: 2.1GB\n",
        "#583node2: 2.1GB; 3GB"
      ],
      "id": "16ab2169-18a8-4b9c-b06d-5f39507601a0"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "d531c0fd"
      },
      "outputs": [],
      "source": [
        "# Install pycaret\n",
        "#!pip install scikit_learn-0.23.2-cp37-cp37m-manylinux1_x86_64.whl\n",
        "#!pip install scikit-learn==0.23.2\n",
        "#import sys\n",
        "#sys.path.append('pycaret-master/pycaret-master')\n",
        "#sys.path.append('pyod-master/pyod-master')"
      ],
      "id": "d531c0fd"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "eec20704",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 122
        },
        "outputId": "e59d3303-1c25-45e0-c9b7-7493daa3efeb"
      },
      "outputs": [
        {
          "output_type": "error",
          "ename": "NameError",
          "evalue": "name 'df' is not defined",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<timed exec>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n",
            "\u001b[0;31mNameError\u001b[0m: name 'df' is not defined"
          ]
        }
      ],
      "source": [
        "%%time\n",
        "# first, fix date column in the yfiance stock data to be friday date (just naming along with numerai targets)\n",
        "#df['friday_date'] = df['date'].apply(lambda x : int(str(x).replace('-', '')))\n",
        "\n",
        "## change date different for owm data from yahoofinance\n",
        "df['friday_date'] = df['date'].apply(lambda x: str(x).replace('-', ''))#.dt.normalize()\n",
        "\n",
        "#print(df.head())\n",
        "#print(df.dtypes)\n",
        "# takes some time till memory release of df merged with EDS\n",
        "gc.collect()\n",
        "#586node2: 23s\n",
        "#589node2: 6138T 12.3s\n",
        "#590node2: 7.93s\n",
        "#594node2: 17s"
      ],
      "id": "eec20704"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "16677ae5",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 122
        },
        "outputId": "77bd144c-19ea-4512-c606-3bd5b184fcc5"
      },
      "outputs": [
        {
          "output_type": "error",
          "ename": "NameError",
          "evalue": "name 'record_per_ticker' is not defined",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<timed exec>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n",
            "\u001b[0;31mNameError\u001b[0m: name 'record_per_ticker' is not defined"
          ]
        }
      ],
      "source": [
        "%%time\n",
        "if CFG.LEAN == 1:\n",
        "    del record_per_ticker, d, ticker_map\n",
        "    gc.collect()\n",
        "#583node2: 1.77s\n",
        "#586node2: 4.46s\n",
        "#589node2: 6138T 1.68s"
      ],
      "id": "16677ae5"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "3572d532",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "31291de9-5ce5-47b8-8399-66575fbdf793"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Most recent Friday: 20240830\n",
            "CPU times: user 966 µs, sys: 0 ns, total: 966 µs\n",
            "Wall time: 1.96 ms\n"
          ]
        }
      ],
      "source": [
        "%%time\n",
        "# recent friday date?\n",
        "recent_friday = datetime.now() + relativedelta(weekday=FR(-1))\n",
        "recent_friday = int(recent_friday.strftime('%Y%m%d'))\n",
        "if CFG.ANALYSE == 1:\n",
        "    print(f'Most recent Friday: {recent_friday}')"
      ],
      "id": "3572d532"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "822d938b",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "ee1c800e-8c1d-4bc0-90c4-85479786e9ce"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Second most recent Friday: 20240823\n",
            "CPU times: user 800 µs, sys: 0 ns, total: 800 µs\n",
            "Wall time: 897 µs\n"
          ]
        }
      ],
      "source": [
        "%%time\n",
        "# in case no recent friday is available...prep the second last\n",
        "recent_friday2 = datetime.now() + relativedelta(weekday=FR(-2))\n",
        "recent_friday2 = int(recent_friday2.strftime('%Y%m%d'))\n",
        "if CFG.ANALYSE == 1:\n",
        "    print(f'Second most recent Friday: {recent_friday2}')"
      ],
      "id": "822d938b"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "0e5ad4ef",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 154
        },
        "outputId": "fa13c967-d6b9-4ac5-e8ba-81cbbce99e1f"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "\"\\n    if np.sum(df['friday_date'] == recent_friday) < 4000:\\n        previous_tickers = set(df.query('friday_date == @recent_friday2')['ticker'])\\n        current_tickers = set(df.query('friday_date == @recent_friday')['ticker'])\\n        missing_df = pd.DataFrame()\\n        missing_df['ticker'] = list(previous_tickers - current_tickers)\\n        for d in ['date', 'friday_date']:\\n            missing_df[d] = recent_friday\\n\\n        # concat\\n        orig_shape = df.shape\\n        df = pd.concat([df, missing_df]).sort_values(by=['ticker', 'friday_date']).fillna(method='ffill')\\n        del missing_df\\n        print('Resolving missing tickers due to market-not-open-on-friday issue: df shape {} => {}'.format(\\n            orig_shape, df.shape\\n        ))\\n\""
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 65
        }
      ],
      "source": [
        "# fix market-not-open-on-Friday problem\n",
        "\"\"\"\n",
        "    if np.sum(df['friday_date'] == recent_friday) < 4000:\n",
        "        previous_tickers = set(df.query('friday_date == @recent_friday2')['ticker'])\n",
        "        current_tickers = set(df.query('friday_date == @recent_friday')['ticker'])\n",
        "        missing_df = pd.DataFrame()\n",
        "        missing_df['ticker'] = list(previous_tickers - current_tickers)\n",
        "        for d in ['date', 'friday_date']:\n",
        "            missing_df[d] = recent_friday\n",
        "\n",
        "        # concat\n",
        "        orig_shape = df.shape\n",
        "        df = pd.concat([df, missing_df]).sort_values(by=['ticker', 'friday_date']).fillna(method='ffill')\n",
        "        del missing_df\n",
        "        print('Resolving missing tickers due to market-not-open-on-friday issue: df shape {} => {}'.format(\n",
        "            orig_shape, df.shape\n",
        "        ))\n",
        "\"\"\""
      ],
      "id": "0e5ad4ef"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "55e43103-3994-4e6b-bc05-d26b13570ba2"
      },
      "source": [
        "# Feature Engineering\n",
        "Yeah finally machine learning part!\n",
        "\n",
        "Here we generate sets of stock price features. There are some caveats to be aware of:\n",
        "\n",
        "- **No Leak**: we cannot use a feature which uses the future information (this is a forecasting task!)\n",
        "- **Stationaly features**: Our features have to work whenever (scales must be stationaly over the periods of time)\n",
        "\n",
        "The implementation of the feature engineering is derived from [J-Quants Tournament](https://japanexchangegroup.github.io/J-Quants-Tutorial/#anchor-2.7). Although this content is in Japanese, I believe this is one of the best resources for feature engineering in the finance domain.\n",
        "\n",
        "Also I add the RSI and MACD (PPO) features as a bonus:D\n",
        "\n",
        "We generate features per ticker repeatedly. To accelerate the process, we use the parallel processing."
      ],
      "id": "55e43103-3994-4e6b-bc05-d26b13570ba2"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "26532731",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 207
        },
        "outputId": "c74964bc-7346-4478-8de6-e91bd86977ae"
      },
      "outputs": [
        {
          "output_type": "error",
          "ename": "NameError",
          "evalue": "name 'df' is not defined",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-66-279a83ad879a>\u001b[0m in \u001b[0;36m<cell line: 90>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     88\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     89\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mpd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mSeries\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mskew\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 90\u001b[0;31m \u001b[0;32mdef\u001b[0m \u001b[0mfeature_engineering\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mticker\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'ZEAL DC'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdf\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdf\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     91\u001b[0m     \"\"\"\n\u001b[1;32m     92\u001b[0m     \u001b[0mfeature\u001b[0m \u001b[0mengineering\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mNameError\u001b[0m: name 'df' is not defined"
          ]
        }
      ],
      "source": [
        "\n",
        "# technical indicators\n",
        "def log_return(series, periods=1):\n",
        "    return np.log(series).diff(periods=periods)\n",
        "def log_return2(series, periods=2):\n",
        "    return np.log(series).diff(periods=periods)\n",
        "def log_return3(series, periods=3):\n",
        "    return np.log(series).diff(periods=periods)\n",
        "def realized_volatility(series):\n",
        "    return np.sqrt(np.sum(series**2))\n",
        "\n",
        "def standardize(df):\n",
        "    std = preprocessing.StandardScaler()\n",
        "    st_feat = std.fit_transform(df.values)\n",
        "    return df\n",
        "#TODO: fix poly naming\n",
        "def add_poly(df, features, deg=2):\n",
        "    \"\"\"\n",
        "    add polynomial features\n",
        "    \"\"\"\n",
        "    poly = preprocessing.PolynomialFeatures(deg)\n",
        "    emb = poly.fit_transform(df[features].values)\n",
        "\n",
        "    for i in range(len(features), emb.shape[1]):\n",
        "        df[f'poly_{i+1}'] = emb[:, i]\n",
        "        #df[df_names[i]] = emb[:, i]\n",
        "    return df\n",
        "\n",
        "def RSI(close: pd.DataFrame, period: int = 14) -> pd.Series:\n",
        "    # https://gist.github.com/jmoz/1f93b264650376131ed65875782df386\n",
        "    \"\"\"See source https://github.com/peerchemist/finta\n",
        "    and fix https://www.tradingview.com/wiki/Talk:Relative_Strength_Index_(RSI)\n",
        "    Relative Strength Index (RSI) is a momentum oscillator that measures the speed and change of price movements.\n",
        "    RSI oscillates between zero and 100. Traditionally, and according to Wilder, RSI is considered overbought when above 70 and oversold when below 30.\n",
        "    Signals can also be generated by looking for divergences, failure swings and centerline crossovers.\n",
        "    RSI can also be used to identify the general trend.\"\"\"\n",
        "\n",
        "    delta = close.diff()\n",
        "\n",
        "    up, down = delta.copy(), delta.copy()\n",
        "    up[up < 0] = 0\n",
        "    down[down > 0] = 0\n",
        "\n",
        "    _gain = up.ewm(com=(period - 1), min_periods=period).mean()\n",
        "    _loss = down.abs().ewm(com=(period - 1), min_periods=period).mean()\n",
        "\n",
        "    RS = _gain / _loss\n",
        "    return pd.Series(100 - (100 / (1 + RS)))\n",
        "\n",
        "def EMA1(x, n):\n",
        "    \"\"\"\n",
        "    https://qiita.com/MuAuan/items/b08616a841be25d29817\n",
        "    \"\"\"\n",
        "    a= 2/(n+1)\n",
        "    return pd.Series(x).ewm(alpha=a).mean()\n",
        "\n",
        "def MACD(close : pd.DataFrame, span1=12, span2=26, span3=9):\n",
        "    \"\"\"\n",
        "    Compute MACD\n",
        "    # https://www.learnpythonwithrune.org/pandas-calculate-the-moving-average-convergence-divergence-macd-for-a-stock/\n",
        "    \"\"\"\n",
        "    exp1 = EMA1(close, span1)\n",
        "    exp2 = EMA1(close, span2)\n",
        "    macd = 100 * (exp1 - exp2) / exp2\n",
        "    signal = EMA1(macd, span3)\n",
        "\n",
        "    return macd, signal\n",
        "def POW(x, n):\n",
        "    \"\"\"\n",
        "    Return Exponential power of series and other, element-wise (binary operator pow).\n",
        "    # https://pandas.pydata.org/docs/reference/api/pandas.Series.pow.html\n",
        "    \"\"\"\n",
        "\n",
        "    return pd.Series(x).pow(n, fill_value=0)\n",
        "\n",
        "def STD(x):\n",
        "    \"\"\"\n",
        "\n",
        "    # https://pandas.pydata.org/docs/reference/api/pandas.Series.std.html\n",
        "    \"\"\"\n",
        "\n",
        "    return pd.Series(x).std()\n",
        "\n",
        "def SKEW(x):\n",
        "    \"\"\"\n",
        "\n",
        "    # https://pandas.pydata.org/docs/reference/api/pandas.Series.skew.html\n",
        "    \"\"\"\n",
        "\n",
        "    return pd.Series(x).skew()\n",
        "def feature_engineering(ticker='ZEAL DC', df=df):\n",
        "    \"\"\"\n",
        "    feature engineering\n",
        "\n",
        "    :INPUTS:\n",
        "    - ticker : numerai ticker name (str)\n",
        "    - df : yfinance dataframe (pd.DataFrame)\n",
        "\n",
        "    :OUTPUTS:\n",
        "    - feature_df : feature engineered dataframe (pd.DataFrame)\n",
        "    \"\"\"\n",
        "    # init\n",
        "    keys = ['friday_date', 'ticker']\n",
        "    feature_df = df.query(f'ticker == \"{ticker}\"')\n",
        "\n",
        "    # price features\n",
        "    new_feats = []\n",
        "    # Generate random variable from uniform distribution\n",
        "    feature_df[\"random_uniform\"] = np.random.uniform(0, 1, len(feature_df))\n",
        "    feature_df[\"random_normal\"] = np.random.randn(len(feature_df))\n",
        "    categories = [0, 1, 2, 3]\n",
        "    feature_df[\"random_category\"] = np.random.choice(categories, len(feature_df))\n",
        "\n",
        "    for i, f in enumerate(['close', ]):\n",
        "        for x in [4,5,6,7,8,9,10,11,12,13,14,15,16,17,18,19,\n",
        "                  20,21,22,23,24,25,26,27,28,29,\n",
        "                  30,31,32,33,34,35,36,37,38,\n",
        "                  39,40,41,42,43,44,45,46,47,48,49,51,52,53,54,\n",
        "                  55,57,60,80,89,100,101,102,103,104,105,106,107,108,109,110,\n",
        "                  111,112,113,114,120,130,144,150,160,170,180,190,200,210,220,250,270,300\n",
        "                 ]:\n",
        "            # return\n",
        "            feature_df[f\"{f}_return_{x}days\"] = feature_df[\n",
        "                f\n",
        "            ].pct_change(x)\n",
        "\n",
        "            # volatility\n",
        "            feature_df[f\"{f}_volatility_{x}days\"] = (\n",
        "                np.log1p(feature_df[f])\n",
        "                .pct_change()\n",
        "                .rolling(x)\n",
        "                .std()\n",
        "            )\n",
        "\n",
        "            # volatility change\n",
        "            feature_df[f\"{f}_volatility_{x}days\"] = (\n",
        "                np.log1p(feature_df[f])\n",
        "                .pct_change()\n",
        "                .pct_change()\n",
        "                .rolling(x)\n",
        "                .std()\n",
        "            )\n",
        "            # kairi mean\n",
        "            feature_df[f\"{f}_MA_gap_{x}days\"] = feature_df[f] / (\n",
        "                feature_df[f].rolling(x).mean()\n",
        "            )\n",
        "\n",
        "            # features to use\n",
        "            new_feats += [\n",
        "                f\"{f}_return_{x}days\",\n",
        "                f\"{f}_volatility_{x}days\",\n",
        "                f\"{f}_MA_gap_{x}days\",\n",
        "                         ]\n",
        "    feature_df[\"return_1month\"] = feature_df[\"close\"].pct_change(20)\n",
        "    feature_df[\"return_2month\"] = feature_df[\"close\"].pct_change(40)\n",
        "    feature_df[\"return_3month\"] = feature_df[\"close\"].pct_change(60)\n",
        "    feature_df[\"close_return_251days\"] = feature_df['close'].pct_change(251)\n",
        "    feature_df[\"volatility_1month\"] = (\n",
        "        np.log(feature_df[\"close\"]).diff().rolling(20).std()\n",
        "    )\n",
        "    feature_df[\"volatility_2month\"] = (\n",
        "        np.log(feature_df[\"close\"]).diff().rolling(40).std()\n",
        "    )\n",
        "    feature_df[\"volatility_3month\"] = (\n",
        "        np.log(feature_df[\"close\"]).diff().rolling(60).std()\n",
        "    )\n",
        "    feature_df[\"MA_gap_1month\"] = feature_df[\"close\"] / (\n",
        "        feature_df[\"close\"].rolling(20).mean()\n",
        "    )\n",
        "    feature_df[\"MA_gap_2month\"] = feature_df[\"close\"] / (\n",
        "        feature_df[\"close\"].rolling(40).mean()\n",
        "    )\n",
        "    feature_df[\"MA_gap_3month\"] = feature_df[\"close\"] / (\n",
        "        feature_df[\"close\"].rolling(60).mean()\n",
        "    )\n",
        "    feature_df['max30d_close'] = feature_df['close'].shift().rolling(min_periods=1,window=30).max()\n",
        "    feature_df['min30d_close'] = feature_df['close'].shift().rolling(min_periods=1,window=30).min()\n",
        "    feature_df['mean30d_close'] = feature_df['close'].shift().rolling(min_periods=1,window=30).mean()\n",
        "    feature_df['max30drange'] = feature_df['close'] / feature_df['max30d_close']\n",
        "    feature_df['min30drange'] = feature_df['close'] / feature_df['min30d_close']\n",
        "    feature_df['mean30drange'] = feature_df['close'] / feature_df['mean30d_close']\n",
        "    #feature_df['max30drange_q'] = pd.qcut(feature_df['max30drange'], q=[0, 0.2, 0.4, 0.6, 0.8, 1], labels=[\"1\", \"2\", \"3\", \"4\", \"5\"],duplicates='raise')\n",
        "    #feature_df['min30drange_q'] = pd.qcut(feature_df['min30drange'], q=[0, 0.2, 0.4, 0.6, 0.8, 1], labels=[\"1\", \"2\", \"3\", \"4\", \"5\"],duplicates='raise')\n",
        "    #feature_df['mean30drange_q'] = pd.qcut(feature_df['mean30drange'], q=[0, 0.2, 0.4, 0.6, 0.8, 1], labels=[\"1\", \"2\", \"3\", \"4\", \"5\"],duplicates='raise')\n",
        "    # TODO add PCA\n",
        "    # RSI\n",
        "    feature_df['RSI2'] = RSI(feature_df['close'], 2)\n",
        "    feature_df['RSI3'] = RSI(feature_df['close'], 3)\n",
        "    feature_df['RSI3_shift1'] = RSI(feature_df['close'], 3).shift(periods=1)\n",
        "    feature_df['RSI4'] = RSI(feature_df['close'], 4)\n",
        "    feature_df['RSI5'] = RSI(feature_df['close'], 5)\n",
        "    feature_df['RSI6'] = RSI(feature_df['close'], 6)\n",
        "    feature_df['RSI7'] = RSI(feature_df['close'], 7)\n",
        "    feature_df['RSI8'] = RSI(feature_df['close'], 8)\n",
        "    feature_df['RSI9'] = RSI(feature_df['close'], 9)\n",
        "    feature_df['RSI10'] = RSI(feature_df['close'], 10)\n",
        "    feature_df['RSI11'] = RSI(feature_df['close'], 11)\n",
        "    feature_df['RSI12'] = RSI(feature_df['close'], 12)\n",
        "    feature_df['RSI13'] = RSI(feature_df['close'], 13)\n",
        "    feature_df['RSI'] = RSI(feature_df['close'], 14)\n",
        "    feature_df['RSI20'] = RSI(feature_df['close'], 20)\n",
        "    feature_df['RSI40'] = RSI(feature_df['close'], 40)\n",
        "    feature_df['RSI60'] = RSI(feature_df['close'], 60)\n",
        "    feature_df['RSI80'] = RSI(feature_df['close'], 80)\n",
        "    feature_df['RSI100'] = RSI(feature_df['close'], 100)\n",
        "    feature_df['RSI120'] = RSI(feature_df['close'], 120)\n",
        "    feature_df['RSI140'] = RSI(feature_df['close'], 140)\n",
        "\n",
        "    # MACD\n",
        "    macd, macd_signal = MACD(feature_df['close'], 12, 26, 9)\n",
        "    feature_df['MACD'] = macd\n",
        "    feature_df['MACD_signal'] = macd_signal\n",
        "    macd0, macd_signal0 = MACD(feature_df['close'], 10, 24, 8)\n",
        "    feature_df['MACD0'] = macd0\n",
        "    feature_df['MACD_signal0'] = macd_signal0\n",
        "    macd2, macd_signal2 = MACD(feature_df['close'], 20, 48, 16)\n",
        "    feature_df['MACD2'] = macd2\n",
        "    feature_df['MACD_signal2'] = macd_signal2\n",
        "    macd3, macd_signal3 = MACD(feature_df['close'], 30, 72, 24)\n",
        "    feature_df['MACD3'] = macd3\n",
        "    feature_df['MACD_signal3'] = macd_signal3\n",
        "    macd4, macd_signal4 = MACD(feature_df['close'], 40, 96, 32)\n",
        "    feature_df['MACD4'] = macd4\n",
        "    feature_df['MACD_signal4'] = macd_signal4\n",
        "    macd5, macd_signal5 = MACD(feature_df['close'], 52, 128, 36)\n",
        "    feature_df['MACD5'] = macd5\n",
        "    feature_df['MACD_signal5'] = macd_signal5\n",
        "    macd6, macd_signal6 = MACD(feature_df['close'], 52, 128, 48)\n",
        "    feature_df['MACD6'] = macd6\n",
        "    feature_df['MACD_signal6'] = macd_signal6\n",
        "    macd7, macd_signal7 = MACD(feature_df['close'], 60, 128, 48)\n",
        "    feature_df['MACD7'] = macd7\n",
        "    feature_df['MACD_signal7'] = macd_signal7\n",
        "\n",
        "    #TODO: remove part and work only with lowercase to avoid inconsistency it is now used:\n",
        "    feature_df['Close'] = feature_df[\"close\"]\n",
        "    feature_df['Open'] = feature_df[\"open\"]\n",
        "    feature_df['Low'] = feature_df[\"low\"]\n",
        "    feature_df['High'] = feature_df[\"high\"]\n",
        "    feature_df['Volume'] = feature_df[\"volume\"]\n",
        "\n",
        "    feature_df['LRET'] = log_return(feature_df[\"close\"])\n",
        "    feature_df['LRET_OPEN'] = log_return(feature_df[\"open\"])\n",
        "    feature_df['LRET_LOW'] = log_return(feature_df[\"low\"])\n",
        "    feature_df['LRET_HIGH'] = log_return(feature_df[\"high\"])\n",
        "    feature_df['VOL_LRET'] = log_return(feature_df[\"volume\"])\n",
        "    fvol = feature_df[\"close\"]*feature_df[\"volume\"]\n",
        "    feature_df['FVOL_LRET'] = log_return(fvol)\n",
        "    feature_df[\"fvol\"] = fvol\n",
        "    feature_df['mcap']=(feature_df['shares']*feature_df['Close'])/1000 #mcap in thousands\n",
        "    feature_df['Daily_Range'] = feature_df['Close'] - feature_df['Open']\n",
        "    feature_df['Daily_Range_shift1'] = feature_df['Close'] - feature_df['Open'].shift(periods=1)\n",
        "    feature_df['Daily_Range_shift2'] = feature_df['Close'] - feature_df['Open'].shift(periods=2)\n",
        "    feature_df['Daily_Range_shift3'] = feature_df['Close'] - feature_df['Open'].shift(periods=3)\n",
        "    feature_df['Openz'] = stats.zscore(feature_df['Open'])\n",
        "    feature_df['Highz'] = stats.zscore(feature_df['High'])\n",
        "    feature_df['Lowz'] = stats.zscore(feature_df['Low'])\n",
        "    feature_df['Closez'] = stats.zscore(feature_df['Close'])\n",
        "    feature_df['Volumez'] = stats.zscore(feature_df['Volume'])\n",
        "    feature_df['Daily_Rangez'] = stats.zscore(feature_df['Daily_Range'])\n",
        "    feature_df['Daily_Range_shift1z'] = stats.zscore(feature_df['Daily_Range_shift1'])\n",
        "    feature_df['Daily_Range_shift2z'] = stats.zscore(feature_df['Daily_Range_shift2'])\n",
        "    feature_df['Daily_Range_shift3z'] = stats.zscore(feature_df['Daily_Range_shift3'])\n",
        "    feature_df['Daily_Rangez_shift1'] = feature_df['Daily_Rangez'].shift(periods=1)\n",
        "    feature_df['Daily_Rangez_shift2'] = feature_df['Daily_Rangez'].shift(periods=2)\n",
        "    feature_df['Daily_Rangez_shift3'] = feature_df['Daily_Rangez'].shift(periods=3)\n",
        "    feature_df['Mcapz'] = stats.zscore(feature_df['mcap'])\n",
        "    feature_df[\"Fvolz\"] = stats.zscore(feature_df[\"fvol\"])\n",
        "    feature_df['close_return_251daysz'] = stats.zscore(feature_df['close_return_251days'])\n",
        "\n",
        "    # fill nan\n",
        "    feature_df.fillna(method='ffill', inplace=True) # safe fillna method for a forecasting task\n",
        "    feature_df.fillna(method='bfill', inplace=True) # just in case ... making sure no nan\n",
        "    #clean DF\n",
        "    feature_df = feature_df.replace([np.inf, -np.inf, np.nan], -20000)\n",
        "    feature_df.dropna(inplace=True)\n",
        "\n",
        "    # Polynomial features\n",
        "    poly_features = ['close_return_251daysz','Daily_Rangez','Openz','Lowz','Closez','Highz','FVOL_LRET','VOL_LRET','LRET','Fvolz','Mcapz']\n",
        "    poly = add_poly(feature_df, poly_features, deg=4)  #'Volumez','close','volume',, \"return_1month\",'MACD','MACD_signal','RSI20'\n",
        "\n",
        "    #sklearn.preprocessing.PolynomialFeatures\n",
        "    #new_feats.extend(feature_df.filter(like='poly', axis=0).columns)\n",
        "\n",
        "    for i, f in enumerate(['Volumez','LRET','VOL_LRET','FVOL_LRET','Fvolz','Mcapz']):\n",
        "        for x in [4,5,6,7,8,9,10,11,12,13,14,15,16,17,18,19,\n",
        "                  20,21,22,23,24,25,26,27,28,29,\n",
        "                  30,31,32,33,34,35,36,37,38,\n",
        "                  39,40,41,42,43,44,45,46,47,48,49,51,52,53,54,\n",
        "                  55,57,60,80,89,100,101,102,103,104,105,106,107,108,109,110,\n",
        "                  111,112,113,114,120,130,144,150,160,170,180,190,200,210,220,250,270,300\n",
        "                 ]:\n",
        "            # return\n",
        "            feature_df[f\"vol_{f}_return_{x}days\"] = feature_df[\n",
        "                f\n",
        "            ].pct_change(x)\n",
        "\n",
        "            # return absolute\n",
        "            feature_df[f\"vol_{f}_return_abs_{x}days\"] = feature_df[\n",
        "                f\n",
        "            ].pct_change(x).abs()\n",
        "\n",
        "            # volatility\n",
        "            feature_df[f\"vol_{f}_volatility_{x}days\"] = (\n",
        "                np.log1p(feature_df[f])\n",
        "                .pct_change()\n",
        "                .rolling(x)\n",
        "                .std()\n",
        "            )\n",
        "\n",
        "            # kairi mean\n",
        "            feature_df[f\"vol_{f}_MA_gap_{x}days\"] = feature_df[f] / (\n",
        "                feature_df[f].rolling(x).mean()\n",
        "            )\n",
        "\n",
        "            # features to use\n",
        "            new_feats += [\n",
        "                f\"vol_{f}_return_{x}days\",\n",
        "\n",
        "                f\"vol_{f}_return_abs_{x}days\",\n",
        "\n",
        "                f\"vol_{f}_volatility_{x}days\",\n",
        "\n",
        "                f\"vol_{f}_MA_gap_{x}days\",\n",
        "                ]\n",
        "\n",
        "\n",
        "    # Financial volume\n",
        "    feature_df['fvol_shift1'] = feature_df.Volume.shift(periods=1) * feature_df.Close.shift(periods=1)\n",
        "    feature_df['fvol_shift2'] = feature_df.Volume.shift(periods=2) * feature_df.Close.shift(periods=2)\n",
        "    feature_df['fvol_shift3'] = feature_df.Volume.shift(periods=3) * feature_df.Close.shift(periods=3)\n",
        "    feature_df['fvol_shift4'] = feature_df.Volume.shift(periods=4) * feature_df.Close.shift(periods=4)\n",
        "    feature_df['fvol_shift5'] = feature_df.Volume.shift(periods=5) * feature_df.Close.shift(periods=5)\n",
        "    feature_df['fvol_shift6'] = feature_df.Volume.shift(periods=6) * feature_df.Close.shift(periods=6)\n",
        "    feature_df['fvol_shift7'] = feature_df.Volume.shift(periods=7) * feature_df.Close.shift(periods=7)\n",
        "    feature_df['fvol_shift8'] = feature_df.Volume.shift(periods=8) * feature_df.Close.shift(periods=8)\n",
        "    feature_df['fvol_shift9'] = feature_df.Volume.shift(periods=9) * feature_df.Close.shift(periods=9)\n",
        "    feature_df['fvol_shift10'] = feature_df.Volume.shift(periods=10) * feature_df.Close.shift(periods=10)\n",
        "\n",
        "    feature_df['fvol_RSI'] = RSI(feature_df['fvol'], 14)\n",
        "\n",
        "    # MACD\n",
        "    fvol_macd, fvol_macd_signal = MACD(feature_df['fvol'], 10, 24, 8)\n",
        "    feature_df['fvol_MACD'] = fvol_macd\n",
        "    feature_df['fvol_MACD_signal'] = fvol_macd_signal\n",
        "    fvol_macd7, fvol_macd_signal7 = MACD(feature_df['fvol'], 60, 128, 48)\n",
        "    feature_df['fvol_MACD7'] = fvol_macd7\n",
        "    feature_df['fvol_MACD_signal7'] = fvol_macd_signal7\n",
        "\n",
        "    # Price exponentional power\n",
        "    feature_df['fvol_POW2'] = POW(feature_df['fvol'], 2)\n",
        "    feature_df['fvol_POW3'] = POW(feature_df['fvol'], 3)\n",
        "    feature_df['fvol_POW3_shift1'] = POW(feature_df['fvol'], 3).shift(periods=1)\n",
        "    #feature_df['fvol_POW4'] = POW(feature_df['fvol'], 4)\n",
        "\n",
        "    # EMA\n",
        "    # EMA\n",
        "    feature_df['lretEMA3'] = EMA1(feature_df['Close'], 3)\n",
        "    feature_df['lretEMA3_shift1'] = feature_df['lretEMA3'].shift(periods=1)\n",
        "    feature_df['lretEMA4'] = EMA1(feature_df['Close'], 4)\n",
        "    feature_df['lretEMA5'] = EMA1(feature_df['Close'], 5)\n",
        "    feature_df['lretEMA6'] = EMA1(feature_df['Close'], 6)\n",
        "    feature_df['lretEMA7'] = EMA1(feature_df['Close'], 7)\n",
        "    feature_df['lretEMA8'] = EMA1(feature_df['Close'], 8)\n",
        "    feature_df['lretEMA9'] = EMA1(feature_df['Close'], 9)\n",
        "    feature_df['lretEMA10'] = EMA1(feature_df['Close'], 10)\n",
        "    feature_df['lretEMA11'] = EMA1(feature_df['Close'], 11)\n",
        "    feature_df['lretEMA12'] = EMA1(feature_df['Close'], 12)\n",
        "    feature_df['lretEMA13'] = EMA1(feature_df['Close'], 13)\n",
        "    feature_df['lretEMA14'] = EMA1(feature_df['Close'], 14)\n",
        "    feature_df['lretEMA15'] = EMA1(feature_df['Close'], 15)\n",
        "    feature_df['lretEMA16'] = EMA1(feature_df['Close'], 16)\n",
        "    feature_df['lretEMA20'] = EMA1(feature_df['Close'], 20)\n",
        "    feature_df['lretEMA40'] = EMA1(feature_df['Close'], 40)\n",
        "    feature_df['lretEMA60'] = EMA1(feature_df['Close'], 60)\n",
        "    feature_df['lretEMA80'] = EMA1(feature_df['Close'], 80)\n",
        "\n",
        "    feature_df['fvol_EMA20'] = EMA1(feature_df['fvol'], 20)\n",
        "    feature_df['fvol_EMA40'] = EMA1(feature_df['fvol'], 40)\n",
        "    feature_df['fvol_EMA60'] = EMA1(feature_df['fvol'], 60)\n",
        "    feature_df['fvol_EMA80'] = EMA1(feature_df['fvol'], 80)\n",
        "\n",
        "    # Standard Deviation\n",
        "    feature_df['fvol_STD'] = STD(feature_df['fvol'])\n",
        "    feature_df['fvol_STD_shift21'] = feature_df['fvol_STD'].shift(periods=21)\n",
        "\n",
        "    # Skew\n",
        "    feature_df['fvol_SKEW'] = SKEW(feature_df['fvol'])\n",
        "    feature_df['fvol_SKEW_shift1'] = feature_df['fvol_SKEW'].shift(periods=1)\n",
        "\n",
        "    feature_df['LRETLRET'] = log_return(feature_df[\"LRET\"])\n",
        "    feature_df['HL_sprd'] = np.log((feature_df[\"high\"] - feature_df[\"low\"]) / feature_df[\"close\"])\n",
        "    feature_df['CO_sprd'] = (feature_df[\"close\"] - feature_df[\"open\"]) / feature_df[\"open\"]\n",
        "    feature_df['logVolume'] = np.log(feature_df[\"volume\"])\n",
        "    mean_price = feature_df[['open', 'high', 'low', 'close']].mean(axis=1)\n",
        "    feature_df['mean_price'] =  mean_price\n",
        "    median_price = feature_df[['open', 'high', 'low', 'close']].median(axis=1)\n",
        "    feature_df['median_price'] = median_price\n",
        "    feature_df['barMean'] = feature_df[['open', 'high', 'low', 'close']].mean(axis=1) #little bit of importance\n",
        "    feature_df[\"barMedian\"] = feature_df[[\"open\", \"high\", \"low\", \"close\"]].median(axis=1) #little bit of importance\n",
        "    feature_df['high2mean'] = feature_df['high'] / mean_price\n",
        "    feature_df['low2mean'] = feature_df['low'] / mean_price\n",
        "    feature_df['high2median'] = feature_df['high'] / median_price\n",
        "    feature_df['low2median'] = feature_df['low'] / median_price\n",
        "    feature_df['volume2vol+1'] = feature_df['volume'] / (feature_df['volume'] + 1)\n",
        "    feature_df[\"opensubclose\"] = feature_df[\"open\"] - feature_df[\"close\"]\n",
        "    feature_df['Upper_Shadow'] = feature_df['high'] - np.maximum(feature_df['close'], feature_df['open'])\n",
        "    feature_df['Lower_Shadow'] = np.minimum(feature_df['close'], feature_df['Open']) - feature_df['low']\n",
        "    feature_df['hlco_ratio'] = (feature_df['High'] - feature_df['low'])/(feature_df['close']-feature_df['open'])\n",
        "    feature_df[\"Close/Open\"] = feature_df[\"close\"] / feature_df[\"open\"]\n",
        "    feature_df[\"Close-Open\"] = feature_df[\"close\"] - feature_df[\"open\"]\n",
        "    feature_df[\"Close-Open_shift1\"] = feature_df[\"close\"].shift(periods=1) - feature_df[\"open\"]\n",
        "    feature_df[\"High-Low\"] = feature_df[\"high\"] - feature_df[\"low\"]\n",
        "    feature_df['High/Mean'] = feature_df['high'] / feature_df['mean_price']\n",
        "    feature_df['Low/Mean'] = feature_df['low'] / feature_df['mean_price']\n",
        "    feature_df['Volume/Close'] = feature_df['volume'] / (feature_df['close'])\n",
        "\n",
        "    #LRET\n",
        "    feature_df['Close'] = feature_df[\"LRET\"]\n",
        "    feature_df['Open'] = feature_df[\"LRET_OPEN\"]\n",
        "    feature_df['High'] = feature_df[\"LRET_HIGH\"]\n",
        "    feature_df['Low'] = feature_df[\"LRET_LOW\"]\n",
        "    feature_df['Volume'] = feature_df[\"VOL_LRET\"]\n",
        "\n",
        "    feature_df[\"LRET_shift1\"] = feature_df['LRET'].shift(periods=1)\n",
        "    feature_df[\"LRET_shift2\"] = feature_df['LRET'].shift(periods=2)\n",
        "    feature_df[\"LRET_shift3\"] = feature_df['LRET'].shift(periods=3)\n",
        "    feature_df[\"LRET_shift4\"] = feature_df['LRET'].shift(periods=4)\n",
        "    feature_df[\"LRET_shift5\"] = feature_df['LRET'].shift(periods=6)\n",
        "    feature_df[\"LRET_shift6\"] = feature_df['LRET'].shift(periods=6)\n",
        "    feature_df[\"LRET_shift7\"] = feature_df['LRET'].shift(periods=7)\n",
        "    feature_df[\"LRET_shift8\"] = feature_df['LRET'].shift(periods=8)\n",
        "    feature_df[\"LRET_shift9\"] = feature_df['LRET'].shift(periods=9)\n",
        "    feature_df[\"LRET_shift10\"] = feature_df['LRET'].shift(periods=10)\n",
        "    feature_df[\"LRET_shift11\"] = feature_df['LRET'].shift(periods=11)\n",
        "    feature_df[\"LRET_shift12\"] = feature_df['LRET'].shift(periods=12)\n",
        "    feature_df[\"LRET_shift13\"] = feature_df['LRET'].shift(periods=13)\n",
        "    feature_df[\"LRET_shift14\"] = feature_df['LRET'].shift(periods=14)\n",
        "    feature_df[\"LRET_shift15\"] = feature_df['LRET'].shift(periods=15)\n",
        "    feature_df[\"LRET_shift16\"] = feature_df['LRET'].shift(periods=16)\n",
        "    feature_df[\"LRET_shift17\"] = feature_df['LRET'].shift(periods=17)\n",
        "    feature_df[\"LRET_shift18\"] = feature_df['LRET'].shift(periods=18)\n",
        "    feature_df[\"LRET_shift19\"] = feature_df['LRET'].shift(periods=19)\n",
        "    feature_df[\"LRET_shift20\"] = feature_df['LRET'].shift(periods=20)\n",
        "    feature_df[\"LRET_shift21\"] = feature_df['LRET'].shift(periods=21)\n",
        "\n",
        "    # RSI on lret\n",
        "    feature_df['RSI3lret'] = RSI(feature_df['Close'], 3)\n",
        "    feature_df['RSI3lret_shift1'] = RSI(feature_df['Close'], 3).shift(periods=1)\n",
        "    feature_df['RSI4lret'] = RSI(feature_df['Close'], 4)\n",
        "    feature_df['RSI5lret'] = RSI(feature_df['Close'], 5)\n",
        "    feature_df['RSI6lret'] = RSI(feature_df['Close'], 6)\n",
        "    feature_df['RSI7lret'] = RSI(feature_df['Close'], 7)\n",
        "    feature_df['RSI8lret'] = RSI(feature_df['Close'], 8)\n",
        "    feature_df['RSI9lret'] = RSI(feature_df['Close'], 9)\n",
        "    feature_df['RSI10lret'] = RSI(feature_df['Close'], 10)\n",
        "    feature_df['RSI11lret'] = RSI(feature_df['Close'], 11)\n",
        "    feature_df['RSI12lret'] = RSI(feature_df['Close'], 12)\n",
        "    feature_df['RSI13lret'] = RSI(feature_df['Close'], 13)\n",
        "    feature_df['RSIlret'] = RSI(feature_df['Close'], 14)\n",
        "    feature_df['RSI20lret'] = RSI(feature_df['Close'], 20)\n",
        "    feature_df['RSI40lret'] = RSI(feature_df['Close'], 40)\n",
        "    feature_df['RSI60lret'] = RSI(feature_df['Close'], 60)\n",
        "    feature_df['RSI80lret'] = RSI(feature_df['Close'], 80)\n",
        "    feature_df['RSI100lret'] = RSI(feature_df['Close'], 100)\n",
        "    feature_df['RSI120lret'] = RSI(feature_df['Close'], 120)\n",
        "    # MACD\n",
        "    macd, macd_signal = MACD(feature_df['Close'], 20, 48, 16)\n",
        "    feature_df['MACDlret'] = macd\n",
        "    feature_df['MACD_signallret'] = macd_signal\n",
        "    macdlret2, macd_signallret2 = MACD(feature_df['Close'], 10, 24, 8)\n",
        "    feature_df['MACD2lret'] = macdlret2\n",
        "    feature_df['MACD_signal2lret'] = macd_signallret2\n",
        "    macd3, macd_signal3 = MACD(feature_df['Close'], 5, 12, 4)\n",
        "    feature_df['MACD3lret'] = macd3\n",
        "    feature_df['MACD_signal3lret'] = macd_signal3\n",
        "\n",
        "    # set of lret formulas\n",
        "    feature_df['HL_sprdlret'] = np.log((feature_df[\"High\"] - feature_df[\"Low\"]) / feature_df[\"Close\"])\n",
        "    feature_df['CO_sprdlret'] = (feature_df[\"Close\"] - feature_df[\"Open\"]) / feature_df[\"Open\"]\n",
        "    feature_df['logVolumelret'] = np.log(feature_df[\"Volume\"])\n",
        "    mean_price = feature_df[['Open', 'High', 'Low', 'Close']].mean(axis=1)\n",
        "    feature_df['mean_pricelret'] =  mean_price\n",
        "    median_price = feature_df[['Open', 'High', 'Low', 'Close']].median(axis=1)\n",
        "    feature_df['median_pricelret'] = median_price\n",
        "    feature_df['barMeanlret'] = feature_df[['Open', 'High', 'Low', 'Close']].mean(axis=1) #little bit of importance\n",
        "    feature_df[\"barMedianlret\"] = feature_df[[\"Open\", \"High\", \"Low\", \"Close\"]].median(axis=1) #little bit of importance\n",
        "    feature_df['upper_shadow1lret'] = feature_df['High'] / feature_df[['Close', 'Open']].max(axis=1)\n",
        "    feature_df['lower_shadow1lret'] = feature_df[['Close', 'Open']].min(axis=1) / feature_df['Low']\n",
        "    feature_df['open2closelret'] = feature_df['Close'] / feature_df['Open']\n",
        "    feature_df['high2lowlret'] = feature_df['High'] / feature_df['Low']\n",
        "    feature_df['Meanlret'] = feature_df['mean_pricelret']\n",
        "\n",
        "    # Price and volume\n",
        "    feature_df['LRET'] = log_return(feature_df[\"Close\"])\n",
        "\n",
        "    # Volume RSI\n",
        "    feature_df['VRSI3'] = RSI(feature_df['Volume'], 3)\n",
        "    feature_df['VRSI3_shift1'] = feature_df['VRSI3'].shift(periods=1)\n",
        "    feature_df['VRSI4'] = RSI(feature_df['Volume'], 4)\n",
        "    feature_df['VRSI5'] = RSI(feature_df['Volume'], 5)\n",
        "    feature_df['VRSI6'] = RSI(feature_df['Volume'], 6)\n",
        "    feature_df['VRSI7'] = RSI(feature_df['Volume'], 7)\n",
        "    feature_df['VRSI8'] = RSI(feature_df['Volume'], 8)\n",
        "    feature_df['VRSI9'] = RSI(feature_df['Volume'], 9)\n",
        "    feature_df['VRSI10'] = RSI(feature_df['Volume'], 10)\n",
        "    feature_df['VRSI11'] = RSI(feature_df['Volume'], 11)\n",
        "    feature_df['VRSI12'] = RSI(feature_df['Volume'], 12)\n",
        "    feature_df['VRSI13'] = RSI(feature_df['Volume'], 13)\n",
        "    feature_df['VRSI'] = RSI(feature_df['Volume'], 14)\n",
        "\n",
        "    # MACD\n",
        "    macd, macd_signal = MACD(feature_df['Close'], 10, 24, 8)\n",
        "    feature_df['MACD_'] = macd\n",
        "    feature_df['MACD_signal_'] = macd_signal\n",
        "    macd, macd_signal = MACD(feature_df['Close'], 20, 48, 16)\n",
        "    feature_df['MACD_2'] = macd\n",
        "    feature_df['MACD_signal_2'] = macd_signal\n",
        "    macd, macd_signal = MACD(feature_df['Close'], 30, 72, 24)\n",
        "    feature_df['MACD_3'] = macd\n",
        "    feature_df['MACD_signal_3'] = macd_signal\n",
        "    macd, macd_signal = MACD(feature_df['Close'], 40, 96, 32)\n",
        "    feature_df['MACD_4'] = macd\n",
        "    feature_df['MACD_signal_4'] = macd_signal\n",
        "\n",
        "\n",
        "    # Price exponentional power\n",
        "    feature_df['lretPOW05'] = POW(feature_df['Close'], 0.5)\n",
        "    feature_df['lretPOW2'] = POW(feature_df['Close'], 2)\n",
        "    feature_df['lretPOW3'] = POW(feature_df['Close'], 3)\n",
        "    feature_df['lretPOW3_shift1'] = POW(feature_df['Close'], 3).shift(periods=1)\n",
        "\n",
        "    # Volume exponentional power\n",
        "    feature_df['lretVPOW05'] = POW(feature_df['Volume'], 0.5)\n",
        "    feature_df['lretVPOW2'] = POW(feature_df['Volume'], 2)\n",
        "\n",
        "    # EMA\n",
        "    feature_df['EMA3'] = EMA1(feature_df['Close'], 3)\n",
        "    feature_df['EMA3_shift1'] = feature_df['EMA3'].shift(periods=1)\n",
        "    feature_df['EMA4'] = EMA1(feature_df['Close'], 4)\n",
        "    feature_df['EMA5'] = EMA1(feature_df['Close'], 5)\n",
        "    feature_df['EMA6'] = EMA1(feature_df['Close'], 6)\n",
        "    feature_df['EMA7'] = EMA1(feature_df['Close'], 7)\n",
        "    feature_df['EMA8'] = EMA1(feature_df['Close'], 8)\n",
        "    feature_df['EMA9'] = EMA1(feature_df['Close'], 9)\n",
        "    feature_df['EMA10'] = EMA1(feature_df['Close'], 10)\n",
        "    feature_df['EMA11'] = EMA1(feature_df['Close'], 11)\n",
        "    feature_df['EMA12'] = EMA1(feature_df['Close'], 12)\n",
        "    feature_df['EMA13'] = EMA1(feature_df['Close'], 13)\n",
        "    feature_df['EMA14'] = EMA1(feature_df['Close'], 14)\n",
        "    feature_df['EMA15'] = EMA1(feature_df['Close'], 15)\n",
        "    feature_df['EMA16'] = EMA1(feature_df['Close'], 16)\n",
        "    feature_df['EMA20'] = EMA1(feature_df['Close'], 20)\n",
        "    feature_df['EMA40'] = EMA1(feature_df['Close'], 40)\n",
        "    feature_df['EMA60'] = EMA1(feature_df['Close'], 60)\n",
        "    feature_df['EMA80'] = EMA1(feature_df['Close'], 80)\n",
        "\n",
        "    # Volume EMA\n",
        "    feature_df['lretVEMA3'] = EMA1(feature_df['Volume'], 3)\n",
        "    feature_df['lretVEMA3_shift1'] = feature_df['lretVEMA3'].shift(periods=1)\n",
        "    feature_df['lretVEMA4'] = EMA1(feature_df['Volume'], 4)\n",
        "    feature_df['lretVEMA5'] = EMA1(feature_df['Volume'], 5)\n",
        "    feature_df['lretVEMA6'] = EMA1(feature_df['Volume'], 6)\n",
        "    feature_df['lretVEMA7'] = EMA1(feature_df['Volume'], 7)\n",
        "    feature_df['lretVEMA8'] = EMA1(feature_df['Volume'], 8)\n",
        "    feature_df['lretVEMA9'] = EMA1(feature_df['Volume'], 9)\n",
        "    feature_df['lretVEMA10'] = EMA1(feature_df['Volume'], 10)\n",
        "\n",
        "    # Standard Deviation\n",
        "    feature_df['lretSTD'] = STD(feature_df['Close'])\n",
        "    feature_df['lretSTD_shift1'] = feature_df['lretSTD'].shift(periods=1)\n",
        "    feature_df['lretSTD_shift2'] = feature_df['lretSTD'].shift(periods=2)\n",
        "    feature_df['lretSTD_shift3'] = feature_df['lretSTD'].shift(periods=3)\n",
        "    feature_df['lretSTD_shift4'] = feature_df['lretSTD'].shift(periods=4)\n",
        "    feature_df['lretSTD_shift5'] = feature_df['lretSTD'].shift(periods=5)\n",
        "    feature_df['lretSTD_shift6'] = feature_df['lretSTD'].shift(periods=6)\n",
        "    feature_df['lretSTD_shift10'] = feature_df['lretSTD'].shift(periods=10)\n",
        "    feature_df['lretSTD_shift19'] = feature_df['lretSTD'].shift(periods=19)\n",
        "    feature_df['lretSTD_shift20'] = feature_df['lretSTD'].shift(periods=20)\n",
        "    feature_df['lretSTD_shift21'] = feature_df['lretSTD'].shift(periods=21)\n",
        "\n",
        "    # Volume standard deviation\n",
        "    feature_df['VSTD'] = STD(feature_df['Volume'])\n",
        "    feature_df['VSTD_shift1'] = feature_df['VSTD'].shift(periods=1)\n",
        "\n",
        "    # Skew\n",
        "    feature_df['SKEW'] = SKEW(feature_df['Close'])\n",
        "    feature_df['SKEW_shift1'] = feature_df['SKEW'].shift(periods=1)\n",
        "    feature_df['SKEW_shift2'] = feature_df['SKEW'].shift(periods=2)\n",
        "\n",
        "    # Volume Skew\n",
        "    feature_df['VSKEW'] = SKEW(feature_df['Volume'])\n",
        "    feature_df['VSKEW_shift1'] = feature_df['VSKEW'].shift(periods=1)\n",
        "\n",
        "    # Price\n",
        "    feature_df['Close']\n",
        "\n",
        "    # Volume\n",
        "    feature_df['Volume']\n",
        "\n",
        "    mean_price = feature_df[['Open', 'High', 'Low', 'Close']].mean(axis=1)\n",
        "    feature_df['lretmean_price'] =  mean_price\n",
        "    feature_df['HL_sprd'] = np.log((feature_df[\"High\"] - feature_df[\"Low\"]) / feature_df[\"Close\"])\n",
        "    feature_df['CO_sprd'] = (feature_df[\"Close\"] - feature_df[\"Open\"]) / feature_df[\"Open\"]\n",
        "    mean_price = feature_df[['Open', 'High', 'Low', 'Close']].mean(axis=1)\n",
        "    feature_df['mean_price'] =  mean_price\n",
        "    median_price = feature_df[['Open', 'High', 'Low', 'Close']].median(axis=1)\n",
        "    feature_df['median_price'] = median_price\n",
        "    feature_df['barMean'] = feature_df[['Open', 'High', 'Low', 'Close']].mean(axis=1) #little bit of importance\n",
        "    feature_df[\"barMedian\"] = feature_df[[\"Open\", \"High\", \"Low\", \"Close\"]].median(axis=1) #little bit of importance\n",
        "    feature_df['lretMean'] = feature_df['mean_price']\n",
        "    feature_df['lrethigh2mean'] = feature_df['High'] / mean_price\n",
        "    feature_df['lretlow2mean'] = feature_df['Low'] / mean_price\n",
        "    feature_df['lrethigh2median'] = feature_df['High'] / median_price\n",
        "    feature_df['lretlow2median'] = feature_df['Low'] / median_price\n",
        "    feature_df['lretvolume2vol+1'] = feature_df['Volume'] / (feature_df['Volume'] + 1)\n",
        "    feature_df[\"lretopensubclose\"] = feature_df[\"Open\"] - feature_df[\"Close\"]\n",
        "    feature_df['lretUpper_Shadow'] = feature_df['High'] - np.maximum(feature_df['Close'], feature_df['Open'])\n",
        "    feature_df['lretLower_Shadow'] = np.minimum(feature_df['Close'], feature_df['Open']) - feature_df['Low']\n",
        "    feature_df['lrethlco_ratio'] = (feature_df['High'] - feature_df['Low'])/(feature_df['Close']-feature_df['Open'])\n",
        "    feature_df[\"lretClose/Open\"] = feature_df[\"Close\"] / feature_df[\"Open\"]\n",
        "    feature_df[\"lretClose-Open\"] = feature_df[\"Close\"] - feature_df[\"Open\"]\n",
        "    feature_df[\"lretClose-Open_shift1\"] = feature_df[\"Close\"].shift(periods=1) - feature_df[\"Open\"]\n",
        "    feature_df[\"lretHigh-Low\"] = feature_df[\"High\"] - feature_df[\"Low\"]\n",
        "    feature_df['lretHigh/Mean'] = feature_df['High'] / feature_df['lretMean']\n",
        "    feature_df['lretLow/Mean'] = feature_df['Low'] / feature_df['lretMean']\n",
        "    feature_df['lretVolume/Close'] = feature_df['Volume'] / (feature_df['Close'] + 0.01)\n",
        "    #date functions\n",
        "    feature_df['date2'] = pd.to_datetime(feature_df['date'], format=\"%Y%m%d\")\n",
        "    times = pd.to_datetime(feature_df[\"date2\"],unit=\"D\",infer_datetime_format=True)\n",
        "    feature_df[\"dayofweek\"] = times.dt.dayofweek\n",
        "    feature_df[\"day\"] = times.dt.day\n",
        "    feature_df[\"monthstart\"] = times.dt.is_month_start\n",
        "    feature_df[\"monthend\"] = times.dt.is_month_end\n",
        "    feature_df[\"quarterstart\"] = times.dt.is_quarter_start\n",
        "    feature_df[\"quarterend\"] = times.dt.is_quarter_end\n",
        "    feature_df[\"yearstart\"] = times.dt.is_year_start# .dt\n",
        "    feature_df[\"yearend\"] = times.dt.is_year_end# .dt\n",
        "    feature_df[\"dayofweekday\"] = feature_df[\"dayofweek\"]+ feature_df[\"day\"]\n",
        "    feature_df[\"monthstartday\"] = feature_df[\"monthstart\"]+ feature_df[\"day\"]\n",
        "    feature_df[\"monthendday\"] = feature_df[\"monthend\"]+ feature_df[\"day\"]\n",
        "    feature_df[\"quarterstartday\"] = feature_df[\"quarterstart\"]+ feature_df[\"day\"]\n",
        "    feature_df[\"quarterendday\"] = feature_df[\"quarterend\"]+ feature_df[\"day\"]\n",
        "    feature_df[\"yearstartday\"] = feature_df[\"yearstart\"]+ feature_df[\"day\"]\n",
        "    feature_df[\"yearendday\"] = feature_df[\"yearend\"]+ feature_df[\"day\"]\n",
        "    feature_df[\"yearendmonthstart\"] = feature_df[\"yearend\"]+ feature_df[\"monthstart\"]\n",
        "    feature_df[\"yearendmonthend\"] = feature_df[\"yearend\"]+ feature_df[\"monthend\"]\n",
        "    feature_df[\"monthstartdayofweek\"] = feature_df[\"monthstart\"]+ feature_df[\"dayofweek\"]\n",
        "    feature_df[\"monthenddayofweek\"] = feature_df[\"monthend\"]+ feature_df[\"dayofweek\"]\n",
        "    feature_df[\"quarterstartdayofweek\"] = feature_df[\"quarterstart\"]+ feature_df[\"dayofweek\"]\n",
        "    feature_df[\"quarterenddayofweek\"] = feature_df[\"quarterend\"]+ feature_df[\"dayofweek\"]\n",
        "    feature_df[\"yearstartdayofweek\"] = feature_df[\"yearstart\"]+ feature_df[\"dayofweek\"]\n",
        "    feature_df[\"yearenddayofweek\"] = feature_df[\"yearend\"]+ feature_df[\"dayofweek\"]\n",
        "    feature_df[\"yearendquarterstart\"] = feature_df[\"yearend\"]+ feature_df[\"quarterstart\"]\n",
        "    feature_df[\"yearendquarterend\"] = feature_df[\"yearend\"]+ feature_df[\"quarterend\"]\n",
        "\n",
        "    feature_df['100returns'] = 100 * feature_df[\"Close\"].pct_change()\n",
        "    # CALCULATE LOG RETURNS BASED ON ABOVE FORMULA\n",
        "    feature_df['log_returns'] = np.log(feature_df[\"Close\"]/feature_df[\"Close\"].shift(1))\n",
        "    feature_df['Vlog_returns'] = np.log(feature_df[\"Volume\"]/feature_df[\"Volume\"].shift(1))\n",
        "    feature_df['log_returns2'] = np.log(feature_df[\"Close\"]/feature_df[\"Close\"].shift(2))\n",
        "    feature_df['Vlog_returns2'] = np.log(feature_df[\"Volume\"]/feature_df[\"Volume\"].shift(2))\n",
        "    feature_df['log_returns3'] = np.log(feature_df[\"Close\"]/feature_df[\"Close\"].shift(3))\n",
        "    feature_df['LRET'] = log_return(feature_df['Close'])\n",
        "    feature_df['VLRET'] = log_return(feature_df['Volume'])\n",
        "    feature_df['RVOL'] = realized_volatility(feature_df['LRET'])\n",
        "    feature_df['RVOL2'] = realized_volatility(feature_df['RVOL'])\n",
        "    feature_df[\"barSkew\"] = feature_df[[\"Open\", \"High\", \"Low\", \"Close\"]].skew(axis=1)\n",
        "    feature_df[\"barKurt\"] = feature_df[[\"Open\", \"High\", \"Low\", \"Close\"]].kurt(axis=1)\n",
        "    feature_df['Mean2'] = feature_df[['Open', 'Close']].mean(axis=1)\n",
        "    feature_df['EMA3_shift1_2'] = feature_df['EMA3_shift1']**2\n",
        "    feature_df['EMA13_2'] = feature_df['EMA13']**2\n",
        "    #quantile this in quintiles\n",
        "    #feature_df['HL_sprd_q'] = pd.qcut(feature_df['HL_sprd'], q=[0, 0.2, 0.4, 0.6, 0.8, 1], labels=False,duplicates='raise')\n",
        "    #feature_df['CO_sprd_q'] = pd.qcut(feature_df['CO_sprd'], q=[0, 0.2, 0.4, 0.6, 0.8, 1], labels=False,duplicates='drop')\n",
        "    #feature_df['_q'] = pd.qcut(feature_df['mean30drange'], q=[0, 0.2, 0.4, 0.6, 0.8, 1], labels=False,duplicates='drop')\n",
        "\n",
        "    new_feats += [\"random_uniform\", \"random_normal\", \"random_category\",\n",
        "                  'Daily_Range','Volumez',#'Openz','Highz','Lowz','Closez','Daily_Rangez','Mcapz','Fvolz',\n",
        "                  'Daily_Range_shift1z','Daily_Range_shift1','Daily_Range_shift2z','Daily_Range_shift2','Daily_Range_shift3z','Daily_Range_shift3',\n",
        "                  'Daily_Rangez_shift1','Daily_Rangez_shift2','Daily_Rangez_shift3',\n",
        "                  'HL_sprd','CO_sprd','logVolume','mean_price','median_price','barMean',\"barMedian\",'high2mean','low2mean','high2median','low2median','volume2vol+1',\"opensubclose\",'Upper_Shadow','Lower_Shadow','hlco_ratio',\"Close/Open\",\"Close-Open\",\"Close-Open_shift1\",\"High-Low\",'High/Mean','Low/Mean','Volume/Close',\n",
        "                  'MACD0','MACD_signal0','MACD2','MACD_signal2','MACD3','MACD_signal3','MACD4','MACD_signal4', 'MACD', 'MACD_signal','MACD5','MACD_signal5',\n",
        "                  'MACD6','MACD_signal6','MACD7','MACD_signal7',\n",
        "                  #'poly_7', 'poly_8',\n",
        "                  'RSI2','RSI3','RSI4','RSI5','RSI6','RSI7','RSI8','RSI9','RSI10','RSI11','RSI12','RSI13','RSI',#'RSI3_shift1',\n",
        "                  'RSI20','RSI40','RSI60','RSI80','RSI100','RSI120','RSI140',\n",
        "                  \"return_1month\",\"return_2month\",\"return_3month\",\"volatility_1month\",\"volatility_2month\",\"volatility_3month\",\"MA_gap_1month\",\"MA_gap_2month\",\"MA_gap_3month\",\n",
        "                  'LRET_OPEN','LRET_LOW','LRETLRET','VOL_LRET','FVOL_LRET','LRET_HIGH',\n",
        "                  'RSIlret','RSI3lret','RSI4lret','RSI5lret','RSI6lret','RSI7lret',#'RSI8lret','RSI9lret','RSI10lret','RSI11lret','RSI12lret','RSI13lret','RSIlret','RSI20lret',#'RSI40lret','RSI60lret','RSI80lret','RSI100lret','RSI120lret',#'RSI3lret_shift1',\n",
        "                  'LRET_shift1','LRET_shift2','LRET_shift3','LRET_shift4','LRET_shift5','LRET_shift6','LRET_shift7','LRET_shift8','LRET_shift9','LRET_shift10','LRET_shift11','LRET_shift12','LRET_shift13','LRET_shift14','LRET_shift15','LRET_shift16','LRET_shift17','LRET_shift18','LRET_shift19','LRET_shift20','LRET_shift21',\n",
        "                  'EMA3','EMA3_shift1','EMA13','EMA4','EMA5','EMA6','EMA7','EMA8','EMA9','EMA10','EMA11','EMA12','EMA14','EMA15','EMA16','EMA20','EMA40','EMA60','EMA80',\n",
        "                  'VRSI3','VRSI3_shift1','VRSI4','VRSI5','VRSI6','VRSI7','VRSI8','VRSI9','VRSI10','VRSI11','VRSI12','VRSI13','VRSI',\n",
        "                  #'VEMA3','VEMA3_shift1','VEMA4','VEMA5','VEMA6','VEMA7','VEMA8','VEMA9','VEMA10',\n",
        "                  #'POW05','POW2','VPOW2','VPOW05','POW3_shift1','POW3',#,'VPOW3''VPOW5','VPOW4','POW4','POW5',lretHL_sprd', 'lretCO_sprd', 'lretmedian_price', 'lretbarMean', 'lretbarMedian'\n",
        "                  #'VSKEW','VSKEW_shift1','SKEW_shift2',#active 20220606 (truth ambig.. eroor) 'VSKEW_shift1',,'SKEW_shift2'\n",
        "                  #To add 100 more poly's and control the number:\n",
        "                  #'poly_01', 'poly_02', 'poly_03', 'poly_04', 'poly_05', 'poly_06', 'poly_07', 'poly_08', 'poly_09', 'poly_10', 'poly_11', 'poly_12', 'poly_13', 'poly_14', 'poly_15',\n",
        "                  #'poly_16', 'poly_17', 'poly_18', 'poly_19', 'poly_20', 'poly_21', 'poly_22', 'poly_23', 'poly_24', 'poly_25', 'poly_26', 'poly_27', 'poly_28',\n",
        "                  #'poly_29', 'poly_30', 'poly_31', 'poly_33', 'poly_34', 'poly_35', 'poly_36', 'poly_37', 'poly_38', 'poly_39', 'poly_40', 'poly_41', 'poly_42',\n",
        "                  #'poly_43', 'poly_44', 'poly_45', 'poly_46', 'poly_47', 'poly_48', 'poly_49', 'poly_50', 'poly_51', 'poly_52', 'poly_53', 'poly_54', 'poly_55',\n",
        "                  #'poly_56', 'poly_57', 'poly_58', 'poly_59', 'poly_60', 'poly_61', 'poly_62', 'poly_63', 'poly_64', 'poly_65', 'poly_66', 'poly_67', 'poly_68',\n",
        "                  #'poly_69', 'poly_70', 'poly_71', 'poly_73', 'poly_74', 'poly_75', 'poly_76', 'poly_77', 'poly_78', 'poly_79', 'poly_80', 'poly_81', 'poly_82',\n",
        "                  #'poly_83', 'poly_84', 'poly_85', 'poly_86', 'poly_87', 'poly_88', 'poly_89', 'poly_90', 'poly_91', 'poly_92', 'poly_93', 'poly_94', 'poly_95',\n",
        "                  #'poly_96', 'poly_97', 'poly_98', 'poly_99',\n",
        "                  'poly_13', 'poly_14', 'poly_15',#'poly_4', 'poly_5', 'poly_6','poly_7', 'poly_8', 'poly_9', 'poly_10', 'poly_11','poly_12',\n",
        "                  'poly_16', 'poly_17', 'poly_18', 'poly_19', 'poly_20', 'poly_21', 'poly_22', 'poly_23', 'poly_24', 'poly_25', 'poly_26', 'poly_27', 'poly_28',\n",
        "                  'poly_29', 'poly_30', 'poly_31', 'poly_33', 'poly_34', 'poly_35', 'poly_36', 'poly_37', 'poly_38', 'poly_39', 'poly_40', 'poly_41', 'poly_42',\n",
        "                  'poly_43', 'poly_44', 'poly_45', 'poly_46', 'poly_47', 'poly_48', 'poly_49', 'poly_50', 'poly_51', 'poly_52', 'poly_53', 'poly_54', 'poly_55',\n",
        "                  'poly_56', 'poly_57', 'poly_58', 'poly_59', 'poly_60', 'poly_61', 'poly_62', 'poly_63', 'poly_64', 'poly_65', 'poly_66', 'poly_67', 'poly_68',\n",
        "                  'poly_69', 'poly_70', 'poly_71', 'poly_73', 'poly_74', 'poly_75', 'poly_76', 'poly_77', 'poly_78', 'poly_79', 'poly_80', 'poly_81', 'poly_82',\n",
        "                  'poly_83', 'poly_84', 'poly_85', 'poly_86', 'poly_87', 'poly_88', 'poly_89', 'poly_90', 'poly_91', 'poly_92', 'poly_93', 'poly_94', 'poly_95',\n",
        "                  'poly_96', 'poly_97', 'poly_98', 'poly_99', 'poly_100', 'poly_101', 'poly_102', 'poly_103', 'poly_104', 'poly_105', 'poly_106', 'poly_107', 'poly_108',\n",
        "                  'poly_109', 'poly_110', 'poly_111', 'poly_113', 'poly_114', 'poly_115', 'poly_116', 'poly_117', 'poly_118', 'poly_119', 'poly_120', 'poly_121', 'poly_122',\n",
        "                  'poly_123', 'poly_124', 'poly_125', 'poly_126', 'poly_127', 'poly_128', 'poly_129', 'poly_130', 'poly_131', 'poly_132', 'poly_133', 'poly_134', 'poly_135',\n",
        "                  'poly_136', 'poly_137', 'poly_138', 'poly_139', 'poly_140', 'poly_141',\n",
        "                  'poly_142', 'poly_143', 'poly_144', 'poly_145', 'poly_146', 'poly_147', 'poly_148',\n",
        "                  'poly_149', 'poly_150', 'poly_151', 'poly_153', 'poly_154', 'poly_155', 'poly_156', 'poly_157', 'poly_158', 'poly_159', 'poly_160', 'poly_161', 'poly_162',\n",
        "                  'poly_163', 'poly_164', 'poly_165', 'poly_166', 'poly_167', 'poly_168', 'poly_169', 'poly_170', 'poly_171', 'poly_172', 'poly_173', 'poly_174', 'poly_175',\n",
        "                  'poly_176', 'poly_177', 'poly_178', 'poly_179', 'poly_180', 'poly_181', 'poly_182', 'poly_183', 'poly_184', 'poly_185', 'poly_186', 'poly_187', 'poly_188',\n",
        "                  'poly_189', 'poly_190', 'poly_191', 'poly_192', 'poly_193', 'poly_194', 'poly_195', 'poly_196', 'poly_197', 'poly_198', 'poly_199', 'poly_200',\n",
        "                  'poly_201', 'poly_202', 'poly_203', 'poly_204', 'poly_205', 'poly_206', 'poly_207', 'poly_208', 'poly_209', 'poly_210', 'poly_211', 'poly_212', 'poly_213', 'poly_214', 'poly_215',\n",
        "                  'poly_216', 'poly_217', 'poly_218', 'poly_219', 'poly_220', 'poly_221', 'poly_222', 'poly_223', 'poly_224', 'poly_225', 'poly_226', 'poly_227', 'poly_228',\n",
        "                  'poly_229', 'poly_230', 'poly_231', 'poly_233', 'poly_234', 'poly_235', 'poly_236', 'poly_237', 'poly_238', 'poly_239', 'poly_240', 'poly_241', 'poly_242',\n",
        "                  'poly_243', 'poly_244', 'poly_245', 'poly_246', 'poly_247', 'poly_248', 'poly_249', 'poly_250', 'poly_251', 'poly_252', 'poly_253', 'poly_254', 'poly_255',\n",
        "                  'poly_256', 'poly_257', 'poly_258', 'poly_259', 'poly_260', 'poly_261', 'poly_262', 'poly_263', 'poly_264', 'poly_265', 'poly_266', 'poly_267', 'poly_268',\n",
        "                  'poly_269', 'poly_270', 'poly_271', 'poly_273', 'poly_274', 'poly_275', 'poly_276', 'poly_277', 'poly_278', 'poly_279', 'poly_280', 'poly_281', 'poly_282',\n",
        "                  'poly_283', 'poly_284', 'poly_285', 'poly_286', 'poly_287', 'poly_288', 'poly_289', 'poly_290', 'poly_291', 'poly_292', 'poly_293', 'poly_294', 'poly_295',\n",
        "                  'poly_296', 'poly_297', 'poly_298', 'poly_299', 'poly_300',\n",
        "                  'poly_301', 'poly_302', 'poly_303', 'poly_304', 'poly_305', 'poly_306', 'poly_307', 'poly_308', 'poly_309', 'poly_310', 'poly_311', 'poly_312', 'poly_313', 'poly_314', 'poly_315',\n",
        "                  'poly_316', 'poly_317', 'poly_318', 'poly_319', 'poly_320', 'poly_321', 'poly_322', 'poly_323', 'poly_324', 'poly_325', 'poly_326', 'poly_327', 'poly_328',\n",
        "                  'poly_329', 'poly_330', 'poly_331', 'poly_333', 'poly_334', 'poly_335', 'poly_336', 'poly_337', 'poly_338', 'poly_339', 'poly_340', 'poly_341', 'poly_342',\n",
        "                  'poly_343', 'poly_344', 'poly_345', 'poly_346', 'poly_347', 'poly_348', 'poly_349', 'poly_350', 'poly_351', 'poly_352', 'poly_353', 'poly_354', 'poly_355',\n",
        "                  'poly_356', 'poly_357', 'poly_358', 'poly_359', 'poly_360', 'poly_361', 'poly_362', 'poly_363', 'poly_364', 'poly_365', 'poly_366', 'poly_367', 'poly_368',\n",
        "                  'poly_369', 'poly_370', 'poly_371', 'poly_373', 'poly_374', 'poly_375', 'poly_376', 'poly_377', 'poly_378', 'poly_379', 'poly_380', 'poly_381', 'poly_382',\n",
        "                  'poly_383', 'poly_384', 'poly_385', 'poly_386', 'poly_387', 'poly_388', 'poly_389', 'poly_390', 'poly_391', 'poly_392', 'poly_393', 'poly_394', 'poly_395',\n",
        "                  'poly_396', 'poly_397', 'poly_398', 'poly_399', 'poly_400',\n",
        "                  'poly_401', 'poly_402', 'poly_403', 'poly_404', 'poly_405', 'poly_406', 'poly_407', 'poly_408', 'poly_409', 'poly_410', 'poly_411', 'poly_412', 'poly_413', 'poly_414', 'poly_415',\n",
        "                  'poly_416', 'poly_417', 'poly_418', 'poly_419', 'poly_420', 'poly_421', 'poly_422', 'poly_423', 'poly_424', 'poly_425', 'poly_426', 'poly_427', 'poly_428',\n",
        "                  'poly_429', 'poly_430', 'poly_431', 'poly_433', 'poly_434', 'poly_435', 'poly_436', 'poly_437', 'poly_438', 'poly_439', 'poly_440', 'poly_441', 'poly_442',\n",
        "                  'poly_443', 'poly_444', 'poly_445', 'poly_446', 'poly_447', 'poly_448', 'poly_449', 'poly_450', 'poly_451', 'poly_452', 'poly_453', 'poly_454', 'poly_455',\n",
        "                  'poly_456', 'poly_457', 'poly_458', 'poly_459', 'poly_460', 'poly_461', 'poly_462', 'poly_463', 'poly_464', 'poly_465', 'poly_466', 'poly_467', 'poly_468',\n",
        "                  'poly_469', 'poly_470', 'poly_471', 'poly_473', 'poly_474', 'poly_475', 'poly_476', 'poly_477', 'poly_478', 'poly_479', 'poly_480', 'poly_481', 'poly_482',\n",
        "                  'poly_483', 'poly_484', 'poly_485', 'poly_486', 'poly_487', 'poly_488', 'poly_489', 'poly_490', 'poly_491', 'poly_492', 'poly_493', 'poly_494', 'poly_495',\n",
        "                  'poly_496', 'poly_497', 'poly_498', 'poly_499', 'poly_500',\n",
        "                  'poly_501', 'poly_502', 'poly_503', 'poly_504', 'poly_505', 'poly_506', 'poly_507', 'poly_508', 'poly_509', 'poly_510', 'poly_511', 'poly_512', 'poly_513', 'poly_514', 'poly_515',\n",
        "                  'poly_516', 'poly_517', 'poly_518', 'poly_519', 'poly_520', 'poly_521', 'poly_522', 'poly_523', 'poly_524', 'poly_525', 'poly_526', 'poly_527', 'poly_528',\n",
        "                  'poly_529', 'poly_530', 'poly_531', 'poly_533', 'poly_534', 'poly_535', 'poly_536', 'poly_537', 'poly_538', 'poly_539', 'poly_540', 'poly_541', 'poly_542',\n",
        "                  'poly_543', 'poly_544', 'poly_545', 'poly_546', 'poly_547', 'poly_548', 'poly_549', 'poly_550', 'poly_551', 'poly_552', 'poly_553', 'poly_554', 'poly_555',\n",
        "                  'poly_556', 'poly_557', 'poly_558', 'poly_559', 'poly_560', 'poly_561', 'poly_562', 'poly_563', 'poly_564', 'poly_565', 'poly_566', 'poly_567', 'poly_568',\n",
        "                  'poly_569', 'poly_570', 'poly_571', 'poly_573', 'poly_574', 'poly_575', 'poly_576', 'poly_577', 'poly_578', 'poly_579', 'poly_580', 'poly_581', 'poly_582',\n",
        "                  'poly_583', 'poly_584', 'poly_585', 'poly_586', 'poly_587', 'poly_588', 'poly_589', 'poly_590', 'poly_591', 'poly_592', 'poly_593', 'poly_594', 'poly_595',\n",
        "                  'poly_596', 'poly_597', 'poly_598', 'poly_599', 'poly_600',\n",
        "                  'poly_601', 'poly_602', 'poly_603', 'poly_604', 'poly_605', 'poly_606', 'poly_607', 'poly_608', 'poly_609', 'poly_610', 'poly_611', 'poly_612', 'poly_613', 'poly_614', 'poly_615',\n",
        "                  'poly_616', 'poly_617', 'poly_618', 'poly_619', 'poly_620', 'poly_621', 'poly_622', 'poly_623', 'poly_624', 'poly_625', 'poly_626', 'poly_627', 'poly_628',\n",
        "                  'poly_629', 'poly_630', 'poly_631', 'poly_633', 'poly_634', 'poly_635', 'poly_636', 'poly_637', 'poly_638', 'poly_639', 'poly_640', 'poly_641', 'poly_642',\n",
        "                  'poly_643', 'poly_644', 'poly_645', 'poly_646', 'poly_647', 'poly_648', 'poly_649', 'poly_650', 'poly_651', 'poly_652', 'poly_653', 'poly_654', 'poly_655',\n",
        "                  'poly_656', 'poly_657', 'poly_658', 'poly_659', 'poly_660', 'poly_661', 'poly_662', 'poly_663', 'poly_664', 'poly_665', 'poly_666', 'poly_667', 'poly_668',\n",
        "                  'poly_669', 'poly_670', 'poly_671', 'poly_673', 'poly_674', 'poly_675', 'poly_676', 'poly_677', 'poly_678', 'poly_679', 'poly_680', 'poly_681', 'poly_682',\n",
        "                  'poly_683', 'poly_684', 'poly_685', 'poly_686', 'poly_687', 'poly_688', 'poly_689', 'poly_690', 'poly_691', 'poly_692', 'poly_693', 'poly_694', 'poly_695',\n",
        "                  'poly_696', 'poly_697', 'poly_698', 'poly_699', 'poly_700',\n",
        "                  'poly_701', 'poly_702', 'poly_703', 'poly_704', 'poly_705', 'poly_706', 'poly_707', 'poly_708', 'poly_709', 'poly_710', 'poly_711', 'poly_712', 'poly_713', 'poly_714', 'poly_715',\n",
        "                  'poly_716', 'poly_717', 'poly_718', 'poly_719', 'poly_720', 'poly_721', 'poly_722', 'poly_723', 'poly_724', 'poly_725', 'poly_726', 'poly_727', 'poly_728',\n",
        "                  'poly_729', 'poly_730', 'poly_731', 'poly_733', 'poly_734', 'poly_735', 'poly_736', 'poly_737', 'poly_738', 'poly_739', 'poly_740', 'poly_741', 'poly_742',\n",
        "                  'poly_743', 'poly_744', 'poly_745', 'poly_746', 'poly_747', 'poly_748', 'poly_749', 'poly_750', 'poly_751', 'poly_752', 'poly_753', 'poly_754', 'poly_755',\n",
        "                  'poly_756', 'poly_757', 'poly_758', 'poly_759', 'poly_760', 'poly_761', 'poly_762', 'poly_763', 'poly_764', 'poly_765', 'poly_766', 'poly_767', 'poly_768',\n",
        "                  'poly_769', 'poly_770', 'poly_771', 'poly_773', 'poly_774', 'poly_775', 'poly_776', 'poly_777', 'poly_778', 'poly_779', 'poly_780', 'poly_781', 'poly_782',\n",
        "                  'poly_783', 'poly_784', 'poly_785', 'poly_786', 'poly_787', 'poly_788', 'poly_789', 'poly_790', 'poly_791', 'poly_792', 'poly_793', 'poly_794', 'poly_795',\n",
        "                  'poly_796', 'poly_797', 'poly_798', 'poly_799', 'poly_800',\n",
        "                  'poly_801', 'poly_802', 'poly_803', 'poly_804', 'poly_805', 'poly_806', 'poly_807', 'poly_808', 'poly_809', 'poly_810', 'poly_811', 'poly_812', 'poly_813', 'poly_814', 'poly_815',\n",
        "                  'poly_816', 'poly_817', 'poly_818', 'poly_819', 'poly_820', 'poly_821', 'poly_822', 'poly_823', 'poly_824', 'poly_825', 'poly_826', 'poly_827', 'poly_828',\n",
        "                  'poly_829', 'poly_830', 'poly_831', 'poly_833', 'poly_834', 'poly_835', 'poly_836', 'poly_837', 'poly_838', 'poly_839', 'poly_840', 'poly_841', 'poly_842',\n",
        "                  'poly_843', 'poly_844', 'poly_845', 'poly_846', 'poly_847', 'poly_848', 'poly_849', 'poly_850', 'poly_851', 'poly_852', 'poly_853', 'poly_854', 'poly_855',\n",
        "                  'poly_856', 'poly_857', 'poly_858', 'poly_859', 'poly_860', 'poly_861', 'poly_862', 'poly_863', 'poly_864', 'poly_865', 'poly_866', 'poly_867', 'poly_868',\n",
        "                  'poly_869', 'poly_870', 'poly_871', 'poly_873', 'poly_874', 'poly_875', 'poly_876', 'poly_877', 'poly_878', 'poly_879', 'poly_880', 'poly_881', 'poly_882',\n",
        "                  'poly_883', 'poly_884', 'poly_885', 'poly_886', 'poly_887', 'poly_888', 'poly_889', 'poly_890', 'poly_891', 'poly_892', 'poly_893', 'poly_894', 'poly_895',\n",
        "                  'poly_896', 'poly_897', 'poly_898', 'poly_899', 'poly_900',\n",
        "                  'poly_901', 'poly_902', 'poly_903', 'poly_904', 'poly_905', 'poly_906', 'poly_907', 'poly_908', 'poly_909', 'poly_910', 'poly_911', 'poly_912', 'poly_913', 'poly_914', 'poly_915',\n",
        "                  'poly_916', 'poly_917', 'poly_918', 'poly_919', 'poly_920', 'poly_921', 'poly_922', 'poly_923', 'poly_924', 'poly_925', 'poly_926', 'poly_927', 'poly_928',\n",
        "                  'poly_929', 'poly_930', 'poly_931', 'poly_933', 'poly_934', 'poly_935', 'poly_936', 'poly_937', 'poly_938', 'poly_939', 'poly_940', 'poly_941', 'poly_942',\n",
        "                  'poly_943', 'poly_944', 'poly_945', 'poly_946', 'poly_947', 'poly_948', 'poly_949', 'poly_950', 'poly_951', 'poly_952', 'poly_953', 'poly_954', 'poly_955',\n",
        "                  'poly_956', 'poly_957', 'poly_958', 'poly_959', 'poly_960', 'poly_961', 'poly_962', 'poly_963', 'poly_964', 'poly_965', 'poly_966', 'poly_967', 'poly_968',\n",
        "                  'poly_969', 'poly_970', 'poly_971', 'poly_973', 'poly_974', 'poly_975', 'poly_976', 'poly_977', 'poly_978', 'poly_979', 'poly_980', 'poly_981', 'poly_982',\n",
        "                  'poly_983', 'poly_984', 'poly_985', 'poly_986', 'poly_987', 'poly_988', 'poly_989', 'poly_990', 'poly_991', 'poly_992', 'poly_993', 'poly_994', 'poly_995',\n",
        "                  'poly_996', 'poly_997', 'poly_998', 'poly_999', 'poly_1000',\n",
        "                  'poly_1001', 'poly_1002', 'poly_1003', 'poly_1004', 'poly_1005', 'poly_1006', 'poly_1007', 'poly_1008', 'poly_1009', 'poly_1010', 'poly_1011', 'poly_1012', 'poly_1013', 'poly_1014', 'poly_1015',\n",
        "                  'poly_1016', 'poly_1017', 'poly_1018', 'poly_1019', 'poly_1020', 'poly_1021', 'poly_1022', 'poly_1023', 'poly_1024', 'poly_1025', 'poly_1026', 'poly_1027', 'poly_1028',\n",
        "                  'poly_1029', 'poly_1030', 'poly_1031', 'poly_1033', 'poly_1034', 'poly_1035', 'poly_1036', 'poly_1037', 'poly_1038', 'poly_1039', 'poly_1040', 'poly_1041', 'poly_1042',\n",
        "                  'poly_1043', 'poly_1044', 'poly_1045', 'poly_1046', 'poly_1047', 'poly_1048', 'poly_1049', 'poly_1050', 'poly_1051', 'poly_1052', 'poly_1053', 'poly_1054', 'poly_1055',\n",
        "                  'poly_1056', 'poly_1057', 'poly_1058', 'poly_1059', 'poly_1060', 'poly_1061', 'poly_1062', 'poly_1063', 'poly_1064', 'poly_1065', 'poly_1066', 'poly_1067', 'poly_1068',\n",
        "                  'poly_1069', 'poly_1070', 'poly_1071', 'poly_1073', 'poly_1074', 'poly_1075', 'poly_1076', 'poly_1077', 'poly_1078', 'poly_1079', 'poly_1080', 'poly_1081', 'poly_1082',\n",
        "                  'poly_1083', 'poly_1084', 'poly_1085', 'poly_1086', 'poly_1087', 'poly_1088', 'poly_1089', 'poly_1090', 'poly_1091', 'poly_1092', 'poly_1093', 'poly_1094', 'poly_1095',\n",
        "                  'poly_1096', 'poly_1097', 'poly_1098', 'poly_1099', 'poly_1100',\n",
        "                  'poly_1101', 'poly_1102', 'poly_1103', 'poly_1104', 'poly_1105', 'poly_1106', 'poly_1107', 'poly_1108',\n",
        "                  'poly_1109', 'poly_1110', 'poly_1111', 'poly_1113', 'poly_1114', 'poly_1115', 'poly_1116', 'poly_1117', 'poly_1118', 'poly_1119', 'poly_1120', 'poly_1121', 'poly_1122',\n",
        "                  'poly_1123', 'poly_1124', 'poly_1125', 'poly_1126', 'poly_1127', 'poly_1128', 'poly_1129', 'poly_1130', 'poly_1131', 'poly_1132', 'poly_1133', 'poly_1134', 'poly_1135',\n",
        "                  'poly_1136', 'poly_1137', 'poly_1138', 'poly_1139', 'poly_1140', 'poly_1141',\n",
        "                  'poly_1142', 'poly_1143', 'poly_1144', 'poly_1145', 'poly_1146', 'poly_1147', 'poly_1148',\n",
        "                  'poly_1149', 'poly_1150', 'poly_1151', 'poly_1153', 'poly_1154', 'poly_1155', 'poly_1156', 'poly_1157', 'poly_1158', 'poly_1159', 'poly_1160', 'poly_1161', 'poly_1162',\n",
        "                  'poly_1163', 'poly_1164', 'poly_1165', 'poly_1166', 'poly_1167', 'poly_1168', 'poly_1169', 'poly_1170', 'poly_1171', 'poly_1172', 'poly_1173', 'poly_1174', 'poly_1175',\n",
        "                  'poly_1176', 'poly_1177', 'poly_1178', 'poly_1179', 'poly_1180', 'poly_1181', 'poly_1182', 'poly_1183', 'poly_1184', 'poly_1185', 'poly_1186', 'poly_1187', 'poly_1188',\n",
        "                  'poly_1189', 'poly_1190', 'poly_1191', 'poly_1192', 'poly_1193', 'poly_1194', 'poly_1195', 'poly_1196', 'poly_1197', 'poly_1198', 'poly_1199', 'poly_1200',\n",
        "                  'poly_1201', 'poly_1202', 'poly_1203', 'poly_1204', 'poly_1205', 'poly_1206', 'poly_1207', 'poly_1208', 'poly_1209', 'poly_1210', 'poly_1211', 'poly_1212', 'poly_1213', 'poly_1214', 'poly_1215',\n",
        "                  'poly_1216', 'poly_1217', 'poly_1218', 'poly_1219', 'poly_1220', 'poly_1221', 'poly_1222', 'poly_1223', 'poly_1224', 'poly_1225', 'poly_1226', 'poly_1227', 'poly_1228',\n",
        "                  'poly_1229', 'poly_1230', 'poly_1231', 'poly_1233', 'poly_1234', 'poly_1235', 'poly_1236', 'poly_1237', 'poly_1238', 'poly_1239', 'poly_1240', 'poly_1241', 'poly_1242',\n",
        "                  'poly_1243', 'poly_1244', 'poly_1245', 'poly_1246', 'poly_1247', 'poly_1248', 'poly_1249', 'poly_1250', 'poly_1251', 'poly_1252', 'poly_1253', 'poly_1254', 'poly_1255',\n",
        "                  'poly_1256', 'poly_1257', 'poly_1258', 'poly_1259', 'poly_1260', 'poly_1261', 'poly_1262', 'poly_1263', 'poly_1264', 'poly_1265', 'poly_1266', 'poly_1267', 'poly_1268',\n",
        "                  'poly_1269', 'poly_1270', 'poly_1271', 'poly_1273', 'poly_1274', 'poly_1275', 'poly_1276', 'poly_1277', 'poly_1278', 'poly_1279', 'poly_1280', 'poly_1281', 'poly_1282',\n",
        "                  'poly_1283', 'poly_1284', 'poly_1285', 'poly_1286', 'poly_1287', 'poly_1288', 'poly_1289', 'poly_1290', 'poly_1291', 'poly_1292', 'poly_1293', 'poly_1294', 'poly_1295',\n",
        "                  'poly_1296', 'poly_1297', 'poly_1298', 'poly_1299', 'poly_1300',\n",
        "                  'poly_1301', 'poly_1302', 'poly_1303', 'poly_1304', 'poly_1305', 'poly_1306', 'poly_1307', 'poly_1308', 'poly_1309', 'poly_1310', 'poly_1311', 'poly_1312', 'poly_1313', 'poly_1314', 'poly_1315',\n",
        "                  'poly_1316', 'poly_1317', 'poly_1318', 'poly_1319', 'poly_1320', 'poly_1321', 'poly_1322', 'poly_1323', 'poly_1324', 'poly_1325', 'poly_1326', 'poly_1327', 'poly_1328',\n",
        "                  'poly_1329', 'poly_1330', 'poly_1331', 'poly_1333', 'poly_1334', 'poly_1335', 'poly_1336', 'poly_1337', 'poly_1338', 'poly_1339', 'poly_1340', 'poly_1341', 'poly_1342',\n",
        "                  'poly_1343', 'poly_1344', 'poly_1345', 'poly_1346', 'poly_1347', 'poly_1348', 'poly_1349', 'poly_1350', 'poly_1351', 'poly_1352', 'poly_1353', 'poly_1354', 'poly_1355',\n",
        "                  'poly_1356', 'poly_1357', 'poly_1358', 'poly_1359', 'poly_1360', 'poly_1361', 'poly_1362', 'poly_1363', 'poly_1364', 'poly_1365',\n",
        "\n",
        "                  #fvol\n",
        "                  'fvol_EMA20','fvol_EMA40','fvol_EMA60','fvol_EMA80',#'fvol_EMA3','fvol_EMA3_shift1','fvol_EMA4','fvol_EMA5','fvol_EMA6','fvol_EMA7','fvol_EMA8','fvol_EMA9','fvol_EMA10','fvol_EMA11','fvol_EMA12','fvol_EMA13','fvol_EMA14','fvol_EMA15','fvol_EMA16',\n",
        "                  'fvol_shift7',\n",
        "                  #'fvol_POW2','fvol_POW3','fvol_POW3_shift1',#'fvol_POW4',\n",
        "                  'fvol','fvol_shift1','fvol_shift2','fvol_shift3','fvol_shift4','fvol_shift5','fvol_shift6','fvol_shift8','fvol_shift9','fvol_shift10',\n",
        "                  'fvol_RSI',#'fvol_RSI3','fvol_RSI3_shift1','fvol_RSI4','fvol_RSI5','fvol_RSI6','fvol_RSI7','fvol_RSI8','fvol_RSI9','fvol_RSI10','fvol_RSI11','fvol_RSI12','fvol_RSI13',\n",
        "                  #'fvol_EMA3','fvol_EMA3_shift1','fvol_EMA4','fvol_EMA5','fvol_EMA6','fvol_EMA7','fvol_EMA8','fvol_EMA9','fvol_EMA10','fvol_EMA11','fvol_EMA12','fvol_EMA13','fvol_EMA14','fvol_EMA15','fvol_EMA16',\n",
        "                  'fvol_STD_shift21',\n",
        "                  'fvol_STD',#'fvol_STD_shift1','fvol_STD_shift2','fvol_STD_shift3','fvol_STD_shift4','fvol_STD_shift5','fvol_STD_shift6','fvol_STD_shift10','fvol_STD_shift19','fvol_STD_shift20',\n",
        "                  'fvol_MACD', 'fvol_MACD_signal', 'fvol_MACD7', 'fvol_MACD_signal7',\n",
        "                  #'fvol','fvol_shift1','fvol_shift2','fvol_shift3','fvol_shift4','fvol_shift5','fvol_shift6','fvol_shift7','fvol_shift8','fvol_shift9','fvol_shift10',\n",
        "                  #'fvol_RSI3','fvol_RSI3_shift1','fvol_RSI4','fvol_RSI5','fvol_RSI6','fvol_RSI7','fvol_RSI8','fvol_RSI9','fvol_RSI10','fvol_RSI11','fvol_RSI12','fvol_RSI13','fvol_RSI',\n",
        "                  #'fvol_SKEW','fvol_SKEW_shift1',\n",
        "                  'MACD_', 'MACD_signal_','MACD_2', 'MACD_signal_2','MACD_3', 'MACD_signal_3','MACD_4', 'MACD_signal_4',\n",
        "                  #'Close','Volume',\n",
        "                  \"dayofweek\",\"day\",\"monthstart\",\"monthend\",\"quarterstart\",\"quarterend\",\"yearstart\",\"yearend\",\n",
        "                  \"monthendday\",\"quarterstartday\",\"quarterendday\",\"yearendday\",\"yearendmonthstart\",\"yearendmonthend\",\n",
        "                  \"yearendquarterstart\",\"yearendquarterend\",#\"monthstartday\", \"quarterstartdayofweek\",\"quarterenddayofweek\",\"yearstartdayofweek\",\"yearenddayofweek\",\"dayofweekday\",     \"yearstartday\",\n",
        "                  #305p11'100returns',\n",
        "                  #'logVolume',\"barSkew\",\"barKurt\",'Mean2',\n",
        "\n",
        "                  #not used for 74-83%\n",
        "                  #305p11'log_returns','Vlog_returns','log_returns2','Vlog_returns2','log_returns3','LRET','VLRET','RVOL','RVOL2',\n",
        "                  'lretVEMA3','lretVEMA3_shift1','lretVEMA4','lretVEMA5','lretVEMA6','lretVEMA7','lretVEMA8','lretVEMA9','lretVEMA10',\n",
        "                  'VSTD','VSTD_shift1','SKEW','SKEW_shift1','VSKEW',\n",
        "                  'Close','Volume',\n",
        "                  ##replaced the open high low close with change/lret of those\n",
        "                  #'lretPOW05','lretPOW2','lretVPOW2','lretPOW3','lretPOW3_shift1','lretVPOW05',#'lretPOW4','lretPOW5','lretVPOW3','lretVPOW4','lretVPOW5',\n",
        "                  'lretSTD','lretSTD_shift1','lretSTD_shift2','lretSTD_shift3','lretSTD_shift4','lretSTD_shift5','lretSTD_shift6','lretSTD_shift10','lretSTD_shift19','lretSTD_shift20','lretSTD_shift21',\n",
        "                  'lretEMA3','lretEMA3_shift1','lretEMA13','lretEMA4','lretEMA5','lretEMA6','lretEMA7',#'lretEMA8','lretEMA9','lretEMA10','lretEMA11','lretEMA12','lretEMA14','lretEMA15','lretEMA16','lretEMA20','lretEMA40','lretEMA60','lretEMA80',\n",
        "                  'MACD_signallret','MACD2lret', 'MACD_signal2lret','MACD3lret', 'MACD_signal3lret',#'MACDlret',\n",
        "                  'HL_sprdlret','CO_sprdlret','logVolumelret','mean_pricelret','median_pricelret','barMeanlret',\"barMedianlret\",'upper_shadow1lret','lower_shadow1lret','high2lowlret','Meanlret',\n",
        "                  'lretMean',#,'open2closelret'\n",
        "                  'lrethigh2mean','lrethigh2median','lretlow2median','lretvolume2vol+1',#'lretlow2mean',\n",
        "                  \"lretopensubclose\",'lretUpper_Shadow','lretLower_Shadow',#'lrethlco_ratio',\n",
        "                  \"lretClose/Open\",\"lretClose-Open\",\"lretClose-Open_shift1\", #\"lretHigh-Low\", 'lretHigh/Mean','lretLow/Mean',\n",
        "                  'lretVolume/Close',\n",
        "                  'shares',\n",
        "                  # Categories\n",
        "                  'GICS_1','GICS_2','GICS_3','GICS_subindustry',\n",
        "                  'CountryRisk_name','CountryDomicile_ISO2','Currency','ISIN_code',\n",
        "                  # Fundamentals\n",
        "                  'earningsQuarterlyGrowth_raw', 'revenueGrowth_raw', 'grossMargins_raw','profitMargins_raw',\n",
        "                  #'priceToBook_raw', #gives problems with null or inf\n",
        "                  'payoutRatio_raw','dividendRate_raw','dividendYield_raw','fiveYearAvgDividendYield_raw',\n",
        "                  #'trailingAnnualDividendRate_raw','trailingAnnualDividendYield_raw', #also give error with feature_df\n",
        "                  #TODO: add fama&french carhart factors\n",
        "                  'sector','industry',\n",
        "                  #higher powers quadratic/cubic\n",
        "                  #'EMA3_shift1_2',#'EMA4_2','EMA5_2','EMA6_2','EMA7_2','EMA8_2','EMA9_2','EMA10_2','EMA11_2','EMA12_2',\n",
        "                  #'EMA13_2',\n",
        "                  #'RSI3_2','RSI3_shift1_2','RSI4_2','RSI5_2','RSI6_2','RSI7_2','RSI8_2','RSI9_2','RSI10_2','RSI11_2','RSI12_2','RSI13_2','RSI_2', #same quadratic\n",
        "                  #'LRET_2','LRET_shift1_2',\n",
        "                  #'VRSI3_2','VRSI3_shift1_2','VRSI4_2','VRSI5_2','VRSI6_2','VRSI7_2','VRSI8_2','VRSI9_2','VRSI10_2','VRSI11_2','VRSI12_2','VRSI13_2','VRSI_2', #same quadratic\n",
        "                  #'EMA3_2',\n",
        "                  'max30d_close','min30d_close','mean30d_close','max30drange','min30drange','mean30drange',#'max30drange_q','min30drange_q','mean30drange_q',\n",
        "                 ]\n",
        "#TODO add mean features from jhtalib\n",
        "\n",
        "    # only new feats\n",
        "    feature_df = feature_df[new_feats + keys]\n",
        "    # fill nan\n",
        "    feature_df.fillna(method='ffill', inplace=True) # safe fillna method for a forecasting task\n",
        "    feature_df.fillna(method='bfill', inplace=True) # just in case ... making sure no nan\n",
        "    #clean DF\n",
        "    feature_df = feature_df.replace([np.inf, -np.inf, np.nan], -20000)\n",
        "    feature_df.dropna(inplace=True)\n",
        "    return reduce_mem_usage(feature_df)\n",
        "\n",
        "def add_features(df):\n",
        "    # FE with multiprocessing\n",
        "    tickers = df['ticker'].unique().tolist()\n",
        "    print('FE for {:,} stocks...using {:,} CPUs...'.format(len(tickers), cpu_count()))\n",
        "    start_time = time.time()\n",
        "    with Pool(cpu_count()) as p:\n",
        "        feature_dfs = list(tqdm(p.imap(feature_engineering, tickers), total=len(tickers)))\n",
        "    return pd.concat(feature_dfs)\n",
        "    # takes 7min till here when ANALYSE=0"
      ],
      "id": "26532731"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "1f24091c"
      },
      "outputs": [],
      "source": [
        "%%time\n",
        "feature_df = add_features(df)\n",
        "\n",
        "#take3, 2221 stocks 2393stocks 46min23s\n",
        "#round559 node2: 2430 stocks, 1h5m32s; 1h11m23s (second run with random and macd expanded);1h1m10s\n",
        "#round562 node2: 2011 stocks 32m59s\n",
        "#round564 node2: 1303 20m41s\n",
        "#round565 node2: 856 16m12s; 1236 28m25s; 1629 45m24s; 1955 44m30s\n",
        "#round566 3948 1h52m\n",
        "#567: 2021 46min\n",
        "#568node2: K 2195; 1920 44m34s; 4342 2h20m kernel died\n",
        "#569node2: 46m42s; 3916 2h1s 297GB failed at merge with targest; 2785\n",
        "#571node2: 2186 59m37s\n",
        "#573node2: 1822 44m7s\n",
        "#574node2: 123.2GB; 2190 87.3GB 10m25s\n",
        "#574node2: 2580 1h13m27s; EDSdata 2104 48m15s; 5840days 2541 kernel died, 2175 50m31s; 2487 1h5m\n",
        "#574node2: 2190 48m48s\n",
        "#575node2: 4606 2h45m55s; 4606 2h40m51s; 777 15m2s; 2982\n",
        "#576node2: 4345 2h36m\n",
        "#578node2: 3847 2h8m38s; 1703 ok with errors in1h; 3847 2h13m; 2175;\n",
        "#579node2: 2693 1h23m system reset; 2111 1h7m; K2141 1h4m31s; E2082 53m49s;\n",
        "#580node2: K2788 kernel died; K2058 55m48s\n",
        "#581node2: K 2585 1h17m54s; 2551 1h13m57s; 2152 1h5m31s\n",
        "#583node2: K5200 1878 50m 48m4s 49m27s; K5400 2152 1h6m3s; 1h11m12s; 57m27s; K5420 2541 1h22m2s;\n",
        "#584node2: K5440 2655 server reset; 2587 ANALYSIS=0 1h24m43s; 2591 2h42m15s\n",
        "#TODO: detail test memory\n",
        "#585node2: K2592 1h15m40s\n",
        "#586node2: K5440 2607 1h24m40s; K5435 2549 1h15m28s; Kall 10y 4284 2h35m53s\n",
        "#TODO: change last friday data for weekdays\n",
        "#589node2: 6138T 2h8m K6038 4287\n",
        "#589node2: K5035 1783 38m42s; K5335 1903; K5380 2095 50m41s\n",
        "#590node2: K5395 2108 50m12s; K5405 2118 1h3m; K5415 2158 58m7s; K5430 2513 1h9m43s;\n",
        "#594node2: 2756 1h25m; K5599 3732 2h30m; K5489 3697 2h6m57s\n",
        "#616colab: without cudf extension loaded 195 4.4GBRam"
      ],
      "id": "1f24091c"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "a5d4e257-d97b-4462-bef7-80c56fa56e62"
      },
      "outputs": [],
      "source": [
        "if CFG.LEAN == 1:\n",
        "    del df  #to free up space, but does not allow model to be rerun\n",
        "    gc.collect()"
      ],
      "id": "a5d4e257-d97b-4462-bef7-80c56fa56e62"
    },
    {
      "cell_type": "raw",
      "metadata": {
        "id": "d176040f"
      },
      "source": [
        "#https://colab.research.google.com/drive/1ECh69C0LDCUnuyvEmNFZ51l_276nkQqo#scrollTo=qMnJoa59b5K\n",
        "\"\"\"\n",
        "# group by era (date) and create quintile labels within each era, useful for learning relative ranking\n",
        "date_groups = full_data.groupby(full_data.index)\n",
        "full_data['RSI_quintile'] = date_groups['RSI'].transform(lambda group: pd.qcut(group, 5, labels=False, duplicates='drop'))\n",
        "full_data.dropna(inplace=True)\n",
        "\n",
        "full_data.head()\n",
        "#create lagged features, lag 0 is that day's value, lag 1 is yesterday's value, etc\n",
        "num_days = 5\n",
        "for day in range(num_days+1):\n",
        "    full_data[f'RSI_quintile_lag_{day}'] = ticker_groups['RSI_quintile'].transform(lambda group: group.shift(day))\n",
        "# create difference of the lagged features (change in RSI quintile by day)\n",
        "for day in range(num_days):\n",
        "    full_data[f'RSI_diff_{day}'] = full_data[f'RSI_quintile_lag_{day}'] - full_data[f'RSI_quintile_lag_{day + 1}']\n",
        "    full_data[f'RSI_abs_diff_{day}'] = np.abs(full_data[f'RSI_quintile_lag_{day}'] - full_data[f'RSI_quintile_lag_{day + 1}'])\n",
        "\"\"\""
      ],
      "id": "d176040f"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "12facbe4"
      },
      "outputs": [],
      "source": [
        "%%time\n",
        "# statistics of the dataframe\n",
        "if CFG.ANALYSE == 1:\n",
        "    print(feature_df.shape)\n",
        "    feature_df.head()\n",
        "    feature_df.tail()\n",
        "#round 305p: 44GB in memory\n",
        "#559node2: 223GB in memory"
      ],
      "id": "12facbe4"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7674627e"
      },
      "source": [
        "# Merge Targets and Features\n",
        "Feature engineering is done. Let's merge it with the numerai historical targets."
      ],
      "id": "7674627e"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "06100291"
      },
      "outputs": [],
      "source": [
        "%%time\n",
        "if CFG.ANALYSE == 1:\n",
        "# do we have enough overlap with respect to 'friday_date'?\n",
        "    venn2([\n",
        "        set(feature_df['friday_date'].astype(str).unique().tolist())\n",
        "        , set(targets['friday_date'].astype(str).unique().tolist())\n",
        "    ], set_labels=('features_days', 'targets_days'))"
      ],
      "id": "06100291"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "c5e93fec"
      },
      "outputs": [],
      "source": [
        "%%time\n",
        "if CFG.ANALYSE == 1:\n",
        "# do we have enough overlap with respect to 'ticker'?\n",
        "    venn2([\n",
        "        set(feature_df['ticker'].astype(str).unique().tolist())\n",
        "        , set(targets['ticker'].astype(str).unique().tolist())\n",
        "    ], set_labels=('features_ticker', 'targets_ticker'))\n",
        "#583node2: 7.81s"
      ],
      "id": "c5e93fec"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "600a7045"
      },
      "outputs": [],
      "source": [
        "\"\"\"# Using the mean_absolute_percentage_error function\n",
        "from sklearn.metrics import mean_absolute_percentage_error\n",
        "\n",
        "error = mean_absolute_percentage_error(y_true, predictions)\n",
        "df_switch=1\n",
        "if df_switch == 1:\n",
        "    df['friday_date'] = df['date'].apply(lambda x : int(str(x).replace('-', '')))\n",
        "else:\n",
        "    ## change date different for owm data from yahoofinance\n",
        "    df['friday_date'] = df['date'].apply(lambda x: str(x).replace('-', ''))#.dt.normalize()\n",
        "\"\"\""
      ],
      "id": "600a7045"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "4ea9ddbc"
      },
      "outputs": [],
      "source": [
        "%%time\n",
        "feature_df['friday_date'] = feature_df['friday_date']\n",
        "targets['friday_date'] = targets['friday_date'].astype(int)\n",
        "#589node2: 6138T 9.13s\n",
        "#590node2: K5380 4.32s"
      ],
      "id": "4ea9ddbc"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "a46f0841"
      },
      "outputs": [],
      "source": [
        "#print(feature_df.dtypes)\n",
        "#print(feature_df.friday_date.head())\n",
        "#print(targets.friday_date.head())"
      ],
      "id": "a46f0841"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "e2e73a3b"
      },
      "outputs": [],
      "source": [
        "%%time\n",
        "if CFG.TYPE == \"K\":\n",
        "    feature_df['friday_date'] = pd.to_datetime(feature_df['friday_date'], format='%Y%m%d').dt.strftime('%Y%m%d').astype(int)#add back for returngazer  %H:%M:%S\n",
        "else:\n",
        "    feature_df['friday_date'] = pd.to_datetime(feature_df['friday_date'], format='%Y%m%d %H:%M:%S').dt.strftime('%Y%m%d').astype(int)\n",
        "\n",
        "feature_df['friday_date'] = feature_df['friday_date'].astype(int)\n",
        "if CFG.ANALYSE == 1:\n",
        "    print(feature_df.dtypes)\n",
        "\n",
        "#round337 node1: 2486\n",
        "#round359 node3: 2591\n",
        "#round369 node3: 3834 68.1GB\n",
        "#round371 node3: too much memory\n",
        "#round566 node2: 1m41s\n",
        "#575node2: 1m51s; #578node2: 1m44s\n",
        "#580node2: 36.1s\n",
        "#581node2: 1m3s\n",
        "#583node2: 47.6s\n",
        "#584node2: 4m54s\n",
        "#586node2: Kall 10y 1m; 2m\n",
        "#589node2: 6138T 1m52s\n",
        "#590node2: K5395 39.9s ; K5415 35.4s"
      ],
      "id": "e2e73a3b"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "f1e55ae7-f16f-42ef-839e-365f05513480"
      },
      "outputs": [],
      "source": [
        "%%time\n",
        "\n",
        "#targets_ddf = dd.from_pandas(targets,npartitions=8)\n",
        "#feature_ddf = dd.from_pandas(feature_df,npartitions=8)\n",
        "#8m25s\n",
        "#54m19s"
      ],
      "id": "f1e55ae7-f16f-42ef-839e-365f05513480"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "1c40aa47-5d07-4c7c-9220-08773409b20e"
      },
      "outputs": [],
      "source": [
        "%%time\n",
        "\"\"\"\n",
        "# Dask merge\n",
        "feature_ddf = feature_ddf.merge(\n",
        "    targets_ddf,\n",
        "    how='left',\n",
        "    on=['friday_date', 'ticker']\n",
        ")\n",
        "feature_ddf = feature_ddf.repartition(npartitions=1)\n",
        "\"\"\"\n",
        "#580node2: 11.9s"
      ],
      "id": "1c40aa47-5d07-4c7c-9220-08773409b20e"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "62bc1760-e02d-472a-8015-ecebb96ff98c"
      },
      "outputs": [],
      "source": [
        "\"\"\"\n",
        "ddf = dd.read_parquet(\n",
        "    \"../../Data/feature_df\",\n",
        "    storage_options={\"anon\": True, \"use_ssl\": True},\n",
        "    engine=\"pyarrow\",\n",
        ")\n",
        "\"\"\""
      ],
      "id": "62bc1760-e02d-472a-8015-ecebb96ff98c"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "fdfcac3a"
      },
      "outputs": [],
      "source": [
        "%%time\n",
        "# pandas set multiindex\n",
        "feature_df.set_index(['ticker', 'friday_date'], inplace=True)\n",
        "targets.set_index(['ticker', 'friday_date'], inplace=True)\n",
        "# merge\n",
        "feature_df = feature_df.merge(\n",
        "    targets,\n",
        "    how='left',\n",
        "    on=['friday_date', 'ticker']\n",
        ")\n",
        "\n",
        "#581node2: 11m\n",
        "#581node2: 14m38s\n",
        "#583node2:\n",
        "#TODO: measure memory peak\n",
        "#584node2: 6m51s; 12m42s; 24m12s\n",
        "#586node2: 12m38s; Kall 10y 54m23s\n",
        "#589node2: 6138T 1h5m21s; kernel died; K5000 8m6s\n",
        "#590node2: 5380 10m47s; K5395 9m29s; K5415 8m55s; K5430 Kernel Restart\n",
        "#594node2: 5499 3732 1h12m47s"
      ],
      "id": "fdfcac3a"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "d923b563-0daf-4c1e-8c00-e16793bb2dac"
      },
      "outputs": [],
      "source": [
        "%%time\n",
        "#feature_ddf.to_parquet('../../Data/feature_df.parquet')\n",
        "#feature_df = feature_ddf.compute() # takes >1h\n",
        "if CFG.LEAN == 1:\n",
        "    del targets#, feature_ddf\n",
        "    gc.collect()\n",
        "#569node2: 7m48s\n",
        "#575node2: 1h57s\n",
        "#576node2: 50m\n",
        "#579node2: 10m56s\n",
        "#580node2: replaced by dask compute: start 443GB >1h interuptes.\n",
        "#repartition and parquat start 443GB\n",
        "#580node2: del targets 2.15s\n",
        "#581node2: 2.26s\n",
        "#582node2: 2.63s\n",
        "#584node2: 2.83s\n",
        "#586node2: 2.87s; Kall 10y 5.86s\n",
        "#590node2: K5380 2.65s"
      ],
      "id": "d923b563-0daf-4c1e-8c00-e16793bb2dac"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "dc00813e"
      },
      "outputs": [],
      "source": [
        "%%time\n",
        "#feature_df = pd.read_parquet('../../Data/feature_df.parquet')\n",
        "# save (just to make sure that we are on the safe side if yfinance is dead some day...)\n",
        "#feature_df.to_pickle(f'{CFG.OUTPUT_DIR}/feature_df.pkl')\n",
        "if CFG.ANALYSE == 1:\n",
        "    feature_df.info()\n",
        "#round363: 5.8gb\n",
        "#round559 76.1GB\n",
        "#round562 node2: 49GB\n",
        "#569node2: 75GB\n",
        "#574node2: 110.2GB\n",
        "#575node2: 281.3GB\n",
        "#576node2: 255.4GB\n",
        "#579node2: 87.5GB; 89.1GB\n",
        "#580node2: 78.8GB\n",
        "#581node2: 84.8GB\n",
        "#582node2: 124.2GB\n",
        "#583node2: 71.1GB 5200"
      ],
      "id": "dc00813e"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2d727c2a"
      },
      "source": [
        "We now have a merged features + target table! It seems like we are ready for modeling."
      ],
      "id": "2d727c2a"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "38c4fedf"
      },
      "source": [
        "# Modeling\n",
        "\n",
        "## XGBoost\n",
        "\n",
        "The hyperparameters are derived from the Integration-Test, which is an example yet a strong baseline for the Numerai Tournament."
      ],
      "id": "38c4fedf"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "945288a5"
      },
      "outputs": [],
      "source": [
        "%%time\n",
        "target = 'target_20d'   #'target_20d_factor_feat_neutral'\n",
        "if 'target_20d' not in feature_df.columns.values.tolist():\n",
        "    print('No target 20d exists...using target_4d instead...')\n",
        "    target = 'target'\n",
        "drops_1 = ['data_type','friday_date', 'ticker', 'bloomberg_ticker']\n",
        "# List of columns to drop that start with \"target_\"\n",
        "target_cols_to_drop = [col for col in feature_df.columns if col.startswith('target_')]\n",
        "\n",
        "# Combine the two lists of columns to drop\n",
        "drops = list(set(drops_1 + target_cols_to_drop))\n",
        "\n",
        "features = [f for f in feature_df.columns.values.tolist() if f not in drops]\n",
        "\n",
        "#logger.info('{:,} features: {}'.format(len(features), features))\n",
        "#TODO: try feature neutral target for highest True Contribution model\n",
        "#TODO: avoid kernel restart"
      ],
      "id": "945288a5"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "f3bde930"
      },
      "outputs": [],
      "source": [
        "#TODO: check use of natural logarithm, https://people.duke.edu/~rnau/411log.htm & https://quantivity.wordpress.com/2011/02/21/why-log-returns/\n",
        "#Realized volatility, kurtosis and trading volume are log transformed since their non-negativity condition needs to be\n",
        "#satisfied when they are modelled. Besides, the use of realized logarithmic volatility in empirical analysis is well\n",
        "#supported in the literature (e.g., Andersen et al., 2001a, and Andersen et al., 2003). In addition, we use realized\n",
        "#logarithmic kurtosis and logarithmic trading volume to achieve a similar scale for the subsequent impulse response\n",
        "#analyses. Therefore, when we refer to realized volatility, kurtosis and trading volume in our study, they are in their\n",
        "#natural logarithmic form. Source: Do,Brooks,Treepongkaruna,Wu-2014-How does trading volume affect financial return distributions?"
      ],
      "id": "f3bde930"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "b1aab42b-788b-4db8-8ed2-15b60858951a"
      },
      "outputs": [],
      "source": [
        "%%time\n",
        "#print(len(feature_df.query('data_type == \"validation\"')))\n",
        "#print(len(feature_df.query('data_type == \"validation\"').dropna()))\n",
        "#print(feature_df.info())\n",
        "#print(feature_df.shape)\n",
        "#round554 node2: 3m25s\n",
        "#round559 node2: 4m2s; 4min45s\n",
        "#round565 node2: 3m29s\n",
        "#569node2: 4m11s\n",
        "#571node2: 4m48s\n",
        "#575node2: 457GB in memory"
      ],
      "id": "b1aab42b-788b-4db8-8ed2-15b60858951a"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "eea6edc8-1321-48b5-8219-a1fb426f0dce"
      },
      "outputs": [],
      "source": [
        "\n",
        "#feature_df.drop(columns=['random_category'], inplace=False)\n",
        "# to encode categorical features which are non-integer\n",
        "encoder = LabelEncoder()\n",
        "#feature_df['random_category'] = encoder.fit_transform(feature_df['random_category'])"
      ],
      "id": "eea6edc8-1321-48b5-8219-a1fb426f0dce"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "bd3e75ed-2772-4173-9e4b-6ffdae30fda2"
      },
      "outputs": [],
      "source": [
        "%%time\n",
        "list_memory_usage = []\n",
        "if CFG.ANALYSE == 1:\n",
        "    # analyse dtypes\n",
        "    pd.set_option('display.max_rows', 5200)\n",
        "    print(feature_df.dtypes)"
      ],
      "id": "bd3e75ed-2772-4173-9e4b-6ffdae30fda2"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "0aa5da28"
      },
      "outputs": [],
      "source": [
        "%%time\n",
        "# Prepare data for XGB feature store queried dataframes\n",
        "if CFG.ANALYSE == 1:\n",
        "    tm.start()\n",
        "feature_df.reset_index(inplace=True)\n",
        "# train-valid split\n",
        "train_set = {\n",
        "    'X': feature_df.query('data_type == \"train\"')[features],\n",
        "    'y': feature_df.query('data_type == \"train\"')[target].astype(np.float64)\n",
        "}\n",
        "val_df = feature_df.query('data_type == \"validation\"').dropna().copy()\n",
        "val_set = {\n",
        "    'X': val_df[features],\n",
        "    'y': val_df[target].astype(np.float64)\n",
        "}\n",
        "\n",
        "assert train_set['y'].isna().sum() == 0\n",
        "assert val_set['y'].isna().sum() == 0\n",
        "\n",
        "#round305p:115GB in memory\n",
        "#round305p run11: 125 in memory 15gb swap\n",
        "#round337 node1: 1min14s\n",
        "#round348 node3: 4.06s\n",
        "#round554 node2: 3m31s 243GB; 3m59s 259GB\n",
        "#round559 node2: 4min8s 234GBstart 363GBend; 5min25s\n",
        "#round562 node2: 1m17s\n",
        "#round565 2m47s\n",
        "#round566 2m40s\n",
        "#569node2: 2m49s; 3m54s\n",
        "#574node2: 4m50s; 5m2s\n",
        "\n",
        "#6m36s\n",
        "#580node2: 4m45s\n",
        "#581node2: 5m\n",
        "#581node2: 8m31s\n",
        "#584node2: 9m28s; 10m14s; 10m31s\n",
        "#586node2: 29m36s\n",
        "#589node2: server reset\n",
        "#590node2: K5385 5m36s; K5395 4m39s; K5405 4m51s; K5415 4m5s\n",
        "#593node2: K5429 9m36s;\n",
        "#594node2: K5499 kernel died"
      ],
      "id": "0aa5da28"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "5032bbaf-4e8d-42c1-ac79-51537adce677"
      },
      "outputs": [],
      "source": [
        "%%time\n",
        "if CFG.ANALYSE == 1:\n",
        "    memory_usage = tm.get_traced_memory()\n",
        "    tm.stop()\n",
        "    list_memory_usage.append({\n",
        "    \t'library': 'numpy',\n",
        "    \t'memory_usage': memory_usage\n",
        "    })"
      ],
      "id": "5032bbaf-4e8d-42c1-ac79-51537adce677"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "6cc09ca8-cee4-4ebc-98af-0b5795358f13"
      },
      "outputs": [],
      "source": [
        "\"\"\"\n",
        "%%time\n",
        "#TODO: install pyspark on node2\n",
        "## now switch to pyspark to test H20\n",
        "import pyspark\n",
        "from pyspark.sql import SparkSession\n",
        "if CFG.H2O == 1:\n",
        "    #Create PySpark SparkSession\n",
        "    spark = SparkSession.builder \\\n",
        "        .master(\"local[1]\") \\\n",
        "        .appName(\"SparkByExamples.com\") \\\n",
        "        .config(\"spark.driver.memory\", \"40g\") \\\n",
        "        .config(\"spark.executor.memory\", \"400g\") \\\n",
        "        .getOrCreate()\n",
        "    #Create PySpark DataFrame from Pandas\n",
        "    #records = feature_df.to_dict(orient='records') this step doubles memory and is too slow\n",
        "    #sparkDF = spark.createDataFrame(records)\n",
        "    sparkDF=spark.createDataFrame(feature_df)\n",
        "    sparkDF.printSchema()\n",
        "    sparkDF.show()\n",
        "#del feature_df\n",
        "gc.collect()\n",
        "\"\"\"\n",
        "#TODO: fix as it does not complete\n",
        "#TODO: setup hadoop distributed with node 3\n",
        "#TODO TypeError: field bloomberg_ticker: Can not merge type <class 'pyspark.sql.types.DoubleType'> and <class 'pyspark.sql.types.StringType'>"
      ],
      "id": "6cc09ca8-cee4-4ebc-98af-0b5795358f13"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "29c715de-4fc5-4850-8576-4873c0acff49"
      },
      "outputs": [],
      "source": [
        "'''\n",
        "# https://medium.com/codex/feature-selection-with-a-random-variable-5b9bb6558a66\n",
        "# Use XGBoost as base estimator for feature selector\n",
        "selector = XGBClassifier()\n",
        "selector.fit(X_train, y_train)\n",
        "# Select features ranked above RV\n",
        "df = pd.DataFrame({\"feature\": X_train.columns,\n",
        "                   \"importance\": selector.feature_importances_})\n",
        "rv_importance = df[df[\"feature\"] == \"random\"][\"importance\"].iloc[0]\n",
        "sel_cols = df[df[\"importance\"] > rv_importance][\"feature\"].tolist()# For 2 sets of models: one w/ all candidate features and one w/     selected features\n",
        "for model_name in [\"all_features\", \"selected_features\"]:\n",
        "    if model_name == \"all_features\":\n",
        "       feature_set = [i for i in X_train.columns if i != \"random\"]\n",
        "    else:\n",
        "       feature_set = sel_cols\n",
        "    new_X_train = X_train.filter(feature_set)\n",
        "    new_X_test = X_test.filter(feature_set)\n",
        "    num_features = len(feature_set)\n",
        "\n",
        "    start = time()    # Fit estimator on training set\n",
        "    model = XGBClassifier()\n",
        "    model.fit(new_X_train, y_train)\n",
        "\n",
        "   # Validate on test set\n",
        "   y_scores = model.predict_proba(new_X_test)[:, 1]\n",
        "\n",
        "   # Append metrics results to placeholder\n",
        "   training_times.append(time() - start)\n",
        "   model_names.append(model_name)\n",
        "   features_set_size.append(num_features)\n",
        "   aurocs.append(roc_auc_score(y_test, y_scores))\n",
        "   p, r, _ = precision_recall_curve(y_test, y_scores)\n",
        "   auprcs.append(auc(r, p))\n",
        "\n",
        "result = pd.DataFrame({\"model_name\": model_names,\n",
        "                       \"features_size\": features_set_size,\n",
        "                       \"auroc\": aurocs,\n",
        "                       \"auprc\": auprcs,\n",
        "                       \"training_time\": training_times})\n",
        "'''"
      ],
      "id": "29c715de-4fc5-4850-8576-4873c0acff49"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "28e78684-9ee7-40d4-bfa5-19a17d67cdca"
      },
      "outputs": [],
      "source": [
        "#Kernel died on XGB?? why..., try delete and compress after reading it back"
      ],
      "id": "28e78684-9ee7-40d4-bfa5-19a17d67cdca"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "a3bbc427"
      },
      "outputs": [],
      "source": [
        "%%time\n",
        "# do we have at least 5 tickers, whose the latest date matches the recent friday?\n",
        "ticker_date_df = feature_df.groupby('ticker')['friday_date'].max().reset_index()\n",
        "if len(ticker_date_df.loc[ticker_date_df['friday_date'] == recent_friday]) >= 5:\n",
        "    ticker_date_df = ticker_date_df.loc[ticker_date_df['friday_date'] == recent_friday]\n",
        "else: # use dates later than the second last friday\n",
        "    ticker_date_df = ticker_date_df.loc[ticker_date_df['friday_date'] == recent_friday2]\n",
        "    recent_friday = recent_friday2\n",
        "if CFG.ANALYSE == 1:\n",
        "    print(len(ticker_date_df))\n",
        "    ticker_date_df\n",
        "#581node2: 1.45s\n",
        "#584node2: 1.72s; 1.74s; 2.11s\n",
        "#590node2: 1.38s\n",
        "#594node2: 3.03s"
      ],
      "id": "a3bbc427"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "79357e72-1487-4111-ae61-6649efcad524"
      },
      "outputs": [],
      "source": [
        "#del feature_df\n",
        "#gc.collect()"
      ],
      "id": "79357e72-1487-4111-ae61-6649efcad524"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "f02f9bb8"
      },
      "outputs": [],
      "source": [
        "%%time\n",
        "if CFG.ANALYSE == 1:\n",
        "    tm.start()\n",
        "if CFG.XGB == 1:\n",
        "    #TODO: regularize more is it overfits from the start\n",
        "    # same parameters of the Integration-Test\n",
        "    params = {\n",
        "        'booster':'gbtree',\n",
        "        'eta':0.01,\n",
        "        #eta is also known as learning reate\n",
        "        'learning_rate': 0.01,\n",
        "        'min_child_weight':1,\n",
        "        'objective': 'reg:squarederror',#'multi:softmax',#'multi:softpred'\n",
        "        'eval_metric': 'rmse',\n",
        "        'colsample_bytree': 0.5,\n",
        "        'max_depth': CFG.XGB_DEPTH,\n",
        "        #'num_class':10,\n",
        "        'gamma':0.01,\n",
        "        #'subsample':0.9,\n",
        "        'lambda':0.5,\n",
        "        'alpha':0.5,\n",
        "        'seed': 46,\n",
        "        #'n_estimators': 3000,\n",
        "        #'tree_method': 'gpu_hist' # if you want to use GPU ...\n",
        "    }\n",
        "    #Store params with neptune.ai model register\n",
        "    if neptune_switch == 1:\n",
        "        run[\"parameters\"] = params\n",
        "        npt_callback = NeptuneCallback(run=run)\n",
        "    #Neptune code for xgboost\n",
        "    dtrain = xgb.DMatrix(train_set['X'], label=train_set['y'], enable_categorical=True)\n",
        "    dval = xgb.DMatrix(val_set['X'], label=val_set['y'], enable_categorical=True)\n",
        "    evals = [(dtrain, \"train\"), (dval, \"valid\")]\n",
        "\n",
        "    # You can also use Neptune with XGBoost scikit-learn API\n",
        "    # Check the user guide in the documentation for more details:\n",
        "    # https://docs.neptune.ai/integrations-and-supported-tools/model-training/xgboost\n",
        "\n",
        "    \"\"\"\n",
        "    if neptune_switch == 0:\n",
        "        xgb.train(\n",
        "        params=params,\n",
        "        dtrain=dtrain,\n",
        "        num_boost_round=30,\n",
        "        evals=evals,\n",
        "        callbacks=[\n",
        "            #npt_callback,\n",
        "            xgb.callback.LearningRateScheduler(lambda epoch: 0.99 ** epoch),\n",
        "            ],\n",
        "        )\n",
        "    \"\"\"\n",
        "    if neptune_switch == 1:\n",
        "        xgb.train(\n",
        "            params=params,\n",
        "            dtrain=dtrain,\n",
        "            num_boost_round=80,\n",
        "            evals=evals,\n",
        "            callbacks=[\n",
        "                npt_callback,\n",
        "                xgb.callback.LearningRateScheduler(lambda epoch: 0.99 ** epoch),\n",
        "            ],\n",
        "        )\n",
        "\n",
        "    # define\n",
        "    model = xgb.XGBRegressor(**params)\n",
        "\n",
        "    # fit\n",
        "    model.fit(\n",
        "        train_set['X'], train_set['y'],\n",
        "        eval_set=[(val_set['X'], val_set['y'])],\n",
        "        verbose=100,\n",
        "        early_stopping_rounds=20,\n",
        "    )\n",
        "\n",
        "    # save model\n",
        "    joblib.dump(model, f'{CFG.OUTPUT_DIR}/xgb_model_val.pkl')\n",
        "    logger.info('xgb model with early stopping saved!')\n",
        "\n",
        "    # feature importance\n",
        "    importance = model.get_booster().get_score(importance_type='gain')\n",
        "    importance = sorted(importance.items(), key=operator.itemgetter(1))\n",
        "    feature_importance_df = pd.DataFrame(importance, columns=['features', 'importance'])\n",
        "\n",
        "#575node2: 777 0.247 30m\n",
        "#576node2: kernel error on xgb; 50m27s 0.23717\n",
        "#579node2: 0.22942 1h17m50s; 0.22888 1h20m29s\n",
        "#580node2: 0.22865 1h22m19s; E 0.23414 1h1m42s\n",
        "#581node2: 0.23021 1h12m37s; 0.22808; 0.22189 2h2m12s;\n",
        "#583node2: 0.22855 2h21\n",
        "#584node2: added industry and sector from Yahoo, 0.22217 1h57m59s; 8h26m21s 0.22194; 8h48m40s\n",
        "#585node2 7h49m16s 0.22179"
      ],
      "id": "f02f9bb8"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2bffbe8d-6bc5-473b-8b80-07c555148ffd"
      },
      "source": [
        "## Analysis feature importance"
      ],
      "id": "2bffbe8d-6bc5-473b-8b80-07c555148ffd"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "9a608511-faf3-445f-8d5b-8c3f9c290119"
      },
      "outputs": [],
      "source": [
        "%%time\n",
        "if CFG.XGB == 1:\n",
        "    if CFG.ANALYSE == 1:\n",
        "        plot_tree(model)\n",
        "        plt.show()"
      ],
      "id": "9a608511-faf3-445f-8d5b-8c3f9c290119"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "b41ee49e-c78e-45fc-96c2-7e6836e13fe0"
      },
      "outputs": [],
      "source": [
        "#TODO: Use the feature_importance of the random_feature to remove the features scoring worse"
      ],
      "id": "b41ee49e-c78e-45fc-96c2-7e6836e13fe0"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "3707d635"
      },
      "outputs": [],
      "source": [
        "%%time\n",
        "if CFG.ANALYSE == 1:\n",
        "    if CFG.XGB == 1:\n",
        "        # feature importance XGB (pandas)\n",
        "        fig, ax = plt.subplots(1, 1, figsize=(48, 96))\n",
        "        sns.barplot(\n",
        "            x='importance',\n",
        "            y='features',\n",
        "            data=feature_importance_df.sort_values(by='importance', ascending=False)[1:250],\n",
        "            ax=ax\n",
        "        )\n",
        "\n",
        "#round563 node2: vol_Fvolz_volatility_38days\n",
        "#TODO: also have a file with names for poly_368, 246, 546, 82 etc.."
      ],
      "id": "3707d635"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "7a2a3e87-c605-4299-b89c-17b64f8ff506"
      },
      "outputs": [],
      "source": [
        "%%time\n",
        "if CFG.ANALYSE == 1:\n",
        "    if CFG.XGB == 1:\n",
        "        feature_importance_df.to_csv(f'../../Signals/Data/Analysis/feature_importances_xgb_{CFG.RUN}_{current_ds}.csv')\n",
        "#round322 node1: 141gb used\n",
        "#round359 TODO: craete importances for h2o model xgb&gbt"
      ],
      "id": "7a2a3e87-c605-4299-b89c-17b64f8ff506"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "042c217f-9475-4871-983e-134832be085b"
      },
      "outputs": [],
      "source": [
        "if CFG.ANALYSE == 1:\n",
        "    if CFG.XGB == 1:\n",
        "        feature_importance_df = feature_importance_df.sort_values(by='importance', ascending=False)\n",
        "        print(feature_importance_df.columns)\n",
        "#print(len(feature_df.columns))"
      ],
      "id": "042c217f-9475-4871-983e-134832be085b"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "0d3482f4-9784-41bf-8fa5-ca35a562deea"
      },
      "outputs": [],
      "source": [
        "%%time\n",
        "CFG.LEAN = 0\n",
        "if CFG.LEAN == 1:\n",
        "    filtered_features = feature_importance_df[feature_importance_df['importance'] < 0.8588]\n",
        "    nonrandom_features = feature_importance_df[feature_importance_df['importance'] >= 0.8588]\n",
        "    if CFG.LGBM == 1:\n",
        "        print('Configured Lean LGBM model')\n"
      ],
      "id": "0d3482f4-9784-41bf-8fa5-ca35a562deea"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "d0fff4f7-1dca-4959-8e77-cca5d4f32029"
      },
      "outputs": [],
      "source": [
        "%%time\n",
        "\n",
        "if CFG.LEAN == 0:\n",
        "  if CFG.ANALYSE == 1:\n",
        "    feature_df = expand_mem_usage(feature_df)\n",
        "    if CFG.TYPE == \"C\":\n",
        "      feature_df.to_parquet(f'/content/drive/MyDrive/Signals/Data/feature_df_{current_ds}.parquet')\n",
        "    if CFG.TYPE == \"K\":\n",
        "      feature_df.to_parquet(f'../../Signals/Data/feature_df_{current_ds}.parquet')\n",
        "#print(len(filtered_features))\n",
        "#print(len(nonrandom_features))\n",
        "# Filter the features with importance below 0.8588\n",
        "#print(nonrandom_features)\n",
        "#599node2: 49m17s"
      ],
      "id": "d0fff4f7-1dca-4959-8e77-cca5d4f32029"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "e4d8fc6a-9bb5-4e44-a6d9-9c4c03798564"
      },
      "outputs": [],
      "source": [
        "%%time\n",
        "if CFG.LEAN == 1:\n",
        "    print('Lean model has filtered features on importance arbitrary 0.8588')\n",
        "    features_to_drop = filtered_features['features'].tolist()\n",
        "    # List of feature names to drop from feature_df\n",
        "\n",
        "\n",
        "    # Drop the selected features from feature_df\n",
        "    #feature_df = feature_df.drop(columns=features_to_drop)\n",
        "    # Extracting the feature names from nonrandom_features\n",
        "    nonrandom_feature_names = nonrandom_features['features'].tolist()\n",
        "\n",
        "    # Ensure the target column 'target_20d' is kept as well as columns from drops like data_type\n",
        "    nonrandom_features.append('target_20d')\n",
        "\n",
        "    # Create a list of columns to retain in feature_df\n",
        "    cols_to_retain = list(set(nonrandom_features + drops))\n",
        "\n",
        "    # Update the feature_df to only include the columns in cols_to_retain\n",
        "    feature_df = feature_df[cols_to_retain]\n",
        "    features2 = [f for f in feature_df.columns.values.tolist() if f not in drops]\n",
        "if CFG.LGBM == 1:\n",
        "    # Retain only columns in feature_df that are in nonrandom_feature_names\n",
        "    print(\"LGBM feature columns: \",len(feature_df.columns)-4)\n",
        "    #TODO: remember/find out why it does -4"
      ],
      "id": "e4d8fc6a-9bb5-4e44-a6d9-9c4c03798564"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "9f2b09d3-4ed4-4f2d-b718-c76812063442"
      },
      "outputs": [],
      "source": [
        "#if CFG.LGBM == 1:\n",
        "    #print(len(feature_df.columns))\n",
        "#round562 node2: 248GB 3881 columns"
      ],
      "id": "9f2b09d3-4ed4-4f2d-b718-c76812063442"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6542db43-1ef2-4446-8c0c-c92e5dfd9095"
      },
      "source": [
        "## LGBM model with good features"
      ],
      "id": "6542db43-1ef2-4446-8c0c-c92e5dfd9095"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "a566ca37-f8d7-48fc-b8f0-5a5bf20c02c3"
      },
      "outputs": [],
      "source": [
        "if CFG.LGBM == 1:\n",
        "    print('LGBM Model code')\n"
      ],
      "id": "a566ca37-f8d7-48fc-b8f0-5a5bf20c02c3"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "3a22273d-8214-405d-86e8-761d490a48cd"
      },
      "outputs": [],
      "source": [
        "if CFG.LEAN == 1:\n",
        "    if CFG.LGBM == 1:\n",
        "        #feature_df = reduce_mem_usage(pd.read_parquet(f'../../Signals/Data/feature_df_nonrandom{current_ds}.parquet'))"
      ],
      "id": "3a22273d-8214-405d-86e8-761d490a48cd"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "9b6e9dee-4334-4a38-ae1e-73888010dc21"
      },
      "outputs": [],
      "source": [
        "%%time\n",
        "# If lean create run with only features 2, non-random variables\n",
        "if CFG.LEAN == 1:\n",
        "    del train_set, val_df, val_set\n",
        "    if CFG.LGBM == 1:\n",
        "        # train-valid split\n",
        "        train_set2 = {\n",
        "            'X': feature_df.query('data_type == \"train\"')[features2],\n",
        "            'y': feature_df.query('data_type == \"train\"')[target].astype(np.float64)\n",
        "        }\n",
        "        val_df2 = feature_df.query('data_type == \"validation\"').dropna().copy()\n",
        "        val_set2 = {\n",
        "            'X': val_df2[features2],\n",
        "            'y': val_df2[target].astype(np.float64)\n",
        "        }\n",
        "\n",
        "        assert train_set2['y'].isna().sum() == 0\n",
        "        assert val_set2['y'].isna().sum() == 0\n",
        "#round559 node2: 4min20s\n",
        "#569node2: 3m47s\n",
        "#571node2: 3m59s"
      ],
      "id": "9b6e9dee-4334-4a38-ae1e-73888010dc21"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "167b1c1a-645a-4bcb-a1f1-2057d020f1ab"
      },
      "outputs": [],
      "source": [
        "%%time\n",
        "if CFG.LEAN == 1:\n",
        "    train_set = train_set2\n",
        "    val_set = val_set2\n",
        "if CFG.LGBM == 1:\n",
        "    # Adapting parameters for LGBM\n",
        "    params = {\n",
        "        'boosting_type': 'gbdt',\n",
        "        'objective': 'regression',\n",
        "        'metric': 'rmse',\n",
        "        'learning_rate': 0.01,\n",
        "        'num_leaves': 30,  # default value for LGBM\n",
        "        'colsample_bytree': 0.1,\n",
        "        'max_depth': CFG.XGB_DEPTH,\n",
        "        'lambda_l1': 0.90,\n",
        "        'lambda_l2': 0.10,\n",
        "        'seed': 46,\n",
        "    }\n",
        "\n",
        "    # LightGBM datasets\n",
        "    train_data = lgb.Dataset(train_set['X'], label=train_set['y'])\n",
        "    val_data = lgb.Dataset(val_set['X'], label=val_set['y'])\n",
        "\n",
        "    if neptune_switch == 1:\n",
        "        run[\"parameters\"] = params\n",
        "\n",
        "    # LightGBM training\n",
        "    if neptune_switch == 1:\n",
        "        lgb.train(\n",
        "            params=params,\n",
        "            train_set=train_data,\n",
        "            num_boost_round=180,\n",
        "            valid_sets=[train_data, val_data],\n",
        "            callbacks=[\n",
        "                npt_callback,  # Assuming Neptune callback works similarly with LGBM\n",
        "                lgb.reset_parameter(learning_rate=lambda epoch: 0.99 ** epoch),\n",
        "            ],\n",
        "            #early_stopping_rounds=20,\n",
        "        )\n",
        "\n",
        "    # scikit-learn API\n",
        "    model_lgbm = lgb.LGBMRegressor(**params)\n",
        "    model_lgbm.fit(\n",
        "        train_set['X'], train_set['y'],\n",
        "        eval_set=[(val_set['X'], val_set['y'])],\n",
        "        callbacks=[\n",
        "            lgb.reset_parameter(learning_rate=lambda epoch: 0.99 ** epoch),\n",
        "            lgb.early_stopping(20, first_metric_only=False, verbose=True, min_delta=0.0)\n",
        "        ]\n",
        "    )\n",
        "\n",
        "    # save model\n",
        "    joblib.dump(model_lgbm, f'{CFG.OUTPUT_DIR}/lgbm_model_val_{current_ds}_{CFG.RUN}.pkl')\n",
        "    logger.info('lgbm model with early stopping saved!')\n",
        "\n",
        "    # feature importance\n",
        "    feature_importance_df2 = pd.DataFrame({\n",
        "        'features': train_set['X'].columns,\n",
        "        'importance': model_lgbm.feature_importances_\n",
        "    }).sort_values(by=\"importance\", ascending=False)\n",
        "    #round559 first run: rmse: 0.235248 (num_leaves=31); rmse 0.234916 (num_leaves=6) 6min31s; rmse 0.235092 (num_leaves=15)\n",
        "    #round563 0.33\n",
        "#round565 node2: 0.24277 3m24s\n",
        "#round567 node2: 0.23557 6m24s\n",
        "#568node2: 0.234011 6m31s\n",
        "#569node2: 0.234951 5m58s\n",
        "#571node2: 0.22806 10m1s\n",
        "#586node2: 0.221925 14m48s; test L1 0.1->0.9; ; 0.221821 14m43s; Kall 10y 0.210213 21m32s\n",
        "#TODO: hyperopt hyperparameter tuning https://www.kaggle.com/code/donkeys/lgbm-with-hyperopt-tuning\n",
        "#589node2: 1784 0.236341 7m44s\n",
        "#590node2: 0.228992; K5385 0.2287 10m5s; K5395 0.228728 9m49s; K5405 0.228595 10m23s\n",
        "#0.227749 10m28s; K5420 0.22165 13m19s\n",
        "#593node2: 0.221962 14m33s\n",
        "#594node2: 0.219872 16m16s; K5489 3697 979868 bins 3866 features  0.213393 21m85s\n",
        "#599node2: 0.219892 14m59s"
      ],
      "id": "167b1c1a-645a-4bcb-a1f1-2057d020f1ab"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "6d874c46-389a-417b-836e-23427cbd753f"
      },
      "outputs": [],
      "source": [
        "if CFG.ANALYSE == 2:\n",
        "    !pip install hyperopt\n",
        "    import hyperopt\n",
        "    from hyperopt import hp, tpe, Trials\n",
        "    from hyperopt.fmin import fmin\n",
        "    # max number of trials hyperopt runs\n",
        "    n_trials = 500\n",
        "    #verbosity in LGBM is how often progress is printed. with 100=print progress every 100 rounds. 0 is quite?\n",
        "    verbosity = 0\n",
        "    #if true, print summary accuracy/loss after each round\n",
        "    print_summary = False\n",
        "\n",
        "    from sklearn.metrics import accuracy_score, log_loss\n",
        "\n",
        "    all_accuracies = []\n",
        "    all_losses = []\n",
        "    all_params = []\n",
        "    def optimize_lgbm(n_classes, max_n_search=None):\n",
        "        # https://github.com/Microsoft/LightGBM/blob/master/docs/Parameters.rst\n",
        "        # https://indico.cern.ch/event/617754/contributions/2590694/attachments/1459648/2254154/catboost_for_CMS.pdf\n",
        "        space = {\n",
        "            #this is just piling on most of the possible parameter values for LGBM\n",
        "            #some of them apparently don't make sense together, but works for now.. :)\n",
        "            'class_weight': hp.choice('class_weight', [None, 'balanced']),\n",
        "            'boosting_type': hp.choice('boosting_type',\n",
        "                                       [{'boosting_type': 'gbdt',\n",
        "    #                                     'subsample': hp.uniform('dart_subsample', 0.5, 1)\n",
        "                                         },\n",
        "                                        {'boosting_type': 'dart',\n",
        "    #                                     'subsample': hp.uniform('dart_subsample', 0.5, 1)\n",
        "                                         },\n",
        "                                        {'boosting_type': 'goss'}]),\n",
        "            'num_leaves': hp.quniform('num_leaves', 30, 150, 1),\n",
        "            'learning_rate': hp.loguniform('learning_rate', np.log(0.01), np.log(0.2)),\n",
        "            'subsample_for_bin': hp.quniform('subsample_for_bin', 20000, 300000, 20000),\n",
        "            'feature_fraction': hp.uniform('feature_fraction', 0.5, 1),\n",
        "            'bagging_fraction': hp.uniform('bagging_fraction', 0.5, 1), #alias \"subsample\"\n",
        "            'min_data_in_leaf': hp.qloguniform('min_data_in_leaf', 0, 6, 1),\n",
        "            'lambda_l1': hp.choice('lambda_l1', [0, hp.loguniform('lambda_l1_positive', -16, 2)]),\n",
        "            'lambda_l2': hp.choice('lambda_l2', [0, hp.loguniform('lambda_l2_positive', -16, 2)]),\n",
        "            'verbose': -1,\n",
        "            #the LGBM parameters docs list various aliases, and the LGBM implementation seems to complain about\n",
        "            #the following not being used due to other params, so trying to silence the complaints by setting to None\n",
        "            'subsample': None, #overridden by bagging_fraction\n",
        "            'reg_alpha': None, #overridden by lambda_l1\n",
        "            'reg_lambda': None, #overridden by lambda_l2\n",
        "            'min_sum_hessian_in_leaf': None, #overrides min_child_weight\n",
        "            'min_child_samples': None, #overridden by min_data_in_leaf\n",
        "            'colsample_bytree': None, #overridden by feature_fraction\n",
        "    #        'min_child_samples': hp.quniform('min_child_samples', 20, 500, 5),\n",
        "            'min_child_weight': hp.loguniform('min_child_weight', -16, 5), #also aliases to min_sum_hessian\n",
        "    #        'reg_alpha': hp.uniform('reg_alpha', 0.0, 1.0),\n",
        "    #        'reg_lambda': hp.uniform('reg_lambda', 0.0, 1.0),\n",
        "    #        'colsample_bytree': hp.uniform('colsample_by_tree', 0.6, 1.0),\n",
        "        }\n",
        "        if n_classes > 2:\n",
        "            space['objective'] = \"multiclass\"\n",
        "            space[\"num_class\"] = n_classes\n",
        "        else:\n",
        "            space['objective'] = \"binary\"\n",
        "            #space[\"num_class\"] = 1\n",
        "\n",
        "        global max_n\n",
        "        max_n = max_n_search\n",
        "        trials = Trials()\n",
        "        best = fmin(fn=objective_sklearn,\n",
        "                    space=space,\n",
        "                    algo=tpe.suggest,\n",
        "                    max_evals=n_trials,\n",
        "                    trials=trials,\n",
        "                   verbose= 1)\n",
        "\n",
        "        # find the trial with lowest loss value. this is what we consider the best one\n",
        "        idx = np.argmin(trials.losses())\n",
        "        print(idx)\n",
        "\n",
        "        print(trials.trials[idx])\n",
        "\n",
        "        # these should be the training parameters to use to achieve the best score in best trial\n",
        "        params = trials.trials[idx][\"result\"][\"params\"]\n",
        "        max_n = None\n",
        "\n",
        "        print(params)\n",
        "        return params\n",
        "\n",
        "    # run a search for binary classification\n",
        "    def classify_binary(X_cols, df_train, df_test, y_param):\n",
        "        global X\n",
        "        global y\n",
        "        y = y_param\n",
        "        nrows = max_n\n",
        "\n",
        "        X = df_train[X_cols]\n",
        "        X_test = df_test[X_cols]\n",
        "\n",
        "        # use 2 classes as this is a binary classification\n",
        "        # the second param is the number of rows to use for training\n",
        "        params = optimize_lgbm(2, 5000)\n",
        "        print(params)\n",
        "\n",
        "        clf = lgbm.LGBMClassifier(**params)\n",
        "\n",
        "        fit_params = create_fit_params(params)\n",
        "\n",
        "        search_results = stratified_test_prediction_avg_vote(clf, X, X_test, y, use_eval_set=True,\n",
        "                                                             n_folds=n_folds, n_classes=2, fit_params=fit_params, verbosity=verbosity)\n",
        "        predictions = search_results[\"predictions\"]\n",
        "        oof_predictions = search_results[\"oof_predictions\"]\n",
        "        avg_accuracy = search_results[\"avg_accuracy\"]\n",
        "        misclassified_indices = search_results[\"misclassified_indices\"]\n",
        "        misclassified_samples_expected = search_results[\"misclassified_samples_expected\"]\n",
        "        misclassified_samples_actual = search_results[\"misclassified_samples_actual\"]\n",
        "\n",
        "        return predictions, oof_predictions, avg_accuracy, misclassified_indices"
      ],
      "id": "6d874c46-389a-417b-836e-23427cbd753f"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "268f3a78-8676-44ed-ad07-270e2a75752d"
      },
      "outputs": [],
      "source": [
        "print(feature_importance_df2.sort_values(by='importance', ascending=False)[1:50])"
      ],
      "id": "268f3a78-8676-44ed-ad07-270e2a75752d"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "51650b20-75f8-4cce-98ed-ea5bc5447fe5"
      },
      "outputs": [],
      "source": [
        "%%time\n",
        "if CFG.ANALYSE == 1:\n",
        "    if CFG.LGBM == 1:\n",
        "        # feature importance LGBM (pandas)\n",
        "        fig, ax = plt.subplots(1, 1, figsize=(48, 28))\n",
        "        sns.barplot(\n",
        "            x='importance',\n",
        "            y='features',\n",
        "            data=feature_importance_df2.sort_values(by='importance', ascending=False)[1:50],\n",
        "            ax=ax\n",
        "        )\n",
        "#round563: vol_Mcapz_return_300days, vol_Mcapz_return_42days\n",
        "#586node2: poly_366, close_MA_gap_270days, RSI120, poly_364, close_MA_gap_60days, vol_Mcapz_return_abs_31days, poly_118, vol_FVOL_LRET_volatility_14days,\n",
        "#vol_Fvolz_volatility_170days, vol_Fvolz_volatility_300days, poly_103, poly_209, poly_949, CountryDomicile_ISO2, poly_1046, fvol_MACD_signal, poly_144, fvol_EMA20,\n",
        "#vol_FVOL_LRET_volatility_130days, vol_FVOL_LRET_volatility_53days, vol_VOL_LRET_volatility_114days\n",
        "#599node2: poly_366,close_MA_gap_270days,vol_Mcapz_MA_gap_200days,poly_209,vol_Volumez_volatility_111days,poly_951,vol_Mcapz_return_abs_160days,vol_Volumez_volatility_10days,RSI120,close_volatility_150days,vol_Mcapz_volatility_300days,vol_FVOL_LRET_volatility_102days,poly_471,close_volatility_130days,poly_1163,poly_23,poly_99,poly_880,vol_Volumez_volatility_31days,vol_Volumez_volatility_200days,fvol_EMA20,close_MA_gap_60days           1"
      ],
      "id": "51650b20-75f8-4cce-98ed-ea5bc5447fe5"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "43aa35a4-cd7d-42db-95f1-6870a0288256"
      },
      "outputs": [],
      "source": [
        "%%time\n",
        "\"\"\"\n",
        "if CFG.H2O == 1:\n",
        "    import pyspark\n",
        "    from pyspark import SparkConf, SparkContext\n",
        "    from pyspark.sql import SparkSession, SQLContext\n",
        "    #spark.stop()\n",
        "    spark = SparkSession.builder.master(\"local[2]\").appName(\"EHSOFT\").getOrCreate()\n",
        "    sc = spark.sparkContext\n",
        "    sc\n",
        "    import h2o\n",
        "    from pyspark import SparkConf\n",
        "    import os\n",
        "\n",
        "    from pyspark.ml import Pipeline, PipelineModel\n",
        "    from pyspark.ml.feature import HashingTF, RegexTokenizer, StopWordsRemover, IDF\n",
        "    from pyspark.sql import SparkSession\n",
        "    from pysparkling import *\n",
        "    print(h2o.__version__)\n",
        "    from h2o.automl import H2OAutoML\n",
        "    h2o.init()\n",
        "TODO: fix --> 226 raise H2OStartupError(\"Cannot start local server: h2o.jar not found. Paths searched:\\n\" +\n",
        "    227                       \"\".join(\"    %s\\n\" % s for s in searched_paths))\n",
        "\n",
        "H2OStartupError: Cannot start local server: h2o.jar not found. Paths searched:\n",
        "    /home/knight2/miniconda3/lib/python3.11/site-packages/h2o/backend/bin/h2o.jar\n",
        "    /home/knight2/miniconda3/h2o_jar/h2o.jar\n",
        "    /usr/local/h2o_jar/h2o.jar\n",
        "    /home/knight2/miniconda3/local/h2o_jar/h2o.jar\n",
        "    /home/knight2/.local/h2o_jar/h2o.jar\n",
        "    /home/knight2/miniconda3/h2o_jar/h2o.jar\n",
        "\"\"\""
      ],
      "id": "43aa35a4-cd7d-42db-95f1-6870a0288256"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ab5e1764-057a-406b-9d7c-5bd90c02a1f7"
      },
      "outputs": [],
      "source": [
        "%%time\n",
        "if CFG.H2O == 1:\n",
        "    feature_train = feature_df.query('data_type == \"train\"')\n",
        "    val_df_h2o = feature_df.query('data_type == \"validation\"').dropna().copy()\n",
        "    #!$SPARK_HOME/sparkling/bin/pysparkling\n",
        "    #Import TRAINING base to the H20 context\n",
        "    train_h2o = h2o.H2OFrame(feature_train)\n",
        "    val_sub_h2o = h2o.H2OFrame(val_df_h2o[drops].copy())\n",
        "    val_set_h2o = h2o.H2OFrame(val_df_h2o)\n",
        "    val_df_h2o = val_df_h2o.reset_index()\n",
        "    if CFG.ANALYSE == 1:\n",
        "        print(len(train_h2o))\n",
        "        print(len(val_df_h2o))\n",
        "        print(val_df_h2o.tail())\n",
        "        print(len(val_sub_h2o))\n",
        "        print(len(val_set_h2o))\n",
        "    # Execution time of the model start\n",
        "    starth2o = datetime.now()\n",
        "    #Define metrics to select the best model in AutoML\n",
        "    sort_metric = 'RMSE'\n",
        "    max_runtime_secs = CFG.H2O_RUNTIME\n",
        "\n",
        "    AUTOML = H2OAutoML(seed=1,\n",
        "                       include_algos = CFG.H2O_ALGOS,\n",
        "                       max_runtime_secs = max_runtime_secs,\n",
        "                       stopping_metric = sort_metric,\n",
        "                       sort_metric = sort_metric)\n",
        "    AUTOML.train(x=features, y=target, training_frame = train_h2o, validation_frame = val_set_h2o, leaderboard_frame=val_set_h2o) #leaderboard should be test from https://www.kaggle.com/code/maxreis/house-price-submission-with-h2o\n",
        "\n",
        "    #View the AutoML Leaderboard\n",
        "    lb = AUTOML.leaderboard\n",
        "    print(lb.head(rows=lb.nrows))\n",
        "    stoph2o = datetime.now()\n",
        "    execution_time_h2o = stoph2o-starth2o\n",
        "    execution_time_nb = stoph2o-startnb\n",
        "    print(\"\\n\"+ \"Execution time h2o: \" + str(execution_time_h2o) + \"\\n\")\n",
        "    print(\"\\n\"+ \"Execution time nb: \" + str(execution_time_nb) + \"\\n\")"
      ],
      "id": "ab5e1764-057a-406b-9d7c-5bd90c02a1f7"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "b8d7e400"
      },
      "source": [
        "# Validation Score\n",
        "The following snipets are derived from\n",
        "\n",
        "https://colab.research.google.com/drive/1ECh69C0LDCUnuyvEmNFZ51l_276nkQqo#scrollTo=tTBUzPep2dm3\n",
        "\n",
        "Let's see how good our model predictions on the validation data are.\n"
      ],
      "id": "b8d7e400"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "f9034929-17a6-4d5e-b1a2-0c1aece0b9c7"
      },
      "outputs": [],
      "source": [
        "%%time\n",
        "# XGB model\n",
        "if CFG.XGB == 1:\n",
        "    valid_sub = val_df[drops].copy()\n",
        "    valid_sub['prediction'] = model.predict(val_set['X'])\n",
        "\n",
        "#581node2 21m7s\n",
        "#585node2: 2h43m44s"
      ],
      "id": "f9034929-17a6-4d5e-b1a2-0c1aece0b9c7"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "f65be56e-5a46-40f9-9d49-811ef4fa2127"
      },
      "outputs": [],
      "source": [
        "%%time\n",
        "if CFG.LGBM == 1:\n",
        "    # LGB model\n",
        "    valid_sub_lgbm = val_df[drops].copy()\n",
        "    valid_sub_lgbm['prediction'] = model_lgbm.predict(val_set['X'])\n",
        "\n",
        "    # Compute score\n",
        "    valid_sub_lgbm['friday_date'] = valid_sub_lgbm['friday_date'].astype(str)\n",
        "\n",
        "#round559 node2: hitrate 98.34% sharpe 1.3075 rmse 0.2338\n",
        "#round563 node2: hr 60.75 sh 0.1994 rmse 0.3225\n",
        "#round564 node2: hr 95.03 sh 1.5331 rmse 0.2412\n",
        "#round565 node2: hr 89.52 sh 1.1080 rmse 0.2409\n",
        "#round567 node2: hr 97.61 sh 1.4968 rmse 0.2342\n",
        "#569node2: hr 97.79 sh 1.3884 rmse 0.2338\n",
        "#571node2: hr 98.16 sh 1.1433 rmse 0.2272 3m29s\n",
        "#586node2: without scoring 4m46s; with scoring 8m17s\n",
        "#590node2: K5385 3m8s; K5405 3m7s; K5415 3m24s;\n",
        "#594node2: K5489\n",
        "#599node2: K5480 5m7s"
      ],
      "id": "f65be56e-5a46-40f9-9d49-811ef4fa2127"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8e13f87d-a71f-4b7a-9f55-1416daccbf56"
      },
      "outputs": [],
      "source": [
        "%%time\n",
        "if CFG.H2O == 1:\n",
        "    #h2o predict & hit rate\n",
        "    # prediction for the validation set\n",
        "    valid_pred_h2o_df = AUTOML.predict(val_set_h2o)\n",
        "    # transfer pred to pandas dataframe\n",
        "    valid_pred_h2o = valid_pred_h2o_df['predict'].as_data_frame()\n",
        "    valid_sub_h2o = val_df_h2o[drops].copy()\n",
        "    valid_sub_h2o['prediction'] = valid_pred_h2o\n",
        "\n",
        "    # compute score\n",
        "    val_era_scores_h2o = valid_sub_h2o.copy()\n",
        "    val_era_scores_h2o['friday_date'] = val_era_scores_h2o['friday_date'].astype(str)\n",
        "    val_era_scores_h2o = val_era_scores_h2o.loc[val_era_scores_h2o['prediction'].isna() == False].groupby(['friday_date']).apply(score)\n",
        "    run_analytics(val_era_scores_h2o)\n",
        "\n"
      ],
      "id": "8e13f87d-a71f-4b7a-9f55-1416daccbf56"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "2f0d5b4c"
      },
      "outputs": [],
      "source": [
        "%%time\n",
        "if CFG.H2O == 1:\n",
        "    napi = numerapi.NumerAPI(verbosity=\"info\")\n",
        "\n",
        "    # write to file for later use\n",
        "    #valid_sub.to_csv(f\"../Data/Analysis/Signals/analysis_{CFG.TYPE}_{CFG.RUN}_{current_ds}_{CFG.XGB_DEPTH}.csv\")\n",
        "    #valid_sub_h2o.to_csv(f\"../Data/Analysis/Signals/analysis_h2o_{CFG.TYPE}_{CFG.RUN}_{current_ds}_{CFG.H2O_RUNTIME}.csv\")\n",
        "\n",
        "    #stop neptune.ai run\n",
        "    if neptune_switch == 1:\n",
        "        run.stop()\n",
        "    #round359 node3: TODO ensemble and validate\n"
      ],
      "id": "2f0d5b4c"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "a4e0208f"
      },
      "source": [
        "# Submission\n",
        "Let's use this trained model to make a submission for the Numerai Signals.\n",
        "\n",
        "Note that, again, yfinance data is not complete. Sometimes there is no recent data available for many tickers;(\n",
        "\n",
        "We need at least 5 tickers for a successful submission. Let's first check if we have at least 5 tickers in which the recent friday_date for them is indeed the recent friday date."
      ],
      "id": "a4e0208f"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "81b3201f"
      },
      "source": [
        "perform the inference on those tickers and submit!"
      ],
      "id": "81b3201f"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "1465a2c2"
      },
      "outputs": [],
      "source": [
        "%%time\n",
        "# live submission\n",
        "feature_df.loc[feature_df['friday_date'] == recent_friday, 'data_type'] = 'live'\n",
        "\n",
        "if CFG.XGB == 1:\n",
        "    test_sub = feature_df.query('data_type == \"live\"')[drops].copy()\n",
        "    test_pred_data = feature_df.query('data_type == \"live\"')[features]\n",
        "    test_sub['prediction'] = model.predict(test_pred_data)\n",
        "    if CFG.ANALYSE == 1:\n",
        "        print(\"test length:\",len(test_sub))\n",
        "    logger.info(test_sub.shape)\n",
        "#571node2: 8.06s\n",
        "#581node2: 1m1s\n",
        "#584node2: 3m6s\n",
        "#585node2: 23.5s"
      ],
      "id": "1465a2c2"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "b27293dc-0a4c-4074-8dd4-e67baa18d985"
      },
      "outputs": [],
      "source": [
        "%%time\n",
        "if CFG.LGBM == 1:\n",
        "    # LGBM live sub\n",
        "    feature_df.loc[feature_df['friday_date'] == recent_friday, 'data_type'] = 'live'\n",
        "    test_sub_lgbm = feature_df.query('data_type == \"live\"')[drops].copy()\n",
        "    test_pred_data_lgbm = feature_df.query('data_type == \"live\"')[features]\n",
        "    test_sub_lgbm['prediction'] = model_lgbm.predict(test_pred_data_lgbm)\n",
        "    if CFG.ANALYSE == 1:\n",
        "        print(\"test length:\",len(test_sub_lgbm))\n",
        "    print(\"test length:\",len(test_sub_lgbm))\n",
        "    logger.info(test_sub_lgbm.shape)\n",
        "#571node2: 4.6s\n",
        "#586node2: 6.66s; Kall 10y 13.8s\n",
        "#590node2: K5385 8.41s; K5415 4.84s\n",
        "#594node2: K5479 9.99s; K5489 13.9s\n",
        "#599node2: K5480 11.9s"
      ],
      "id": "b27293dc-0a4c-4074-8dd4-e67baa18d985"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "c7e0bd95-175b-4fa7-a969-ae00333c0cd4"
      },
      "outputs": [],
      "source": [
        "# To submit, you need to have Numerai account and have API's id and secret key. Also you need to have at least one (numerai signals') model slot.\n",
        "public_id = '<Your Numerai API ID>'\n",
        "secret_key = '<Your Numerai Secret Key>'\n",
        "slot_name = '<Your Numerai Signals Submission Slot Name>'"
      ],
      "id": "c7e0bd95-175b-4fa7-a969-ae00333c0cd4"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "78ea6ea1"
      },
      "outputs": [],
      "source": [
        "%%time\n",
        "### XGB live prediction and merge with validation for submission\n",
        "from datetime import datetime\n",
        "if CFG.XGB == 1:\n",
        "    #today = datetime.now().strftime('%Y-%m-%d')\n",
        "    today = '202310'\n",
        "\n",
        "    napi = numerapi.NumerAPI(verbosity=\"info\")\n",
        "\n",
        "    print('Current round: ',current_ds)\n",
        "    # recent friday date?\n",
        "    recent_friday = datetime.now() + relativedelta(weekday=FR(-1))\n",
        "    recent_friday = int(recent_friday.strftime('%Y%m%d'))\n",
        "    print(f'Most recent Friday: {recent_friday}')\n",
        "    recent_friday2 = datetime.now() + relativedelta(weekday=FR(-2))\n",
        "    recent_friday2 = int(recent_friday2.strftime('%Y%m%d'))\n",
        "    print(f'Second most recent Friday: {recent_friday2}')\n",
        "    #print(\"prediction length:\",len(pred))\n",
        "    test_sub['prediction'].hist(bins=100)\n",
        "    def submit_signal(sub: pd.DataFrame, public_id: str, secret_key: str, slot_name: str):\n",
        "        \"\"\"\n",
        "        submit numerai signals prediction\n",
        "        \"\"\"\n",
        "        # setup private API\n",
        "        napi = numerapi.SignalsAPI(public_id, secret_key)\n",
        "\n",
        "        # write predictions to csv\n",
        "        model_id = napi.get_models()[f'{slot_name}']\n",
        "        filename = f\"sub_{model_id}.csv\"\n",
        "        sub.to_csv(filename, index=False)\n",
        "\n",
        "        # submit\n",
        "        submission = napi.upload_predictions(filename, model_id=model_id)\n",
        "        print(f'Submitted : {slot_name}!')\n",
        "    print(\"validation length:\",len(valid_sub))\n",
        "    # concat valid and test\n",
        "    sub = pd.concat([valid_sub, test_sub], ignore_index=True)\n",
        "\n",
        "    # rename to 'signal'\n",
        "    sub.rename(columns={'prediction': 'signal'}, inplace=True)\n",
        "\n",
        "    # select necessary columns\n",
        "    sub = sub[['ticker', 'friday_date', 'data_type', 'signal']]\n",
        "\n",
        "    submission_length_preDedup = len(sub)\n",
        "    # remove duplicates based on ticker and friday_date\n",
        "    sub = sub.drop_duplicates(subset=['ticker', 'friday_date'])\n",
        "    submission_length_postDedup = len(sub)\n",
        "    if CFG.ANALYSE == 1:\n",
        "        print(\"duplicate rows removed = \", submission_length_preDedup - submission_length_postDedup)\n",
        "\n",
        "        # submit_signal(sub, public_id, secret_key, slot_name) # uncomment if you submit\n",
        "        print(\"submission length:\",len(sub))\n",
        "#571node2: 6.57s\n",
        "#581node2: 2.81s\n",
        "#584node2: 2.06s"
      ],
      "id": "78ea6ea1"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "6a72058f-bbe5-4b5f-b106-ea3a9870eff0"
      },
      "outputs": [],
      "source": [
        "%%time\n",
        "if CFG.XGB == 1:\n",
        "    # save\n",
        "    sub.to_csv(f'../../Signals/Submission/{current_ds}_submission_xgb_{CFG.TYPE}_{CFG.RUN}_{CFG.START}_{CFG.END}_{today}.csv', index=False)\n",
        "#571node2: 2.1s\n",
        "#584node2: 20.3s\n",
        "#585node2: 22.4s"
      ],
      "id": "6a72058f-bbe5-4b5f-b106-ea3a9870eff0"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "791a516f-ad55-4741-bec3-a4e4acc3e1da"
      },
      "outputs": [],
      "source": [
        "%%time\n",
        "if CFG.LGBM == 1:\n",
        "    ### LGBM submission\n",
        "\n",
        "    # concat valid and test\n",
        "    sub_lgbm = pd.concat([valid_sub_lgbm, test_sub_lgbm], ignore_index=True)\n",
        "\n",
        "    # rename to 'signal'\n",
        "    sub_lgbm.rename(columns={'prediction': 'signal'}, inplace=True)\n",
        "\n",
        "    # select necessary columns\n",
        "    sub_lgbm = sub_lgbm[['ticker', 'friday_date', 'data_type', 'signal']]\n",
        "\n",
        "    submission_length_preDedup = len(sub_lgbm)\n",
        "    # remove duplicates based on ticker and friday_date\n",
        "    sub_lgbm = sub_lgbm.drop_duplicates(subset=['ticker', 'friday_date'])\n",
        "    submission_length_postDedup = len(sub_lgbm)\n",
        "    if CFG.ANALYSE == 1:\n",
        "        test_sub_lgbm['prediction'].hist(bins=100)\n",
        "\n",
        "        print(\"validation length:\",len(valid_sub_lgbm))\n",
        "        print(\"duplicate rows removed = \", submission_length_preDedup - submission_length_postDedup)\n",
        "\n",
        "        # submit_signal(sub, public_id, secret_key, slot_name) # uncomment if you submit\n",
        "        print(\"submission length:\",len(sub_lgbm))\n",
        "#571node2: 1.22s\n",
        "#590node2: K5385 2.95s; K5415 1.74s\n",
        "#599node2: K5480 3.57s"
      ],
      "id": "791a516f-ad55-4741-bec3-a4e4acc3e1da"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "7464de36-4df2-4426-af00-2e865b121cc8"
      },
      "outputs": [],
      "source": [
        "%%time\n",
        "if CFG.LGBM == 1:\n",
        "    # save\n",
        "    sub_lgbm.to_csv(f'../../Signals/Submission/{current_ds}_submission_lgbm_{CFG.TYPE}_{CFG.RUN}_{CFG.START}_{CFG.END}_{today}.csv', index=False)\n",
        "#571node2: 2.28s\n",
        "#590node2: 2.33s; 2.49s\n",
        "#599node2: 14.9s"
      ],
      "id": "7464de36-4df2-4426-af00-2e865b121cc8"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "909f0cf6-2223-4276-8e96-203fb7ae880f"
      },
      "outputs": [],
      "source": [
        "%%time\n",
        "CFG.ANALYSE = 1\n",
        "if CFG.ANALYSE == 1:\n",
        "    # https://colab.research.google.com/drive/1ECh69C0LDCUnuyvEmNFZ51l_276nkQqo#scrollTo=tTBUzPep2dm3\n",
        "    def score(df, target_name=target, pred_name='prediction'):\n",
        "        '''Takes df and calculates spearman correlation and RMSE from pre-defined cols'''\n",
        "        spearman_corr = np.corrcoef(\n",
        "            df[target_name],\n",
        "            df[pred_name].rank(pct=True, method=\"first\")\n",
        "        )[0,1]\n",
        "\n",
        "        rmse = np.sqrt(((df[target_name] - df[pred_name]) ** 2).mean())\n",
        "\n",
        "        return spearman_corr, rmse\n",
        "\n",
        "    def run_analytics(era_scores, era_rmse):\n",
        "        print(f\"Mean Correlation: {era_scores.mean():.4f}\")\n",
        "        print(f\"Median Correlation: {era_scores.median():.4f}\")\n",
        "        print(f\"Standard Deviation: {era_scores.std():.4f}\")\n",
        "        print(f\"Mean RMSE: {era_rmse.mean():.4f}\")\n",
        "        print('\\n')\n",
        "        print(f\"Mean Pseudo-Sharpe: {era_scores.mean()/era_scores.std():.4f}\")\n",
        "        print(f\"Median Pseudo-Sharpe: {era_scores.median()/era_scores.std():.4f}\")\n",
        "        print('\\n')\n",
        "        print(f'Hit Rate (% positive eras): {era_scores.apply(lambda x: np.sign(x)).value_counts()[1]/len(era_scores):.2%}')\n",
        "\n",
        "        era_scores.rolling(20).mean().plot(kind='line', title='Rolling Per Era Correlation Mean', figsize=(15,4))\n",
        "        plt.axhline(y=0.0, color=\"r\", linestyle=\"--\"); plt.show()\n",
        "\n",
        "        era_scores.cumsum().plot(title='Cumulative Sum of Era Scores', figsize=(15,4))\n",
        "        plt.axhline(y=0.0, color=\"r\", linestyle=\"--\"); plt.show()\n",
        "        if neptune_switch == 1:\n",
        "            run[\"eval/Mean_Correlation\"] = era_scores.mean()\n",
        "            run[\"eval/Median Correlation\"] = era_scores.median()\n",
        "            run[\"eval/Standard Deviation\"] = era_scores.std()\n",
        "            run[\"eval/Mean Pseudo-Sharpe\"] = era_scores.mean()/era_scores.std()\n",
        "            run[\"eval/Median Pseudo-Sharpe\"] = era_scores.mean()/era_scores.std()\n",
        "            run[\"eval/Hit Rate (% positive eras)\"] = era_scores.apply(lambda x: np.sign(x)).value_counts()[1]/len(era_scores)\n",
        "    # Compute score\n",
        "    if CFG.ANALYSE == 1:\n",
        "        if CFG.XGB == 1:\n",
        "            valid_sub['friday_date'] = valid_sub['friday_date'].astype(str)\n",
        "\n",
        "            results = valid_sub.groupby(['friday_date']).apply(score).apply(pd.Series).rename(columns={0: 'Score', 1: 'RMSE'})\n",
        "            val_era_scores = results['Score']\n",
        "            val_era_rmse = results['RMSE']\n",
        "\n",
        "            #if CFG.ANALYSE == 1:\n",
        "            run_analytics(val_era_scores, val_era_rmse)\n",
        "        if CFG.LGBM == 1:\n",
        "            valid_sub_lgbm['friday_date'] = valid_sub_lgbm['friday_date'].astype(str)\n",
        "\n",
        "            results = valid_sub_lgbm.groupby(['friday_date']).apply(score).apply(pd.Series).rename(columns={0: 'Score', 1: 'RMSE'})\n",
        "            val_era_scores_lgbm = results['Score']\n",
        "            val_era_rmse_lgbm = results['RMSE']\n",
        "\n",
        "            #if CFG.ANALYSE == 1:\n",
        "            run_analytics(val_era_scores_lgbm, val_era_rmse_lgbm)\n",
        "#573node2: hr 99.08 sh 1.4398 rmse 0.2212 22m10s; EDSdata hr 98.71 sh 2.0984 rmse 0.2323 hr 98.71 sh 2.1066 rmse 0.2312\n",
        "#578node2: 13m28s\n",
        "#579node2: 21m18s; 19m hr 97.61 sh 1.1306 rmse 0.2283; hr 97.61 sh 1.1291 rmse 0.2278\n",
        "#580node2: 19m37s hr 97.61 sh 1.1577 rmse 0.2276\n",
        "#581node2: hr 97.79 sh 1.2822 rmse 0.2291; hr 98.9 sh 1.2117 0.2270; hr 98.9 sh 1.5194 rmse 0.2213\n",
        "#584node2: hr 98.16 sh 1.1612 rmse 0.2216\n",
        "#584node2: hr 98.16 sh 1.2154 rmse 0.2214\n",
        "#585node2: hr 99.82 sh 1.5315 rmse 0.2212\n",
        "#586node2: lgbm hr 98.90 sh 1.6383 rmse 0.2213; Kall 10y 0.2098 hr 99.27 sh 2.4721 rmse 0.2098\n",
        "#589node2: lgbm hr 97.08 sh 1.7430 rmse 0.2277\n",
        "#590node2: lgbm hr 96.90 sh 1.5826 rmse 0.2276; hr 96.35 sh 1.4404 rmse 0.2275\n",
        "#hr 98.36 sh 1.7722 rmse 0.2266; hr 95.26 sh 1.3140 rmse 0.2211\n",
        "#594node2: hr 99.09 sh 1.5139 rmse 0.2127\n",
        "#599node2: hr 97.64 sh 1.6444 rmse 0.2193"
      ],
      "id": "909f0cf6-2223-4276-8e96-203fb7ae880f"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "07ae6cc2-90fc-40dc-a03c-632ebc69310b"
      },
      "outputs": [],
      "source": [
        "%%time\n",
        "if CFG.H2O == 1:\n",
        "    # live sub h2o\n",
        "    test_df_h2o = feature_df.query('data_type == \"live\"').copy()\n",
        "    test_sub_h2o = h2o.H2OFrame(test_df_h2o[drops].copy())\n",
        "    test_set_h2o = h2o.H2OFrame(test_df_h2o)\n",
        "    #test_sub_h2o = test_sub_h2o[drops].copy()\n",
        "    #test_h2o = h2o.H2OFrame(test_sub_h2o)\n",
        "    #h2o predict\n",
        "    #pred_h2o_df = AUTOML.predict(test_h2o[features])\n",
        "    # transfer pred to pandas dataframe\n",
        "    #pred_h2o = pred_h2o_df.as_data_frame()\n",
        "    test_pred_data_h2o = feature_df.query('data_type == \"live\"')[features]\n",
        "    print(test_sub_h2o.tail())\n",
        "    print(test_pred_data_h2o.tail())\n",
        "    # make h2o frame of test_pred_data\n",
        "    test_pred_data_h2o = h2o.H2OFrame(test_pred_data_h2o)\n",
        "    test_sub_h2o['prediction'] = AUTOML.predict(test_pred_data_h2o)\n",
        "    print(\"test length:\",len(test_sub_h2o))\n",
        "    logger.info(test_sub_h2o.shape)\n",
        "    #TODO: fix submission output file\n",
        "\n",
        "    #print(\"test h2o length:\",len(test_sub_h2o))\n",
        "    #test_sub_h2o['prediction'] = pred_h2o\n",
        "    print(test_sub_h2o.tail())\n"
      ],
      "id": "07ae6cc2-90fc-40dc-a03c-632ebc69310b"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8ce4ddb1-1d38-406d-8f87-f0e34210d965"
      },
      "outputs": [],
      "source": [
        "%%time\n",
        "if CFG.H2O == 1:\n",
        "    #H2O Submission\n",
        "    print(valid_sub_h2o.tail())\n",
        "\n",
        "\n",
        "    #print(\"Current date and time: \",str(datetime.datetime.now()))\n",
        "    test_sub_h2o['prediction'].as_data_frame().hist(bins=100)\n",
        "    def submit_signal(sub: pd.DataFrame, public_id: str, secret_key: str, slot_name: str):\n",
        "        \"\"\"\n",
        "        submit numerai signals prediction\n",
        "        \"\"\"\n",
        "        # setup private API\n",
        "        napi = numerapi.SignalsAPI(public_id, secret_key)\n",
        "\n",
        "        # write predictions to csv\n",
        "        model_id = napi.get_models()[f'{slot_name}']\n",
        "        filename = f\"sub_{model_id}.csv\"\n",
        "        sub.to_csv(filename, index=False)\n",
        "\n",
        "        # submit\n",
        "        submission = napi.upload_predictions(filename, model_id=model_id)\n",
        "        print(f'Submitted : {slot_name}!')\n",
        "    #print(\"validation length:\",len(valid_sub_h2o))\n",
        "    #valid_sub_h2o.to_csv(f'../{current_ds}_submission_h2o_4_val.csv', index=False)\n",
        "    # concat valid and test\n",
        "\n",
        "    sub_h2o = pd.concat([valid_sub_h2o, test_sub_h2o.as_data_frame()], ignore_index=True)\n",
        "\n",
        "    # rename to 'signal'\n",
        "    sub_h2o.rename(columns={'prediction': 'signal'}, inplace=True)\n",
        "\n",
        "    # select necessary columns\n",
        "    sub_h2o = sub_h2o[['ticker', 'friday_date', 'data_type', 'signal']]\n",
        "\n",
        "    public_id = '<Your Numerai API ID>'\n",
        "    secret_key = '<Your Numerai Secret Key>'\n",
        "    slot_name = '<Your Numerai Signals Submission Slot Name>'\n",
        "    # submit_signal(sub, public_id, secret_key, slot_name) # uncomment if you submit\n",
        "    print(\"submission h2o length:\",len(sub_h2o))\n",
        "    # save\n",
        "    sub_h2o.to_csv(f'../../Signals/Submission/{current_ds}_submission_h2o_{CFG.TYPE}_{CFG.RUN}_{CFG.START}_{CFG.END}_{today}.csv', index=False)"
      ],
      "id": "8ce4ddb1-1d38-406d-8f87-f0e34210d965"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ffd5b962-a7ef-4b17-9d54-8e87d0d72c75"
      },
      "outputs": [],
      "source": [
        "%%time\n",
        "if CFG.H2O == 1:\n",
        "    lb = h2o.automl.get_leaderboard(AUTOML, extra_columns = \"ALL\")\n",
        "    lb\n",
        "    lb_df = pd.DataFrame(lb)\n",
        "    lb_df.to_csv('h2oAutoML_lb.csv')\n",
        "#round369 node3: 140GB in use\n",
        "#round389 node3: 160GB in use 2min"
      ],
      "id": "ffd5b962-a7ef-4b17-9d54-8e87d0d72c75"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "1310e1c4-5f5a-4563-915f-f02f08370a44"
      },
      "outputs": [],
      "source": [
        "%%time\n",
        "if CFG.H2O == 1:\n",
        "    # Get model ids for all models in the AutoML Leaderboard\n",
        "    model_ids = list(AUTOML.leaderboard['model_id'].as_data_frame().iloc[:,0])\n",
        "    print(model_ids)\n",
        "\n",
        "    # Get a specific model by model ID\n",
        "    #TODO: test below\n",
        "    m = h2o.get_model(model_ids[0])"
      ],
      "id": "1310e1c4-5f5a-4563-915f-f02f08370a44"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "257a428e-7920-48ea-ab48-98edc16a43d2"
      },
      "outputs": [],
      "source": [
        "%%time\n",
        "if CFG.H2O == 1:\n",
        "    #print(m)\n",
        "    # View the parameters for the leader model selected above\n",
        "    m_keys = m.params.keys()\n",
        "    #print(m_keys)\n",
        "    #print(m.params)\n",
        "    m_keys_df = pd.DataFrame(m.params)\n",
        "    m_keys_df.to_csv('h2o_leader_params.csv')\n"
      ],
      "id": "257a428e-7920-48ea-ab48-98edc16a43d2"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "b5fd7fea-e6e4-499b-a961-584a1f4c3d29"
      },
      "outputs": [],
      "source": [
        "%%time\n",
        "if CFG.H2O == 1:\n",
        "    # Get the best XGBoost model using default sort metric\n",
        "    xgb = AUTOML.get_best_model(algorithm=\"xgboost\")\n",
        "    # View the parameters for the XGBoost model selected above\n",
        "    xgb_keys = xgb.params.keys()\n",
        "    #print(xgb_keys)\n",
        "    #print(xgb.params)\n",
        "    xgb_keys_df = pd.DataFrame(xgb.params)\n",
        "    xgb_keys_df.to_csv('h2o_xgb_params.csv')"
      ],
      "id": "b5fd7fea-e6e4-499b-a961-584a1f4c3d29"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "68b99a70-7772-4459-b454-e8ac26f3c0e5"
      },
      "outputs": [],
      "source": [
        "# Get the best ensemble model using ID\n",
        "#TODO: make contains \"ens\" first from list\n",
        "# Get a specific model by model ID\n",
        "#ens = h2o.get_model(\"StackedEnsemble_AllModels_1_AutoML_1_20221217_175245\")\n",
        "# View the parameters for the XGBoost model selected above\n",
        "#ens_keys = ens.params.keys()\n",
        "#print(xgb_keys)\n",
        "#print(xgb.params)\n",
        "#ens_keys_df = pd.DataFrame(ens.params)\n",
        "#ens_keys_df.to_csv('h2o_ens_params.csv')"
      ],
      "id": "68b99a70-7772-4459-b454-e8ac26f3c0e5"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "2173f3d2-9505-4bcc-898f-3213a7549cca"
      },
      "outputs": [],
      "source": [
        "%%time\n",
        "if CFG.H2O == 1:\n",
        "    #https://docs.h2o.ai/h2o/latest-stable/h2o-docs/save-and-load-model.html\n",
        "    # Store the H2O model\n",
        "    # save the model\n",
        "    model_path = h2o.save_model(m, path=\"\")#, force=True\n",
        "    model_path = h2o.save_model(xgb, path=\"\")#, force=True\n",
        "    #model_path = h2o.save_model(ens, path=\"\")#, force=True\n",
        "    #print model_path\n",
        "\n",
        "    # load the model\n",
        "    #saved_model_m = h2o.load_model(model_path)\n",
        "    # download the model built above to your local machine\n",
        "    #my_local_model = h2o.download_model(m, path=\"\")\n",
        "\n",
        "    # load the model\n",
        "    #saved_model = h2o.load_model(model_path)\n",
        "    # download the model built above to your local machine\n",
        "    #my_local_model = h2o.download_model(xgb, path=\"\")\n",
        "\n",
        "    # load the model\n",
        "    #saved_model = h2o.load_model(model_path)\n",
        "    # download the model built above to your local machine\n",
        "    #my_local_model = h2o.download_model(ens, path=\"\")"
      ],
      "id": "2173f3d2-9505-4bcc-898f-3213a7549cca"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "b2e17539-34b4-4e01-bf59-f0b2b74f13f5"
      },
      "outputs": [],
      "source": [
        "if CFG.H2O == 1:\n",
        "    #print(test_sub.head())\n",
        "    #print(test_sub_h2o.head())\n",
        "    if CFG.ANALYSE == 1:\n",
        "        print(len(test_sub))\n",
        "        print(len(test_sub_h2o))\n",
        "        print(len(feature_df.query('data_type == \"live\"')))\n",
        "        # histogram of prediction\n",
        "        test_sub_h2o['prediction'].hist(bins=100)"
      ],
      "id": "b2e17539-34b4-4e01-bf59-f0b2b74f13f5"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "97724d18"
      },
      "source": [
        "Let's submit! What is good with the Numerai Signals is that if you submit your predictions on the validation data, on the website, you can get more information about your model performance such as APY.\n",
        "\n",
        "Due to the change in the NumerAPI, now we need to split our submission file into the validation only one for getting diagnostics and the rest for the weekly submission."
      ],
      "id": "97724d18"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "d99439f2"
      },
      "outputs": [],
      "source": [
        "%%time\n",
        "\n",
        "# The following is mine. You cannot use them so replace them with yours:D\n",
        "#from kaggle_secrets import UserSecretsClient\n",
        "#user_secrets = UserSecretsClient()\n",
        "#public_id = user_secrets.get_secret(\"public_id\")\n",
        "#secret_key = user_secrets.get_secret(\"secret_key\")\n",
        "#slot_name = 'VANDEMONIA'"
      ],
      "id": "d99439f2"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "2fb2bb7a-cb38-4cc5-bae3-04b022e3628f"
      },
      "outputs": [],
      "source": [
        "%%time\n",
        "\"\"\"\n",
        "if CFG.LEAN == 1:\n",
        "    #write XGB selected nonrandom feature_df to parquet\n",
        "    if CFG.ANALYSE == 1:\n",
        "        feature_df = expand_mem_usage(feature_df)\n",
        "        feature_df.to_parquet(f'../../Signals/Data/feature_df_nonrandom{current_ds}.parquet')\n",
        "    feature_df.info()\n",
        "\"\"\"\n",
        "print(feature_df.info())\n",
        "#567node2: 81.5GB, 30m\n",
        "#590node2: 93.8GB 32.29m; 94.5GB 3895 columns; 89.7GB; 92.6GB"
      ],
      "id": "2fb2bb7a-cb38-4cc5-bae3-04b022e3628f"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "5127635d"
      },
      "outputs": [],
      "source": [
        "\"\"\"\n",
        "submit numerai signals prediction\n",
        "\"\"\"\n",
        "\"\"\"\n",
        "# setup private API\n",
        "#napi = numerapi.SignalsAPI(public_id, secret_key)\n",
        "#model_id = napi.get_models()[f'{slot_name.lower()}']\n",
        "\n",
        "# submit to get diagnostics\n",
        "filename = f\"example_sub_val{model_id}.csv\"\n",
        "test_sub.query('data_type == \"test\"').to_csv(filename, index=False)\n",
        "#napi.upload_diagnostics(filename, model_id=model_id)\n",
        "#print('Validation prediction uploaded for diagnostics!')\n",
        "\n",
        "# submit\n",
        "filename = f\"example_sub.csv\"\n",
        "test_sub.to_csv(filename, index=False)\n",
        "\"\"\""
      ],
      "id": "5127635d"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "7ab14e77"
      },
      "outputs": [],
      "source": [
        "def submit_signal(sub: pd.DataFrame, public_id: str, secret_key: str, slot_name: str):\n",
        "    \"\"\"\n",
        "    submit numerai signals prediction\n",
        "    \"\"\"\n",
        "    # setup private API\n",
        "    napi = numerapi.SignalsAPI(public_id, secret_key)\n",
        "    model_id = napi.get_models()[f'{slot_name.lower()}']\n",
        "\n",
        "    # submit to get diagnostics\n",
        "    filename = f\"example_sub_val{model_id}.csv\"\n",
        "    sub.query('data_type == \"validation\"').to_csv(filename, index=False)\n",
        "    napi.upload_diagnostics(filename, model_id=model_id)\n",
        "    print('Validation prediction uploaded for diagnostics!')\n",
        "\n",
        "    # submit\n",
        "    filename = f\"example_sub{model_id}.csv\"\n",
        "    sub.to_csv(filename, index=False)\n",
        "    try:\n",
        "        napi.upload_predictions(filename, model_id=model_id)\n",
        "        print(f'Submitted : {slot_name}!')\n",
        "    except Exception as e:\n",
        "        print(f'Submission failure: {e}')\n",
        "\n",
        "# concat valid and test\n",
        "sub = pd.concat([valid_sub, test_sub], ignore_index=True)\n",
        "\n",
        "# rename to 'signal'\n",
        "sub.rename(columns={'prediction': 'signal'}, inplace=True)\n",
        "\n",
        "# select necessary columns\n",
        "sub = sub[['ticker', 'friday_date', 'data_type', 'signal']]\n",
        "\n",
        "# submit\n",
        "submit_signal(sub, public_id, secret_key, slot_name)"
      ],
      "id": "7ab14e77"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8521d050"
      },
      "source": [
        "**Weekly submission is only accepted between 18:00 on Saturday ~ 14:30 on Monday (UTC)**, but you can always upload your validation prediction to see the diagnostics in the [Numerai website](https://signals.numer.ai/models).\n",
        "\n",
        "By clicking \"More\" --> \"Diagnostics\" in your model slot, you can see a pretty figure summarizing your validation scores. The following is from this notebook. Looks good, doesn't it!?\n",
        "\n",
        "\n",
        "![スクリーンショット 2021-10-14 16.01.14.png](attachment:3fbe6eaf-c2b9-43e5-b3ba-d50adfe9a55c.png)"
      ],
      "id": "8521d050"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "476eb239"
      },
      "outputs": [],
      "source": [
        "print(sub.shape)\n",
        "sub.head()"
      ],
      "id": "476eb239"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "c92d200b"
      },
      "outputs": [],
      "source": [
        "sub.tail()"
      ],
      "id": "c92d200b"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "e1bc8fd0-159e-46c8-8377-f79e86b128a2"
      },
      "source": [
        "# Multiple feature LSTM\n",
        "Sources:\n",
        "\n",
        "https://www.crosstab.io/articles/time-series-pytorch-lstm/\n",
        "https://stackoverflow.com/questions/65144346/feeding-multiple-inputs-to-lstm-for-time-series-forecasting-using-pytorch\n"
      ],
      "id": "e1bc8fd0-159e-46c8-8377-f79e86b128a2"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "349885ac-6dde-4c67-8465-bba1dc55d751"
      },
      "outputs": [],
      "source": [
        "%%time\n",
        "df = feature_df.rename(columns={'friday_date':'Date'})\n",
        "print(df.head())\n",
        "#Stock Prices Prediction Using Machine Learning and Deep Learning Techniques (with Python codes)\n",
        "#        Understanding the Problem Statement\n",
        "#        Moving Average\n",
        "#        Linear Regression\n",
        "#        k-Nearest Neighbors\n",
        "#        Auto ARIMA\n",
        "#       Prophet\n",
        "#        Long Short Term Memory (LSTM)\n",
        "#Source: https://www.analyticsvidhya.com/blog/2018/10/predicting-stock-price-machine-learningnd-deep-learning-techniques-python/\n",
        "import torch\n",
        "\n",
        "class LSTMForecast(torch.nn.Module):\n",
        "    \"\"\"\n",
        "    A very simple baseline LSTM model that returns\n",
        "    an output sequence given a multi-dimensional input seq. Inspired by the StackOverflow link below.\n",
        "    https://stackoverflow.com/questions/56858924/multivariate-input-lstm-in-pytorch\n",
        "    \"\"\"\n",
        "    def __init__(self, seq_length: int, n_time_series: int, output_seq_len=1, hidden_states:int=20, num_layers=2, bias=True, batch_size=100):\n",
        "        super().__init__()\n",
        "        self.forecast_history = seq_length\n",
        "        self.n_time_series = n_time_series\n",
        "        self.hidden_dim = hidden_states\n",
        "        self.num_layers = num_layers\n",
        "        self.lstm = torch.nn.LSTM(n_time_series, hidden_states, num_layers, bias, batch_first=True)\n",
        "        self.final_layer = torch.nn.Linear(seq_length*hidden_states, output_seq_len)\n",
        "        self.init_hidden(batch_size)\n",
        "\n",
        "    def init_hidden(self, batch_size)->None:\n",
        "        # This is what we'll initialise our hidden state\n",
        "        self.hidden = (torch.zeros(self.num_layers, batch_size, self.hidden_dim).to('cuda'), torch.zeros(self.num_layers, batch_size, self.hidden_dim).to('cuda'))\n",
        "\n",
        "    def forward(self, x: torch.Tensor) -> torch.Tensor:\n",
        "        batch_size = x.size()[0]\n",
        "        self.init_hidden(batch_size)\n",
        "        out_x, self.hidden = self.lstm(x, self.hidden)\n",
        "        x = self.final_layer(out_x.contiguous().view(batch_size, -1))\n",
        "        return x\n",
        "\n",
        "model = LSTMForecast(1, 1, batch_size=1).to('cuda')\n",
        "a = torch.rand(1, 1, 1).to('cuda')\n",
        "model(a)\n",
        "#### end Torch model ###\n",
        "\n",
        "#### tensorflow model ###\n",
        "#importing required libraries\n",
        "from sklearn.preprocessing import MinMaxScaler\n",
        "from keras.models import Sequential\n",
        "from keras.layers import Dense, Dropout, LSTM\n",
        "\n",
        "#creating dataframe\n",
        "data = df.sort_index(ascending=True, axis=0)\n",
        "new_data = pd.DataFrame(index=range(0,len(df)),columns=['Date', 'Close'])\n",
        "for i in range(0,len(data)):\n",
        "    new_data['Date'][i] = data['Date'][i]\n",
        "    new_data['Close'][i] = data['Close'][i]\n",
        "\n",
        "#setting index\n",
        "new_data.index = new_data.Date\n",
        "new_data.drop('Date', axis=1, inplace=True)\n",
        "\n",
        "#creating train and test sets\n",
        "dataset = new_data.values\n",
        "\n",
        "train = dataset[0:987,:]\n",
        "valid = dataset[987:,:]\n",
        "\n",
        "#converting dataset into x_train and y_train\n",
        "scaler = MinMaxScaler(feature_range=(0, 1))\n",
        "scaled_data = scaler.fit_transform(dataset)\n",
        "\n",
        "x_train, y_train = [], []\n",
        "for i in range(60,len(train)):\n",
        "    x_train.append(scaled_data[i-60:i,0])\n",
        "    y_train.append(scaled_data[i,0])\n",
        "x_train, y_train = np.array(x_train), np.array(y_train)\n",
        "\n",
        "x_train = np.reshape(x_train, (x_train.shape[0],x_train.shape[1],1))\n",
        "\n",
        "# create and fit the LSTM network\n",
        "model = Sequential()\n",
        "model.add(LSTM(units=50, return_sequences=True, input_shape=(x_train.shape[1],1)))\n",
        "model.add(LSTM(units=50))\n",
        "model.add(Dense(1))\n",
        "\n",
        "model.compile(loss='mean_squared_error', optimizer='adam')\n",
        "model.fit(x_train, y_train, epochs=1, batch_size=1, verbose=2)\n",
        "\n",
        "#predicting 246 values, using past 60 from the train data\n",
        "inputs = new_data[len(new_data) - len(valid) - 60:].values\n",
        "inputs = inputs.reshape(-1,1)\n",
        "inputs  = scaler.transform(inputs)\n",
        "\n",
        "X_test = []\n",
        "for i in range(60,inputs.shape[0]):\n",
        "    X_test.append(inputs[i-60:i,0])\n",
        "X_test = np.array(X_test)\n",
        "\n",
        "X_test = np.reshape(X_test, (X_test.shape[0],X_test.shape[1],1))\n",
        "closing_price = model.predict(X_test)\n",
        "closing_price = scaler.inverse_transform(closing_price)\n",
        "\n",
        "# output results\n",
        "#times = [time/60 for time in times]\n",
        "#print('Times:', times)\n",
        "#print('Scores:', scores)\n",
        "print('Winning pipelines:', winning_pipes)"
      ],
      "id": "349885ac-6dde-4c67-8465-bba1dc55d751"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "3c7d5027-4f08-45f2-a01d-26163f4fde3e"
      },
      "outputs": [],
      "source": [
        "%%time\n",
        "#print(X_test)\n",
        "#print(closing_price)\n",
        "criterion = torch.nn.MSELoss(reduction='sum')\n",
        "\n",
        "#converting dataset into x_train and y_train\n",
        "\n"
      ],
      "id": "3c7d5027-4f08-45f2-a01d-26163f4fde3e"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "dea72b30-c58a-4f57-8b7d-612d09fbfbc9"
      },
      "outputs": [],
      "source": [
        "%%time\n",
        "scaled_data_test = scaler.fit_transform(valid)\n",
        "#predicting 246 values, using past 60 from the train data\n",
        "inputs = new_data[len(new_data) - len(valid) - 60:].values\n",
        "inputs = inputs.reshape(-1,1)\n",
        "scaled_data_test  = scaler.transform(inputs)\n",
        "\n",
        "X_test, y_test = [], []\n",
        "for i in range(60,len(valid)):\n",
        "    X_test.append(scaled_data_test[i-60:i,0])\n",
        "    y_test.append(scaled_data_test[i,0])\n",
        "#_test, y_test = np.array(X_test), np.array(y_test)\n",
        "\n",
        "#_test = np.reshape(y_test, (y_test.shape[0],y_test.shape[1],1))\n",
        "#_test= scaler.inverse_transform(y_test)"
      ],
      "id": "dea72b30-c58a-4f57-8b7d-612d09fbfbc9"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "aa0a9b71-41b7-47b5-8885-e4ffed76db23"
      },
      "outputs": [],
      "source": [
        "%%time\n",
        "print(np.array(y_test))\n",
        "closing_price = model.predict(X_test)\n",
        "print(closing_price)"
      ],
      "id": "aa0a9b71-41b7-47b5-8885-e4ffed76db23"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "65967248-dfe3-4062-943a-12ff686d7388"
      },
      "outputs": [],
      "source": [
        "loss = criterion(closing_price, y_test)"
      ],
      "id": "65967248-dfe3-4062-943a-12ff686d7388"
    },
    {
      "cell_type": "code",
      "source": [
        "#FMP SDK\n",
        "!pip install fmpsdk"
      ],
      "metadata": {
        "id": "kqZEyybHoMnT"
      },
      "id": "kqZEyybHoMnT",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "hDepSgb-oPoI"
      },
      "id": "hDepSgb-oPoI",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2b744bc5-e129-4a88-ae38-2e8861df2204"
      },
      "source": [
        "#"
      ],
      "id": "2b744bc5-e129-4a88-ae38-2e8861df2204"
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.4"
    },
    "papermill": {
      "default_parameters": {},
      "duration": 2584.77982,
      "end_time": "2022-10-15T21:12:34.244765",
      "environment_variables": {},
      "exception": null,
      "input_path": "__notebook__.ipynb",
      "output_path": "__notebook__.ipynb",
      "parameters": {},
      "start_time": "2022-10-15T20:29:29.464945",
      "version": "2.3.4"
    },
    "accelerator": "GPU"
  },
  "nbformat": 4,
  "nbformat_minor": 5
}